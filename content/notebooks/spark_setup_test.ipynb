{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "We want to read data from S3 with Spark. Ideally we want to be able to read Parquet files from S3 into our Spark Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "On my Kubernetes cluster I am using the [Pyspark notebook](https://github.com/jupyter/docker-stacks/tree/master/pyspark-notebook). In the home folder on the container I downloaded and extracted [Spark 2.4.0](https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz). After extracting I set the `SPARK_HOME` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/jovyan/spark-2.4.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download the libraries to be able to communicate with AWS and use S3 as a file system. Download the following two jars to the `jars` folder in the Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-04 16:16:09--  http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar\n",
      "Resolving central.maven.org (central.maven.org)... 151.101.48.209\n",
      "Connecting to central.maven.org (central.maven.org)|151.101.48.209|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11948376 (11M) [application/java-archive]\n",
      "Saving to: ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar’\n",
      "\n",
      "aws-java-sdk-1.7.4. 100%[===================>]  11.39M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2019-01-04 16:16:09 (129 MB/s) - ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar’ saved [11948376/11948376]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar -P $SPARK_HOME/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-04 16:16:09--  http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\r\n",
      "Resolving central.maven.org (central.maven.org)... 151.101.48.209\r\n",
      "Connecting to central.maven.org (central.maven.org)|151.101.48.209|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 126287 (123K) [application/java-archive]\r\n",
      "Saving to: ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar’\r\n",
      "\r\n",
      "\r",
      "hadoop-aws-2.7.3.ja   0%[                    ]       0  --.-KB/s               \r",
      "hadoop-aws-2.7.3.ja 100%[===================>] 123.33K  --.-KB/s    in 0.002s  \r\n",
      "\r\n",
      "2019-01-04 16:16:09 (59.2 MB/s) - ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar’ saved [126287/126287]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P $SPARK_HOME/jars/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read from AWS S3, we need to set some parameters in the configuration file for spark. This is normally located at `$SPARK_HOME/conf/spark-defaults.conf`. Enter the following three key value pairs replacing the obvious values:\n",
    "\n",
    "```\n",
    "# spark-defaults.conf\n",
    "spark.hadoop.fs.s3a.access.key=MY_ACCESS_KEY\n",
    "spark.hadoop.fs.s3a.secret.key=MY_SECRET_KEY\n",
    "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script\n",
    "Set the Spark configuration and create the Spark context and the SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setAppName(\"S3 Configuration Test\")\n",
    "         .set(\"spark.executor.instances\", \"1\")\n",
    "         .set(\"spark.executor.cores\", 1)\n",
    "         .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path where to write to and read from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"s3a://user-jwaterschoot/mario-colors/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple RDD and save it as a dataframe in Parquet format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('Mario', 'Red'), ('Luigi', 'Green'), ('Princess', 'Pink')])\n",
    "rdd.toDF(['name', 'color']).write.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data back from the S3 path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    name|color|\n",
      "+--------+-----+\n",
      "|Princess| Pink|\n",
      "|   Luigi|Green|\n",
      "|   Mario|  Red|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
