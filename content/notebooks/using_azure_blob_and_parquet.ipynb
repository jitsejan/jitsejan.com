{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This notebook shows how to interact with Parquet on Azure Blob Storage.\n",
    "\n",
    "Please note that it is not possible to write Parquet to Blob Storage using PySpark. I have tried with version 2.2, 2.3 and 2.4 but none of them work (yet). It connects and creates the folder, but no data is written. Azure support was not able to help me, except for advising me to use HDinsights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "from configparser import RawConfigParser\n",
    "from pyspark import SparkConf, SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOB_NAME = \"characters.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration\n",
    "config = RawConfigParser()\n",
    "config.read(\"blobconfig.ini\")\n",
    "# Create blob_service\n",
    "blob_service = BlockBlobService(\n",
    "    account_name=config[\"blob-store\"][\"blob_account_name\"],\n",
    "    account_key=config[\"blob-store\"][\"blob_account_key\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark\n",
    "In order to connect to Azure Blob Storage with Spark, we need to download two JARS (`hadoop-azure-2.7.3.jar` and `azure-storage-6.1.0.jar`) and add them to the Spark configuration. I chose these specific versions since they were the only ones working with reading data using Spark 2.4.0. Additionally, the `fs.azure` needs to be set to the Azure FileSytem and `fs.azure.account.key.<youraccountname>.blob.core.windows.net` should contain the account key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def setup_spark(config):\n",
    "    \"\"\" Setup Spark to connect to Azure Blob Storage \"\"\"\n",
    "    jars = [\n",
    "        \"spark-2.4.0-bin-hadoop2.7/jars/hadoop-azure-2.7.3.jar\",\n",
    "        \"spark-2.4.0-bin-hadoop2.7/jars/azure-storage-6.1.0.jar\",\n",
    "    ]\n",
    "    conf = (\n",
    "        SparkConf()\n",
    "        .setAppName(\"Spark Blob Test\")\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(jars))\n",
    "        .set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "        .set(\n",
    "            f\"fs.azure.account.key.{config['blob-store']['blob_account_name']}.blob.core.windows.net\",\n",
    "            config[\"blob-store\"][\"blob_account_key\"],\n",
    "        )\n",
    "    )\n",
    "    sc = SparkContext(conf=conf).getOrCreate()\n",
    "\n",
    "    return SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark context\n",
    "sql_context = setup_spark(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  color\n",
      "0     Mario    Red\n",
      "1     Luigi  Green\n",
      "2  Princess   Pink\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(\n",
    "    [(\"Mario\", \"Red\"), (\"Luigi\", \"Green\"), (\"Princess\", \"Pink\")]\n",
    ").rename(columns={0: \"name\", 1: \"color\"})\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PyArrow with Pandas it is easy to write a dataframe to Blob Storage. Convert the Pandas dataframe into Parquet using a buffer and write the buffer to a blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pandas_dataframe_to_blob(blob_service, df, container_name, blob_name):\n",
    "    \"\"\" Write Pandas dataframe to blob storage \"\"\"\n",
    "    buffer = BytesIO()\n",
    "    df.to_parquet(buffer)\n",
    "    blob_service.create_blob_from_bytes(\n",
    "        container_name=container_name, blob_name=blob_name, blob=buffer.getvalue()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to blob using pyarrow\n",
    "write_pandas_dataframe_to_blob(blob_service, df, config['blob-store']['blob_container'], BLOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data back from blob using Pandas is identical. The data is read into a `ByteStream` from the blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pandas_dataframe_from_parquet_on_blob(blob_service, container_name, blob_name):\n",
    "    \"\"\" Get a dataframe from Parquet file on blob storage \"\"\"\n",
    "    byte_stream = BytesIO()\n",
    "    try:\n",
    "        blob_service.get_blob_to_stream(\n",
    "            container_name=container_name, blob_name=blob_name, stream=byte_stream\n",
    "        )\n",
    "        df = pq.read_table(source=byte_stream).to_pandas()\n",
    "    finally:\n",
    "        byte_stream.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  color\n",
      "0     Mario    Red\n",
      "1     Luigi  Green\n",
      "2  Princess   Pink\n"
     ]
    }
   ],
   "source": [
    "# Read from blob using pyarrow\n",
    "rdf = get_pandas_dataframe_from_parquet_on_blob(\n",
    "    blob_service, config['blob-store']['blob_container'], BLOB_NAME\n",
    ")\n",
    "print(rdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data using Spark for a single file Parquet blob is done using the following function. Note the path that uses the `wasbs` protocol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pyspark_dataframe_from_parquet_on_blob(config, sql_context, container_name, blob_name):\n",
    "    \"\"\" Get a dataframe from Parquet file on blob storage using PySpark \"\"\"\n",
    "    path = f\"wasbs://{container_name}@{config['blob-store']['blob_account_name']}.blob.core.windows.net/{blob_name}\"\n",
    "    return sql_context.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------------+\n",
      "|    name|color|__index_level_0__|\n",
      "+--------+-----+-----------------+\n",
      "|   Mario|  Red|                0|\n",
      "|   Luigi|Green|                1|\n",
      "|Princess| Pink|                2|\n",
      "+--------+-----+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read from blob using PySpark\n",
    "sdf = get_pyspark_dataframe_from_parquet_on_blob(\n",
    "    config, sql_context, config['blob-store']['blob_container'], BLOB_NAME\n",
    ")\n",
    "print(sdf.show())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
