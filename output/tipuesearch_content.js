{ "pages": [{ "title": "Latex cheatsheet", "text": "This is my Latex cheatsheet Add style to a code listing styles.tex \\definecolor { dkgreen }{ rgb }{ 0,0.6,0 } \\definecolor { gray }{ rgb }{ 0.5,0.5,0.5 } \\definecolor { mauve }{ rgb }{ 0.58,0,0.82 } \\definecolor { light-gray }{ gray }{ 0.25 } \\usepackage { listings } \\lstdefinestyle { java }{ language=Java, aboveskip=3mm, belowskip=3mm, showstringspaces=false, columns=flexible, basicstyle= { \\footnotesize\\ttfamily } , numberstyle= { \\tiny } , numbers=left, keywordstyle= \\color { blue } , commentstyle= \\color { dkgreen } , stringstyle= \\color { mauve } , breaklines=true, breakatwhitespace=true, tabsize=3, } main.tex \\input { styles.tex } \\lstinputlisting [style=Java, frame=single, caption={Hello world}, captionpos=b] { helloworld.java } Bonus: rename the caption from Listing to Code snippet with the following: \\renewcommand*\\lstlistingname { Code snippet } Subfigures \\usepackage { graphicx } \\usepackage { caption } \\usepackage { subcaption } \\begin { figure* } [h] \\centering \\begin { subfigure } [t] { 0.5 \\textwidth } \\centering \\includegraphics [width=2.2in] { image _ one.png } \\caption { Image one } \\end { subfigure } % ~ \\begin { subfigure } [t] { 0.5 \\textwidth } \\centering \\includegraphics [width=2.2in] { image _ two.png } \\caption { Image two } \\end { subfigure } \\caption { These are two images } \\end { figure* }", "tags": "Latex", "url": "pages/latex-cheatsheet.html", "loc": "pages/latex-cheatsheet.html" }, { "title": "Pandas cheatsheet", "text": "This is my Pandas cheatsheet. Note that I import pandas the 'standard' way: import pandas as pd Convert with dataframes Create dataframe from a dictionary character_df = pd . DataFrame . from_dict ( characters ) characters = character_df . to_dict ( orient = 'records' ) Convert CSV to dataframe character_df = pd . DataFrame . from_csv ( \"characters.csv\" , sep = ' \\t ' , encoding = 'utf-8' ) character_df . to_csv ( 'characters.csv' , sep = ' \\t ' , encoding = 'utf-8' ) Convert dataframe to JSON character_df = pd . DataFrame . from_json ( 'characters.json' ) character_df . to_json ( 'characters.json' , orient = 'records' ) Convert dataframe to pickle character_df = pd . read_pickle ( 'characters.pandas' ) character_df . to_pickle ( 'characters.pandas' ) Convert database query to dataframe db = create_engine ( 'postgresql:// %s : %s @ %s : %d /characters' % ( POSTGRES_USER , POSTGRES_PASS , POSTGRES_HOST , POSTGRES_PORT )) character_df = pd . read_sql_query ( 'SELECT * FROM \"character_collection\"' , con = db ) Cleaning dataframes Replace in column character_df [ 'name' ] = character_df [ 'name' ] . str . replace ( '-' , ' ' ) Regex replace in whole dataframe character_df . replace ( r '-' , r ' ' , regex = True , inplace = True ) Regex extract in column character_df [ 'introduction_year' ] = character_df [ 'date_of_introduction' ] . str . extract ( '(\\d {4} )-..-..' , expand = True ) Remove all Not-a-Numbers character_df = character_df . replace ({ 'NaN' : None }, regex = True ) Rename a column character_df . rename ( columns = { 'name' : 'character_name' }, inplace = True ) Or replace characters: character_df . columns = character_df . columns . str . replace ( '.' , '_' ) Drop a column character_df = character_df . drop ( 'origin' , axis = 1 ) Drop a row Drop all rows where the name is NaN. character_df . dropna ( subset = [ 'name' ], inplace = True ) Delete a column del character_df [ 'special_diet' ] Convert to integer character_df [ 'introduction_year' ] = character_df [ 'introduction_year' ] . fillna ( - 1 ) . astype ( 'int64' ) Convert to category character_df [ 'superpower' ] = character_df [ 'superpower' ] . astype ( 'category' ) Convert string to list # Convert a string with surrounding brackets and quotes to a list def convert_string_to_list ( column ): \"\"\" Convert unicode string to list \"\"\" return column . str . strip ( ' {} ' ) . astype ( str ) . apply ( lambda x : x . split ( ',' )[ 0 ] . strip ( \" \\\" \" ) if len ( x ) > 0 else \"\" ) character_df [ 'superpowers' ] = convert_string_to_list ( character_df [ 'superpowers' ]) Create column from index character_df . index . names = [ 'Name' ] character_df = character_df . reset_index () Extend dictionary cell to columns df = pd . concat ([ df . drop ([ 'meta' ], axis = 1 ), df [ 'meta' ] . apply ( pd . Series )], axis = 1 ) Find data Describe the data character_df [ 'age' ] . describe () Unique values characters = character_df [ 'character_name' ] . unique () Field contains character_df [ character_df [ 'name' ] . str . contains ( \"Koopa\" ) . fillna ( False )] Count by character_df . groupby ([ 'superpowers' ]) . count () Loop through data for element in character_df . index : superpower = character_df . iloc [ element ][ 'superpower' ] if not pd . isnull ( superpower ): print 'Super!' Substract Substract two consecutive cells df [ 'difference' ] = df [ 'amount' ] - df [ 'amount' ] . shift ( + 1 ) Add a maximum column for a groupby df [ 'group_maximum' ] = df . groupby ([ 'category' ])[ 'score' ] . transform ( max ) Get maximum 10 df . groupby ([ 'category' ])[ 'viewers' ] . sum () . nlargest ( 10 ) Create category based on values def set_category ( row ): if row [ 'score' ] < float ( row [ 'maximum' ] / 3 ): return 'beginner' elif row [ 'score' ] >= float ( row [ 'maximum' ] / 3 * 2 ): return 'expert' else : return 'intermediate' df [ 'category' ] = df . apply ( set_category , axis = 1 ) Apply lambda function df [ 'inverse_number' ] = df [ 'number' ] . apply ( lambda x : x ** ( - 1 )) Sort values df . sort_values ( 'name' , ascending = False ) Normalize a JSON column pd . io . json . json_normalize ( df [ 'json_col' ]) Select data df [ df . name . notnull ()] or df . query ( 'name.notnull()' , engine = 'python' ) Expand cell with list to rows df [ 'list_cells' ] \\ . apply ( pd . Series ) \\ . stack () \\ . reset_index ( level = 1 , drop = True ) \\ . to_frame ( 'list_cell' ) Find and drop empty columns empty_cols = [ col for col in df . columns if df [ col ] . isnull () . all ()] df . drop ( empty_cols , axis = 1 , inplace = True ) Merge two dataframe combined_data_df = first_df . merge ( second_df , left_on = 'left_id' , right_on = 'right_id' , how = 'left' ) Calculate difference between two consecutive rows df [ 'diff' ] = df [ 'amount' ] \\ . diff () \\ . fillna ( 0 ) Filter each column larger than threshold THRESHOLD = 100 df [ df . gt ( THRESHOLD ) . all ( axis = 1 )] . sort_values ( 'total' , ascending = False )", "tags": "pandas", "url": "pages/pandas-cheatsheet.html", "loc": "pages/pandas-cheatsheet.html" }, { "title": "Python cheatsheet", "text": "This is my Python cheatsheet Pretty print a dictionary print json . dumps ( characters [: 1 ], indent = 4 ) Find index of item in dictionary index = next ( index for ( index , d ) in enumerate ( characters ) if d [ \"name\" ] == 'Mario' ) Databases Retrieve from Postgresql import psycopg2 connnection = psycopg2 . connect ( database = \"character_db\" , user = \"character_user\" , password = \"character1234\" , host = \"localhost\" , port = '5433' ) cursor = connnection . cursor () cursor . execute ( \"SELECT * FROM \\\" characters \\\" \" ) characters = cursor . fetchall () Dictionary to CSV import csv def write_dictionary_to_csv ( o_file , d ): \"\"\" Write dictionary to output file \"\"\" with open ( o_file , 'wb' ) as csvfile : outputwriter = csv . writer ( csvfile , delimiter = ';' , quoting = csv . QUOTE_MINIMAL ) outputwriter . writerow ( d . keys ()) outputwriter . writerows ( zip ( * d . values ())) output_file = 'output.csv' write_dictionary_to_csv ( output_file , data ) Web crawling Use urllib2 to retrieve page content import urllib2 req = urllib2 . Request ( 'http://www.mariowiki.com' ) data = urllib2 . urlopen ( req ) . read () Set header for urllib2 import urllib2 # Header to request a page with NL as country HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } req = urllib2 . Request ( 'http://www.mariowiki.com' , headers = HEADER ) data = urllib2 . urlopen ( req ) . read () Use requests to retrieve page content import requests resp = requests . get ( 'http://www.mariowiki.com' ) data = resp . content Use Selenium to retrieve page content from selenium import webdriver browser = webdriver . Chrome () browser . get ( 'http://www.mariowiki.com' ) data = browser . page_source browser . quit () Scroll down in Selenium browser . execute_script ( \"window.scrollTo(0, document.body.scrollHeight);\" ) Find an element and click in Selenium By CSS selector: browser . find_element_by_css_selector ( '#clickme' ) . click () By attribute: browser . find_element_by_xpath ( '//input[@title=\"Open page\"]' ) . click () Make a list of links with cssselect import lxml.html tree = lxml . html . fromstring ( data ) links = [ 'http://www.mariowiki.com' + link . get ( 'href' ) for link in tree . cssselect ( 'div[role*= \\' navigation \\' ] a' )] Switch between tabs with Selenium browser . switch_to . window ( window_name = browser . window_handles [ 1 ]) browser . quit () browser . switch_to . window ( window_name = browser . window_handles [ 0 ]) Pickle Use the pickle libary to save a variable to a file and load it again. import pickle colors = [ 'blue' , 'red' ] pickle . dump ( colors , open ( \"colors.p\" , \"wb\" ) ) saved_colors = pickle . load ( open ( \"colors.p\" , \"rb\" ) ) saved_colors Filter A simple trick to select columns from a dataframe: # Create the filter condition condition = lambda col : col not in DESIRED_COLUMNS # Filter the dataframe filtered_df = df . drop ( * filter ( condition , df . columns )) Use seaborn to create heatmap import seaborn as sns sns . heatmap ( df \\ . groupby ([ 'field_a' , 'field_b' ])[ 'amount' ] \\ . sum () \\ . to_frame () \\ . reset_index () \\ . pivot ( 'field_a' , 'field_b' , 'amount' )); or sdf = df \\ . groupby ([ 'field_a' , 'field_b' ])[ 'amount' ] \\ . sum () \\ . reset_index () sns . heatmap ( sdf . pivot ( 'genre' , 'country' , 'view_hours' ))", "tags": "python", "url": "pages/python-cheatsheet.html", "loc": "pages/python-cheatsheet.html" }, { "title": "Spark cheatsheet", "text": "Import PySpark import pyspark Setup SparkSession spark = pyspark . sql . SparkSession . builder \\ . master ( \"local[*]\" ) \\ . enableHiveSupport () \\ . getOrCreate () Read data json_sdf = spark . read . json ( \"mydata.json\" ) Convert RDD to Pandas DataFrame json_pdf = json_sdf . toPandas () Convert PySpark row to dictionary row . asDict ( recursive = True ) Join two dataframes import pyspark.sql.functions as F df = df_01 . alias ( 'dfone' ) . join ( df_02 . alias ( 'dftwo' ), on = [ F . col ( 'dfone.id' ) == F . col ( 'dftwo.id' )], how = 'left' ) . drop ( 'id' ) Select fields from dataframe df . select ( 'id' , 'name' , 'country' , 'amount' ) . show () Expand JSON df . withColumn ( 'json' , F . from_json ( F . col ( '_json_col' ) . cast ( 'string' ), json_schema )) . show () Casting a datatype from pyspark.sql.types import IntegerType dataframe . withColumn ( \"count\" , F . col ( \"count\" ) . cast ( IntegerType ()))", "tags": "Python, Spark, PySpark", "url": "pages/spark-cheatsheet.html", "loc": "pages/spark-cheatsheet.html" }, { "title": "Splunk cheatsheet", "text": "This is my Splunk cheatsheet. Replace single quote with double quote | rex mode=sed \"s/\\'/\\\"/g\" field=myfield Extract JSON data from an JSON array The following will try to find ten matches for strings contained in curly brackets. Next it will be expanded to a multi value field so we can use spath on each extracted field. | rex max_match=10 \"(?<json_field>{[&#94;}]+})\" field=myjsonarrayfield | mvexpand json_field | spath input=json_field | rename field_in_json_field AS field Drilldown of areachart <drilldown> <set token= \"form.character\" > $click.name2$ </set> </drilldown> Create a range between limits | eval range_field = mvrange ( start , end , step ) | mvexpand range_field | stats count by range_field Frequency of Splunk restarts index=_internal \"Splunkd starting\" | timechart span=1d count(_raw) as Event", "tags": "splunk", "url": "pages/splunk-cheatsheet.html", "loc": "pages/splunk-cheatsheet.html" }, { "title": "Creating a quick app using Supabase", "text": "In this project I am building the following: Supabase back-end as database and REST API. Pythonista script on iPhone to write to the database through the API. VueJS front-end to display the content of the table in the database. Idea The idea for this project is to finally use Pythonista on my phone. Every time I hit the Share Menu I see the option to Run a Pythonista script but I never had the opportunity (or imagination) to actually use it. Since I was already building a Dictionary app as part of a VueJS/FastAPI/Postgres project to get familiar with FastAPI I thought it would be interesting to use my phone to add words to the database. In addition to adding words to the database the Pythonista script will also query a dictionary API to add the definition to the table as well. Before I got to the Pythonista script I (luckily) stumbled upon Supabase to replace the FastAPI and Postgres part since it comes with the batteries included and it would avoid me writing any code. Why not learn about Pythonista and Supabase at the same time? Finally, I will use VueJS to create a front-end to display the words and definitions stored in the database. Initially the layout can be very plain and styling it or adding pagination will be for a future me. Tools Supabase Supabase is used in this project because of its functionality, the great documentation and it being free (for now?). According to the Pricing page it will be free for personal development projects so it should be more than fine for what I am trying to achieve here. Supabase is using a Postgres database and comes with a built-in API and event stream. All great things that would take some proper effort to setup yourself. First of all, I create a new project where I set the Organization name and add the name for the database. After a quick coffee the project will be ready and the database and API are ready to use. Once the database is ready I create a new table. As described in the introduction, I am creating a simple project where I can select words in Safari on my iPhone, run a Pythonista script from the Share Menu , get the definition from a dictionary and add the word and the definition to the database. Logically the table in my Dictionary database will be named Words . For simplicity I add a id , word and definition column. Very impressive indeed! Finally, now that the table is created we need to copy the API key to be able to connect to the database and start inserting some data. Go the Settings > API and copy the url and key . Pythonista Perhaps I shouldn't admit this but I have had Pythonista installed on my iPhone for ages. When I tried to do a project in the IDE it wasn't working as smooth as I hoped and it was hard to install libraries. With Pythonista 3 it all looks a bit better and I am now aware I should not count on using exotic libraries. The script looks like the following on the phone. It looks clean but of course developing is faster on a computer which is why I am using iCloud to store the script. Once I press save on my Mac it will take a couple of seconds but it will refresh the Pythonista IDE and have the updated script. The full script is shown below (and stored as gist as well). import appex import json import requests LANGUAGE = \"en-gb\" OXFORD_ID = \"9acc1234\" OXFORD_KEY = \"6baccf388cd6456456326e85054f30aba\" OXFORD_URL = \"https://od-api.oxforddictionaries.com/api/v2\" OXFORD_HEADERS = { \"app_id\" : OXFORD_ID , \"app_key\" : OXFORD_KEY } SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYW5vbiIsImlhdCI6MTYxNzE4NjQ0\" SUPABASE_URL = \"https://zbowexzxwkfvehmevgvl.supabase.co/rest/v1\" SUPABASE_HEADERS = { \"apikey\" : SUPABASE_KEY , \"Authorization\" : f \"Bearer { SUPABASE_KEY } \" } def insert_entry ( word : str , definition : str ): \"\"\" Insert entry into Supabase database \"\"\" if not isinstance ( word , str ): return post_headers = { \"Content-Type\" : \"application/json\" , \"Prefer\" : \"return=representation\" } data = { \"word\" : word , \"definition\" : definition } return requests . post ( f \" { SUPABASE_URL } /Words\" , data = json . dumps ( data ), headers = { ** SUPABASE_HEADERS , ** post_headers }) def get_words (): \"\"\" Retrieve the existing words from the words table \"\"\" list_resp = requests . get ( f \" { SUPABASE_URL } /Words?select=word\" , headers = SUPABASE_HEADERS ) return list_resp . json () def get_definition ( word : str ): \"\"\" Get the definition from the Oxford dictionary \"\"\" url = f \" { OXFORD_URL } /entries/ { LANGUAGE } / { word . lower () } \" params = { 'fields' : 'definitions' } resp = requests . get ( url , headers = OXFORD_HEADERS , params = params ) . json () return resp [ 'results' ][ 0 ][ 'lexicalEntries' ][ 0 ][ 'entries' ][ 0 ][ 'senses' ][ 0 ][ 'definitions' ][ 0 ] def main (): \"\"\" Main function \"\"\" copied_text = appex . get_text () if not copied_text : print ( \"No input found\" ) return definition = get_definition ( copied_text ) insert_entry ( word = copied_text , definition = definition ) if __name__ == \"__main__\" : main () The Pythonista script will perform the following actions: Get the text from the clipboard ussing the appex library If there is a word found it will query and return the definition from the Oxford Dictionary It will insert the word and the definition into the Supabase table When I navigate to a website in Safari and I select a word I can go to the Share option like below: I select Run Pythonista Script from the context menu. Here I created a shortcut to run my script directly from this overview instead of navigating to the script. The blue icon will kick of the script and perform the actions mentioned before. And yes, there is no error handling, so when something goes wrong it will throw a Python error and not be handled properly (yet). Because this is just a pet project I did not spend too much time in making it super robust. The only check right now is to verify that when the script is called there is actually text selected. Front-end application To create a quick way to visualize the content of my dictionary table I have made a small VueJS app that connect to Supabase and displays the words and their definitions. I followed the installation guide to make sure I had the latest Vue CLI installed since that will easily create the scaffold for the front-end app. Additionally, I check the versions of npm and nodejs since I don't use them every day and often get outdated. ❯ vue -V @vue/cli 4 .5.12 ❯ npm --version 6 .14.10 ❯ node --version v14.15.4 After running vue create I have the following structure: ~/code/vuejs-supabase @ master ❯ tree src src ├── App.vue ├── assets │ └── logo.png ├── components │ └── WordOverview.vue ├── config.js ├── main.js └── services └── WordDataService.js App.vue The main application contains the template with the WordOverview component, a link to the WordDataService.js and calls the service to retrieve the words. < template > < WordOverview v - bind : words = \"words\" /> </ template > < script > import WordDataService from \"./services/WordDataService\" ; import WordOverview from './components/WordOverview.vue' export default { name : 'App' , data : () => ({ words : [] }), methods : { getWords () { WordDataService . getAll () . then (( res ) => { console . log ( res ); this . words = res . body ; }) . catch (( error ) => { console . error ( error ); }); }, }, async created () { this . getWords (); }, components : { WordOverview } } </ script > < style > #app { font - family : Avenir , Helvetica , Arial , sans - serif ; - webkit - font - smoothing : antialiased ; - moz - osx - font - smoothing : grayscale ; text - align : center ; color : #2c3e50; margin - top : 60 px ; } </ style > components/WordOverview.vue The WordOverview component contains a very basic markup to display the words from the database in an unordered list. <template> <div> <h1> Word overview </h1> <ul class= \"word-ul\" > <li class= \"word-li-entry\" v-for= \"elem in words\" :key= \"elem.id\" > <h2 class= \"word-title\" > {{ elem.word }} </h2> <q class= \"word-definition\" > {{ elem.definition }} </q> </li> </ul> </div> </template> <script> export default { name: 'WordOverview', props: { words: Array, } } </script> <style scoped > h2.word-title { margin: 40px 0 0; } ul.word-ul { list-style-type: none; padding: 10; } li.word-li-entry { margin: 10px; } q.word-definition { font-style: italic; } </style> services/WordDataService.js The data service for this Word app will import the SUPABASE_CLIENT , call the words database and return the id , word and definition showing the lasted word on top. This is the place you should add more functionality if you also want to insert and update data from within the App but that is not the scope of this project. import SUPABASE_CLIENT from \"../config\" class WordDataService { getAll () { return SUPABASE_CLIENT . from ( \"Words\" ) . select ( \"id,word,definition\" ) . order ( 'created_at' , { ascending : false }) } } export default new WordDataService (); config.js The configuration file uses the supabase package to create the SUPABASE_CLIENT based on the two environment variables. Make sure to prepend the variables with VUE_APP_ to make them available inside the Vue application. I use the a .env file in the root of the Vue app to define these environment variables to keep things clean. import { createClient } from \"@supabase/supabase-js\" const SUPABASE_KEY = process . env . VUE_APP_SUPABASE_KEY ; const SUPABASE_URL = process . env . VUE_APP_SUPABASE_URL ; let SUPABASE_CLIENT = null ; try { SUPABASE_CLIENT = createClient ( SUPABASE_URL , SUPABASE_KEY ); } catch ( e ) { alert ( e . message + \" See config.js\" ); } export default SUPABASE_CLIENT ; main.js This file hasn't been modified and has the standard code. import { createApp } from 'vue' import App from './App.vue' createApp ( App ). mount ( '#app' ) Check the repository for my final code for this project. Auto deployment In my previous article I mentioned I moved my personal Pelican blog from Github Pages to Cloudflare pages . As you can expect, Cloudflare Pages has support for VueJS as well. Following the same steps to grant access to my repository and setting up the build was all I need to do to get this app live. Since this is a VueJS application the default build command is npm run build and the output is written to the /dist folder. Apart from setting up the build (with the defaults) I had to add the two environment variables for the Supabase client. Without setting these the app will start but won't load any content since it cannot connect to the Supabase database. Of course I had to add a custom domain to make it easier to remember this app. Final result And this is the final result, a simple page with the random words I have used for testing words from a summary of Metamorphosis and the Supabase homepage.", "tags": "posts", "url": "creating-quick-app-with-supabase.html", "loc": "creating-quick-app-with-supabase.html" }, { "title": "Moving blog to Cloudflare pages", "text": "With all the new ways of deploying your static websites without hosting your own servers I thought it was a good time for me to move away from Github Pages. For my Github hosted website I was using two repositories of which one is for the Pelican source data (Markdown and Jupyter notebooks) and one for the HTML output. Using one of these new services I do not need to worry about the output folder since the build of the HTML files will be done on the server and is not stored in your repository. I did a quick test with the famous Netlify which worked really well but it was slightly more difficult to setup a domain name. Because I am already using Cloudflare for my DNS it made sense to me to try out Cloudflare Pages to easily link my domain name and avoid having two repositories for one website. Setting up is easy and just requires you to login to your Github account and give access to the repository you want to deploy on Cloudflare Pages. Once you grant the access you set the Production branch and Build configuration . Default for a Pelican blog the build command is make html which will output the HTML files to the output folder. The output folder will be the root of the website that is hosted on Cloudflare Pages. Also the domain name change is very simple. After filling in www.jitsejan.com it knows that the DNS is with Cloudflare already and needs to point to the pages.dev domain instead of the github.io domain. Lastly, I want to point out a few small issues I faced during the migration. First of all, I had to make sure to add a runtime.txt to the root of my website repository to use Python 3.7. Currently Cloudflare Pages only support Python 3.5 and 3.7, running any other version will give errors during the build. Additionally, I had to add the plugins folder through a Git module to enable the plugins on the server as well. After running git submodule add https://github.com/getpelican/pelican-plugins plugins this all worked fine. Overall it was a very smooth experience to setup the website on Cloudflare Pages. Builds are fast and I have reduced my clutter.", "tags": "posts", "url": "moving-blog-to-cloudflare-pages.html", "loc": "moving-blog-to-cloudflare-pages.html" }, { "title": "Assume a role with boto3 in Python", "text": "The following script will use the jjmain profile defined in ~/.aws/credentials and retrieve a session token after a MFA token is provided. With the session token the admin-role can be assumed and data from the AWS Cost Explorer is retrieved. from datetime import datetime , timedelta import boto3 ACCOUNT_NUMBER = 123456789 MAX_DURATION = 129600 NUM_DAYS = 30 USER = \"jitse-jan\" # Prompt user for the MFA token token = input ( \"MFA token: \" ) # Get the credentials to assume the role session = boto3 . session . Session ( profile_name = \"jjmain\" ) sts_client = session . client ( \"sts\" ) assumed_role_object = sts_client . get_session_token ( DurationSeconds = MAX_DURATION , SerialNumber = f \"arn:aws:iam:: { ACCOUNT_NUMBER } :mfa/ { USER } \" , TokenCode = token , ) credentials = assumed_role_object [ \"Credentials\" ] # Setup the Cost Explorer client ce_client = session . client ( \"ce\" , aws_access_key_id = credentials [ \"AccessKeyId\" ], aws_secret_access_key = credentials [ \"SecretAccessKey\" ], aws_session_token = credentials [ \"SessionToken\" ], ) # Get the cost and usage for the provided time period now = datetime . utcnow () . now () start = ( now - timedelta ( days = NUM_DAYS )) . strftime ( \"%Y-%m- %d \" ) end = now . strftime ( \"%Y-%m- %d \" ) data = ce_client . get_cost_and_usage ( TimePeriod = { \"Start\" : start , \"End\" : end }, Granularity = \"MONTHLY\" , Metrics = [ \"UnblendedCost\" ], ) # Get the total unblended cost total_cost = 0 for timeperiod in data [ \"ResultsByTime\" ]: total_cost += float (( timeperiod [ \"Total\" ][ \"UnblendedCost\" ][ \"Amount\" ])) print ( f \"Total unblended cost in the past { NUM_DAYS } days is { total_cost : .2f } USD\" )", "tags": "posts", "url": "assume-role-with-boto3-in-python.html", "loc": "assume-role-with-boto3-in-python.html" }, { "title": "Setting up a private PyPi server", "text": "It has been quite a few years that I have been working with Python but I never took the time to take a deeper look into how to package my code. To be fair, I never really had the use case and the following tutorial is purely a simple introduction on how to create your package and deploy it to your private server. I have used the complex way of packaging as described on python.org in a different project (maybe a future article?) and found it to be pretty tedious so I looked for a different method. A popular way of creating and deploying packages is by using poetry that can take care of your virtual environments and has a built-in mechanism to publish your packages to PyPI . Prerequisites Pyenv - for managing Python versions. Note: I am aware that pyenv also supports virtual environments but I prefer Poetry to manage them. Poetry - for virtual environments and packaging. Pyenv First of all, I make sure I have pyenv installed on my server to manage the Python versions. Since I will host the PyPi server on my Ubuntu VPS ( theviji ) I use the pyenv-installer as suggested in the docs. jitsejan@theviji:~$ curl https://pyenv.run | bash Update the ~/.zshrc by exporting the environment variables and making pyenv available: jitsejan@theviji:~$ echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.zshrc jitsejan@theviji:~$ echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.zshrc jitsejan@theviji:~$ echo -e 'if command -v pyenv 1>/dev/null 2>&1; then\\n eval \"$(pyenv init -)\"\\nfi' >> ~/.zshrc Reload the ~/.zshrc and verify the version of pyenv . At the time of writing the latest version is 1.2.23. jitsejan@theviji:~$ source ~/.zshrc jitsejan@theviji:~$ pyenv --version pyenv 1 .2.23 Because of Linux build problems as mentioned on the PyEnv Wiki I had to install the following packages to make pyenv work on Ubuntu. jitsejan@theviji:~$ sudo apt-get install -y \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ wget \\ curl \\ llvm \\ libncurses5-dev \\ libncursesw5-dev \\ xz-utils \\ tk-dev \\ libffi-dev \\ liblzma-dev \\ python-openssl \\ git Finally, I'll install the latest Python version which is 3.9.2 at the time of writing and make it the default Python version using the pyenv global command so it will be used automatically in a virtual environment. jitsejan@theviji:~$ pyenv install 3 .9.2 jitsejan@theviji:~$ pyenv versions * system ( set by /home/jitsejan/.pyenv/version ) 3 .9.2 jitsejan@theviji:~$ pyenv global 3 .9.2 jitsejan@theviji:~$ pyenv versions system * 3 .9.2 ( set by /home/jitsejan/.pyenv/version ) Poetry Poetry will be installed for managing my virtual environments and packaging the Python repositories to be published to my private server. jitsejan@theviji:~$ curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -jitsejan@theviji:~$ source $HOME /.poetry/env jitsejan@theviji:~$ poetry --version Poetry version 1 .1.5 And for shell completion I will add the following command to my zshell configuration. This makes it easier to work with poetry on the command line. jitsejan@theviji:~$ poetry completions zsh > $ZSH_CUSTOM /plugins/poetry/_poetry We are all set! Setup PyPi server Setting up the repository The first step is to create a folder and initialize a Poetry package inside of it. I am using Python 3.9 and fill in the rest of the metadata. jitsejan@theviji:~$ mkdir ~/python-packages jitsejan@theviji:~$ cd $_ jitsejan@theviji:~/python-packages$ poetry init This command will guide you through creating your pyproject.toml config. ... jitsejan@theviji:~/python-packages$ cat pyproject.toml [ tool.poetry ] name = \"python-packages\" version = \"0.1.0\" description = \"This project contains my Python packages.\" authors = [ \"Jitse-Jan\" ] license = \"BSD\" [ tool.poetry.dependencies ] python = \"&#94;3.9\" [ tool.poetry.dev-dependencies ] [ build-system ] requires = [ \"poetry-core>=1.0.0\" ] build-backend = \"poetry.core.masonry.api\" Installing the package To host the Python packages I will be using pypiserver which is one of the recommended and easy ways to host your own PyPI server. For this project I will be using 1.4.2. It supports different backends but I will simply use the VPS as my storage to host the packages. jitsejan@theviji:~/python-packages$ poetry add pypiserver Creating virtualenv python-packages-nJx9lSAW-py3.9 in /home/jitsejan/.cache/pypoetry/virtualenvs Using version &#94;1.4.2 for pypiserver Updating dependencies Resolving dependencies... ( 0 .2s ) Writing lock file Package operations: 1 install, 0 updates, 0 removals • Installing pypiserver ( 1 .4.2 ) Running the server In order to run the server I activate the shell and start the pypi-server on port 8082. This port is by default public so be careful what to publish. When running it in production make sure to take care of the security. jitsejan@theviji:~/python-packages$ poetry shell Spawning shell within /home/jitsejan/.cache/pypoetry/virtualenvs/python-packages-nJx9lSAW-py3.9 . /home/jitsejan/.cache/pypoetry/virtualenvs/python-packages-nJx9lSAW-py3.9/bin/activate jitsejan@theviji:~/python-packages$ . /home/jitsejan/.cache/pypoetry/virtualenvs/python-packages-nJx9lSAW-py3.9/bin/activate ( python-packages-nJx9lSAW-py3.9 ) jitsejan@theviji:~/python-packages$ pypi-server -p 8082 . Navigating to my website and checking the port shows the welcome page of the server. So far so good. Adding security The next step is add a password to the server so only authenticated users can publish packages. The recommended way is to install passlib and create the .htaccess file with the password. Now when we start the server we add the -P argument with the newly created .htaccess file. jitsejan@theviji:~$ sudo apt install apache2-utils jitsejan@theviji:~/python-packages$ poetry add passlib jitsejan@theviji:~/python-packages$ htpasswd -sc .htaccess pyjitsejan New password: Re-type new password: Adding password for user pyjitsejan jitsejan@theviji:~/python-packages$ pypi-server -p 8082 -P .htaccess Creating subdomain To make it cleaner and easier to connect to the server I will be using a subdomain instead of a public port to connect to the server. For this I will use the Nginx webserver. jitsejan@theviji:~/python-packages$ cd .. ~ ❯ sudo apt install nginx After installing nginx we can see the welcome page by going back to my domain. To use a subdomain with Nginx that points to the port I'll be adding the new subdomain pypi.jitsejan.com and create the configuration in the sites-available folder. jitsejan@theviji:~$ sudo touch /etc/nginx/sites-available/pypi.jitsejan.com The content of the file is as follows. It will listen to port 80 and redirect to port 8082 if the request name is pypi.jitsejan.com . upstream pypi { server 127 .0.0.1:8082 fail_timeout = 0 ; } server { listen 80 ; server_name pypi.jitsejan.com ; location / { proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-Proto https ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; add_header Pragma \"no-cache\" ; proxy_pass http://pypi ; } } The final step to make the domain available is by enabling the new configuration by creating a system link in the sites-enabled folder and restart the webserver. Now instead of hosting the PyPi server publically I enable it only for localhost and let Nginx take care of the redirection. The --overwrite argument enables me to overwrite existing packages, without this flag you will get an error trying to upload the same package twice. jitsejan@theviji:~$ sudo ln -s /etc/nginx/sites-available/pypi.jitsejan.com /etc/nginx/sites-enabled ~ ❯ sudo service nginx restart jitsejan@theviji:~/python-packages$ pypi-server -i 127 .0.0.1 -p 8082 -P .htaccess --overwrite . After creating a new entry in my DNS records at Cloudflare I am ready to verify my new subdomain. Enabling SSL The last thing I will add to the server is a certificate so we can get rid of the Not Secure warning in the address bar and actually use a secure HTTP connection. In order to do this I install certbot and the integration for Nginx. jitsejan@theviji:~$ sudo apt-get install certbot jitsejan@theviji:~$ sudo apt-get install python3-certbot-nginx jitsejan@theviji:~$ sudo certbot --nginx -d pypi.jitsejan.com Enable automatic refresh of the SSL certificate by adding a rule to your crontab. This will make sure the certbot recreates the SSL certificate before it expires. jitsejan@theviji:~$ crontab -e and add the following: 0 12 * * * /usr/bin/certbot renew --quiet Basic packaging Using Poetry I will create a simple package to try publishing a package to the server. Use poetry new to create a default package template. jitsejan@theviji:~/code$ poetry new hello-poetry Created package hello_poetry in hello-poetry jitsejan@theviji:~/code$ cd hello-poetry/ jitsejan@theviji:~/code/hello-poetry$ tree . . ├── hello_poetry │ └── __init__.py ├── pyproject.toml ├── README.rst └── tests ├── __init__.py └── test_hello_poetry.py 2 directories, 5 files As the dev repository I will use the server that I have just created before. Once I have configured the repo I can publish to dev . Poetry will pack the content in a TAR ball and publish it to the PyPI server. jitsejan@theviji:~/code/hello-poetry$ poetry config repositories.dev https://pypi.jitsejan.com jitsejan@theviji:~/code/hello-poetry$ poetry publish -r dev No suitable keyring backends were found Using a plaintext file to store and retrieve credentials Username: pyjitsejan Password: Publishing hello-poetry ( 0 .1.0 ) to dev - Uploading hello-poetry-0.1.0.tar.gz 100 % - Uploading hello_poetry-0.1.0-py3-none-any.whl 100 % Navigating to the Simple Index I can see the hello-poetry package is now available on the server. To avoid the need to type the username and password every time I want to push my package I set them through the poetry config . jitsejan@theviji:~/code/hello-poetry$ poetry config http-basic.dev pyjitsejan Sup3rS3cr3t£ Bump version As an additional test of packaging I will add a simple class to hello_poetry/hellopoetry.py inside my hello-poetry project. It does nothing more than printing Hello Poetry once it gets instantiated. class HelloPoetry : def __init__ ( self ): print ( \"Hello Poetry\" ) The structure of the repository now looks like this: jitsejan@theviji:~/code/hello-poetry $ tree . ├── hello_poetry │ ├── hellopoetry.py │ └── __init__.py ├── poetry.lock ├── pyproject.toml ├── README.md └── tests ├── __init__.py └── test_hello_poetry.py Using poetry version I bump the package version to 0.2.0 which will update the pyproject.toml file. With the new version in place I rerun the build step which will print it is adding the distribution for the new version. Finally, this new package gets published to the private PyPi server. jitsejan@theviji:~/code/hello-poetry$ poetry version 0 .2.0 Bumping version from 0 .1.0 to 0 .2.0 jitsejan@theviji:~/code/hello-poetry$ poetry build Building hello-poetry ( 0 .2.0 ) - Building sdist - Built hello-poetry-0.2.0.tar.gz - Building wheel - Built hello_poetry-0.2.0-py3-none-any.whl jitsejan@theviji:~/code/hello-poetry$ poetry publish -r dev Publishing hello-poetry ( 0 .2.0 ) to dev - Uploading hello-poetry-0.2.0.tar.gz 100 % - Uploading hello_poetry-0.2.0-py3-none-any.whl 100 % Installation Option 1 - Install using pip Option 1.1 - Specifying the argument via the CLI ❯ pip install --extra-index-url https://pypi.jitsejan.com/simple --trusted-host pypi.jitsejan.com hello-poetry Option 1.2 - Set the PypPi configuration Add the following to ~/.config/pip/pip.conf . [global] timeout = 60 extra-index-url = https://pypi.jitsejan.com/simple Now pip will look at the normal index as well as the custom index: ❯ pip install hello-poetry Looking in indexes: https://pypi.org/simple, https://pypi.jitsejan.com/simple ... Installing collected packages: hello-poetry Successfully installed hello-poetry-0.2.0 Option 2 - Install using poetry Assuming you are already in a poetry repository/environment you only need to add the extra source ( [[tool.poetry.source]] )to the project configuration pyproject.toml : [tool.poetry] name = \"testpypi\" version = \"0.1.0\" description = \"\" authors = [\"Jitse-Jan <code@jitsejan.com>\"] [tool.poetry.dependencies] python = \"&#94;3.9\" [tool.poetry.dev-dependencies] [[tool.poetry.source]] name = \"pyjitsejan\" url = \"https://pypi.jitsejan.com/simple/\" [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\" Now that the extra index is added installation of my package will succeed and add it to the project dependencies. ❯ poetry add hello-poetry Using version &#94;0.2.0 for hello-poetry Updating dependencies Resolving dependencies... ( 4 .1s ) Writing lock file Package operations: 6 installs, 0 updates, 0 removals • Installing certifi ( 2020 .12.5 ) • Installing chardet ( 4 .0.0 ) • Installing idna ( 2 .10 ) • Installing urllib3 ( 1 .26.4 ) • Installing requests ( 2 .25.1 ) • Installing hello-poetry ( 0 .2.0 ) Checking the pyproject.toml I can confirm it is now expecting at least version 0.2.0. [tool.poetry.dependencies] python = \"&#94;3.9\" hello-poetry = \"&#94;0.2.0\" Usage After installing the package through one of the above mentioned methods, or your own fancy way, it is time to check if the package is actually working. I have a simple test.py file that will import the class and create an instance. This should print Hello Poetry if all is correct. # test.py from hello_poetry.hellopoetry import HelloPoetry h = HelloPoetry () Running this outputs the expected string! ❯ python test.py Hello Poetry Note that I could probably do better with naming the folders given that inside my hello-poetry repository I have the hello_poetry folder containing the hellopoetry.py file with the HelloPoetry class. Using the bad naming convention it does make it easier where things are located. Obviously in a real package the naming would make more sense, i.e. from myconnectors.databaseconnector import DatabaseConnector . Please see my Github repository with the final code. Sources https://pip.pypa.io/en/stable/user_guide/ https://pypi.org/project/pypiserver/ https://github.com/pyenv/pyenv https://github.com/pyenv/pyenv-installer https://github.com/python-poetry/poetry https://www.nginx.com/blog/using-free-ssltls-certificates-from-lets-encrypt-with-nginx/ https://medium.com/@christianhettlage/setting-up-a-pypi-server-679f1b55b96 https://medium.com/lambda-automotive/python-poetry-finally-easy-build-and-deploy-packages-e1e84c23401f", "tags": "posts", "url": "setting-up-private-pypi-server.html", "loc": "setting-up-private-pypi-server.html" }, { "title": "Creating a simple REST API with FastAPI and SQLAlchemy", "text": "Introduction In my previous article I made a quick example explaining how to use Flask with SQLALchemy as REST API . Since then I have moved away from Flask and picked up FastAPI which makes it, in my opinion, much easier to create a REST API and it is even more performant. Throughout this tutorial I will use the same data as in the Flask article and assume the data is already stored in Postgres with the same account. The only difference is the code to get the API running. FastAPI framework, high performance, easy to learn, fast to code, ready for production Overview Database - connects to the Postgres database Models - describes the data model(s) CRUD - contains the Create, Read, Update and Delete actions which execute queries on the models against the database Schemas - defines the schemas of the inputs and outputs of the API Routers - explains the different routes of the API and what CRUD action to call Main - instantiates the FastAPI application and adds the routes from the routers Database The important bit is to have your database running and have some data to display. I will simply reuse the same connection that I have used for the Flask REST API which means I store my credentials as environment variables and construct the PostgreSQL connection string to connect to the database through SQLalchemy. The code is similar to the connection code for Flask, however, we can directly use SQLalchemy without an additional package whereas for Flask we had to use flask_sqlalchemy . This is one of the important arguments why people use FastAPI over Flask! database.py import os from sqlalchemy import create_engine # type: ignore from sqlalchemy.ext.declarative import declarative_base # type: ignore from sqlalchemy.orm import sessionmaker # type: ignore host = os . environ [ \"POSTGRES_HOST\" ] port = os . environ [ \"POSTGRES_PORT\" ] user = os . environ [ \"POSTGRES_USER\" ] password = os . environ [ \"POSTGRES_PASS\" ] db = os . environ [ \"POSTGRES_DB\" ] dbtype = \"postgresql\" SQLALCHEMY_DATABASE_URI = f \" { dbtype } :// { user } : { password } @ { host } : { port } / { db } \" engine = create_engine ( SQLALCHEMY_DATABASE_URI ) SessionLocal = sessionmaker ( autocommit = False , autoflush = False , bind = engine ) Base = declarative_base () The # type: ignore comment is to avoid running into errors with the mypy typechecker. Models Secondly, I create the data model for the application now that I have a database connection. This will be identical to the one that we have used for the Flask app in the other article. The model only contains one table items with a name and a price column. Note that because FastAPI is all driven by typehinting and Pydantic modelling I did add a TypedDict as return value for the serialize method. Using this type I can indicate that I expect a dictionary back that contains the key name and the key price and their respective types. Obviously this is identical to the database model a few lines below. As mentioned in the previous paragraph, we do not need extra packages like flask_sqlalchemy and connect directly to SQLalchemy which makes the models here inherit from Base ( declarative_base() ) rather than the flask_sqlalchemy SQLalchemy model. models.py from typing import TypedDict from sqlalchemy import Column , Integer , String # type: ignore from .database import Base class ItemDict ( TypedDict ): name : str price : int class Item ( Base ): \"\"\" Defines the items model \"\"\" __tablename__ = \"items\" name = Column ( String , primary_key = True ) price = Column ( Integer ) def __init__ ( self , name : str , price : int ): self . name = name self . price = price def __repr__ ( self ) -> str : return f \"<Item { self . name } >\" @property def serialize ( self ) -> ItemDict : \"\"\" Return item in serializeable format \"\"\" return { \"name\" : self . name , \"price\" : self . price } CRUD The CRUD element of the API is kept unchanged and still only contains a function to get a single item by name or get all the items. These actions were in one single file in the Flask app where the route that was called directly executed the query against the database but for FastAPI it is common to seperate the routes from the database actions. Again I am using type hinting to indicate that the functions return a single Item or a list of Item s respectively. crud.py from typing import List from sqlalchemy.orm import Session # type: ignore from .models import Item def get_item_by_name ( session : Session , name : str ) -> Item : return session . query ( Item ) . filter ( Item . name == name ) . first () def get_items ( session : Session , skip : int = 0 , limit : int = 100 ) -> List [ Item ]: return session . query ( Item ) . offset ( skip ) . limit ( limit ) . all () Schemas As mentioned before, FastAPI heavily relies on pydantic for enforcing schemas for all the in- and outputs that are used in the API. There are more complex examples in their documentation where the input model (i.e. ItemInputModel ) contains only a name and the return model (i.e. ItemOutputModel ) consists of the name, the attributes and the particular ID. This file would be the place to define that and the schemas will be referred to in the main.py where the routes are defined. This particular example is kept (too) simple and the ItemBase contains a name and a price . This schema is identical to the return value that I defined before in the models.py but instead of pydantic I used the lower level typing library. schemas.py from pydantic import BaseModel class ItemBase ( BaseModel ): name : str price : int Routers The routers file will connect the endpoint the user is calling to the corresponding CRUD action. As explained in the CRUD paragraph there are two actions we want to execute, one for getting an item and one for getting all the items. The route will call the right CRUD method and pass the database session to it. It uses the schemas to define the response_model for each route which helps to build the automated docs that FastAPI is also famous for. Note that for both API endpoints I serialize the output to make sure that the returned value is a dictionary (or a list of dictionaries) which match the ItemBase schema definition. routers.py from typing import List from fastapi import APIRouter , Depends , HTTPException from sqlalchemy.orm import Session # type: ignore from . import crud , models from .database import SessionLocal , engine from .schemas import ItemBase models . Base . metadata . create_all ( bind = engine ) itemrouter = APIRouter () def get_session (): session = SessionLocal () try : yield session finally : session . close () @itemrouter . get ( \"/items/\" , response_model = List [ ItemBase ]) def read_items ( skip : int = 0 , limit : int = 100 , session : Session = Depends ( get_session ) ): items = crud . get_items ( session = session , skip = skip , limit = limit ) return [ i . serialize for i in items ] @itemrouter . get ( \"/items/ {name} \" , response_model = ItemBase ) def read_item ( name : str , session : Session = Depends ( get_session )): item = crud . get_item_by_name ( session = session , name = name ) if item is None : raise HTTPException ( status_code = 404 , detail = \"Item not found\" ) return item . serialize Main The main file obviously glues all the above together but since it is all nicely split in small files its content is kept very short. It initiates a FastAPI app (equal to the Flask app) and add the itemrouter that we defined in routers.py . main.py from fastapi import FastAPI from .routers import itemrouter app = FastAPI () app . include_router ( itemrouter ) Execution At this stage I have the following structure: ❯ tree fastapiapp ├── __init__.py ├── crud.py ├── database.py ├── main.py ├── models.py ├── routers.py └── schemas.py Now to run the app I use the following command (make sure to move up from the app folder): ❯ uvicorn fastapiapp.main:app --reload --workers 1 --host 0 .0.0.0 --port 8001 INFO: Uvicorn running on http://0.0.0.0:8001 ( Press CTRL+C to quit ) INFO: Started reloader process [ 84872 ] using statreload INFO: Started server process [ 84874 ] INFO: Waiting for application startup. INFO: Application startup complete. This will use the asynchronous web server uvicorn to kick off the main app on port 8001 open to the public (0.0.0.0). It will reload automatically here because I was debugging the app but this should of course be avoided. I only use a single worker because that should be enough for me hitting F5 to see the changes. Verification Now that the app is running verify that the docs are working. Navigate to localhost: /docs (8001 in this case) and this will show the documentation for the API. Indeed it shows my two endpoints! The next step is to actually test one of the endpoints which can be done through the docs page by expanding one of the two endpoints and click on \"Try it out\". Scroll down and hit Execute to see all the items returned in a JSON format. If you don't like user interfaces or documentation you can simply use HTTPie to call the items endpoint. Default it will use GET and I have set the limit to 3 and the skip to 2. ❯ http localhost:8001/items/ \\? limit \\= 3 \\& skip \\= 2 HTTP/1.1 200 OK content-length: 101 content-type: application/json date: Sat, 27 Feb 2021 02 :22:32 GMT server: uvicorn [ { \"name\" : \"Blue Ring\" , \"price\" : 250 } , { \"name\" : \"Red Water of Life\" , \"price\" : 68 } , { \"name\" : \"Food\" , \"price\" : 60 } ] The final code can be found here . Sources https://fastapi.tiangolo.com/tutorial/sql-databases/", "tags": "posts", "url": "creating-simple-rest-api-with-fastapi-and-sqlalchemy.html", "loc": "creating-simple-rest-api-with-fastapi-and-sqlalchemy.html" }, { "title": "Creating a simple REST API with Flask and SQLAlchemy", "text": "Introduction In my earlier posts I have discussed how to install Postgres , make it work with Python and create a simple scraper to pull data from a website and add it to a Postgres table. In this quick post I make a very straightforward API that only supports two actions: Retrieve all items Retrieve a single item I am not planning to update the API to support full CRUD but want to show how easy it is to create a Flask API on top of Postgres. This post is a very basic example that could be a minimal setup for your own project. Note that there is no security or authorization at all (which should be okay when you run it locally). Requirements Install the following three packages. I often use pipenv but feel free to use your own environment. The first two are essential packages to get Flask working with SQLAlchemy. The latter is a package to add a config file to your Flask folder to make it easier to run your application. $ pipenv install flask-sqlalchemy flask-Migrate python-dotenv Implement the API A .env file is used to set some configuration items for the Flask application. Create the .flaskenv in your Flask folder and define the application, if debug mode is enabled, what environment you are running this and optionally a different port to expose your API. Read more about the dotenv in the Flask docs . .flaskenv FLASK_APP = app FLASK_DEBUG = True FLASK_ENV = development FLASK_RUN_PORT = 5055 As you might know if you've worked with Flask before the app.py contains the main application that creates the API (or other backend). Since I keep this API as simple as possible I will only use the ItemsModel as defined below this file. The Postgres details are stored as environment variables to avoid exposing them in the code. After initializing the Flask app and configuring the link to my Postgres database I setup the database and the migration. Finally I setup two routes as explained before, one to expose all the items and one to get details for a single item. The API only supports GET and will error if anything else is done with the API. I know the code can still be tidier by moving the database to a different file, or moving the routes to another file but for simplicity I keep it all in a single file. app.py import os from flask import Flask , jsonify , request from flask_migrate import Migrate from flask_sqlalchemy import SQLAlchemy from models import ItemsModel host = os . environ [ \"POSTGRES_HOST\" ] port = os . environ [ \"POSTGRES_PORT\" ] username = os . environ [ \"POSTGRES_USER\" ] password = os . environ [ \"POSTGRES_PASS\" ] database = os . environ [ \"POSTGRES_DB\" ] app = Flask ( __name__ ) app . config [ \"SQLALCHEMY_DATABASE_URI\" ] = f \"postgresql:// { username } : { password } @ { host } : { port } / { database } \" app . config [ \"SQLALCHEMY_TRACK_MODIFICATIONS\" ] = False db = SQLAlchemy ( app ) migrate = Migrate ( app , db ) @app . route ( \"/items\" , methods = [ \"GET\" ]) def handle_items (): if request . method == \"GET\" : items = ItemsModel . query . all () return jsonify ([ item . serialize for item in items ]) else : return { \"message\" : \"failure\" } @app . route ( \"/items/<item_name>\" , methods = [ \"GET\" ]) def handle_item ( item_name ): if request . method == \"GET\" : try : item = ItemsModel . query . filter_by ( name = item_name ) . first_or_404 () return jsonify ( item . serialize ) except : return jsonify ({ \"error\" : f \"Item { item_name } not found\" }) else : return { \"message\" : \"Request method not implemented\" } if __name__ == \"__main__\" : app . run ( debug = True ) The final file that is needed to run the API is the models.py that defines the model(s) I will use. As I have described before and shown in the previous articles, I have added a set of items to the database with only two properties (name and price). This will be the data that I return through my API. The ItemsModel looks very similar to the model I have defined for the scraper which makes sense since it is both build using SQLAlchemy. models.py from flask_sqlalchemy import SQLAlchemy db = SQLAlchemy () class ItemsModel ( db . Model ): \"\"\" Defines the items model \"\"\" __tablename__ = \"items\" name = db . Column ( \"name\" , db . String , primary_key = True ) price = db . Column ( \"price\" , db . Integer ) def __init__ ( self , name , price ): self . name = name self . price = price def __repr__ ( self ): return f \"<Item { self . name } >\" @property def serialize ( self ): \"\"\" Return item in serializeable format \"\"\" return { \"name\" : self . name , \"price\" : self . price } Run the API Since I have used the .flaskenv file Flask knows the application file, the port to use and the other details. Run the API using flask run in the API folder. ~/rest-api $ flask run * Serving Flask app \"app\" ( lazy loading ) * Environment: development * Debug mode: on * Running on http://127.0.0.1:5055/ ( Press CTRL+C to quit ) * Restarting with stat * Debugger is active! * Debugger PIN: 999 -994-971 Verify the API Using httpie we can easily validate the API by calling the items endpoint: $ http get localhost:5000/items HTTP/1.0 200 OK Content-Length: 329 Content-Type: application/json Date: Wed, 06 Jan 2021 20 :22:10 GMT Server: Werkzeug/1.0.1 Python/3.9.1 [ { \"name\" : \"Boomerang\" , \"price\" : 300 } , { \"name\" : \"Heart Container\" , \"price\" : 4 } , { \"name\" : \"Blue Ring\" , \"price\" : 250 } , { \"name\" : \"Red Water of Life\" , \"price\" : 68 } , { \"name\" : \"Food\" , \"price\" : 60 } , { \"name\" : \"Blue Water of Life\" , \"price\" : 40 } , { \"name\" : \"Blue Candle\" , \"price\" : 60 } , { \"name\" : \"Arrow\" , \"price\" : 80 } , { \"name\" : \"Bow\" , \"price\" : 980 } , { \"name\" : \"Bomb\" , \"price\" : 20 } ] Also the endpoint for a single item seems to work fine too as shown below. $ http get localhost:5000/items/Bomb HTTP/1.0 200 OK Content-Length: 27 Content-Type: application/json Date: Wed, 06 Jan 2021 20 :22:32 GMT Server: Werkzeug/1.0.1 Python/3.9.1 { \"name\" : \"Bomb\" , \"price\" : 20 } The final code can be found on Github .", "tags": "posts", "url": "creating-simple-rest-api-with-flask-and-sqlalchemy.html", "loc": "creating-simple-rest-api-with-flask-and-sqlalchemy.html" }, { "title": "Scraping data with Scrapy and PostgreSQL", "text": "Introduction In the following tutorial I will use Scrapy to retrieve the items in The Legend of Zelda from Gamepedia . I will focus on those items that have a name and a cost and add them to the database. In this post I have installed PostgreSQL on my VPS and configured it to work with Python. These items will help in the next project where I will use them to create a simple REST API with Flask. Objective Retrieve data from website using Scrapy. Store results in a PostgreSQL database. Prerequisites pipenv installed (or any other Python virtual environment tool). Scrapy , sqlalchemy and psycopg2 installed in the environment. PostgreSQL installed. $ pipenv --python 3 $ pipenv shell data-retrieval $ python --version Python 3 .9.1 data-retrieval $ pipenv install Scrapy data-retrieval $ pip freeze | grep Scrapy Scrapy == 2 .4.1 data-retrieval $ pip freeze | grep SQLAlchemy SQLAlchemy == 1 .3.22 data-retrieval $ pip freeze | grep psycopg2 psycopg2 == 2 .8.6 Initialize Scrapy Start a new Scrapy project inside your new Python project. I picked the creative name crawl for the Scrapy project. data-retrieval $ scrapy startproject crawl New Scrapy project 'crawl' , using template directory '/Users/jitsejan/.local/share/virtualenvs/testscrapy-jJKHMw2I/lib/python3.9/site-packages/scrapy/templates/project' , created in : /Users/jitsejan/code/testscrapy/data-retrieval/crawl You can start your first spider with: cd crawl scrapy genspider example example.com Running the startproject command will create a folder with the structure outlined below. There is a top folder with the project name ( crawl ) that contains the Scrapy configuration and a subfolder with the same name containing the actual crawling code. NB: I don't want to go into too much detail about Scrapy because there are many tutorials for the tool online, and because I normally use requests with lxml to make (very simple) data crawlers. Many people prefer to use BeautifulSoup or other higher level data crawl libraries so feel free to go for that. I picked Scrapy in this particular case because it creates a nice scaffold when working with crawlers and databases but this can be completely done from scratch as well. data-retrieval $ tree crawl crawl ├── crawl │ ├── __init__.py │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ └── __init__.py └── scrapy.cfg 2 directories, 7 files You could choose to not use the generator and write the Scrapy files yourself but for simplicity I use the boilerplate that comes with Scrapy. Now navigate to the top level project folder and create the spider ( crawler ) using genspider . In my case I will be crawling data from Gamepedia.com about items that were found in the Zelda games. A completely random choice of website but it fits with the theme I normally use when playing around with code. data-retrieval $ cd crawl data-retrieval/crawl $ scrapy genspider zelda_items zelda.gamepedia.com Created spider 'zelda_items' using template 'basic' in module: crawl.spiders.zelda_items If we look again at the tree structure we see that inside the spiders folder a new file ( zelda_items.py ) has been created. This file creates the basic structure for a spider. data-retrieval/crawl $ tree . ├── crawl │ ├── __init__.py │ ├── items.py │ ├── middlewares.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── zelda_items.py └── scrapy.cfg The content of the zelda_items.py file is the minimal setup to get started with crawling data. import scrapy class ZeldaItemsSpider ( scrapy . Spider ): name = 'zelda_items' allowed_domains = [ 'zelda.gamepedia.com' ] start_urls = [ 'http://zelda.gamepedia.com/' ] def parse ( self , response ): pass Setup the scraper The first element we want to crawl is the link to all the items to get the detailed information for each of them. The code to retrieve these links is added to the parse function. Looking at the source code it is easy to copy the selector to get to the right element. The selector returns the following information. #mw-content-text > div > center:nth-child(1) > ul > li:nth-child(2) > div > div.gallerytext > p > a As a selector I will use the CSS selector li.gallerybox .gallerytext p a::attr(href) to get the hyperlinks to all the items. def parse ( self , response ): \"\"\" Retrieve the links to the items \"\"\" selector = \"li.gallerybox .gallerytext p a::attr(href)\" for href in response . css ( selector ) . extract (): yield Request ( f \" { self . base_url }{ href } \" , callback = self . parse_item ) For each link that is retrieved the parse_item function is executed. The structure of the information page is a little ugly but we only care about the information table. Since this is a Wiki the data can be very unstructured and not all the tables will have the same fields which should be taken into account when retrieving data. For my particular case I want to retrieve the name and the price of each item (under the condition that the table contains the cost information). The following shows a simplified version of the HTML from which I am extracting the data. < table class = \"infobox wikitable\" > < tbody > < tr > < th class = \"infobox-name centered\" colspan = \"2\" >< span class = \"term\" > Arrow </ span ></ th > </ tr > < tr > < td class = \"infobox-image centered\" colspan = \"2\" > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Main appearance(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Other appearance(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Cost(s) </ th > < td > < div class = \"infobox-field-content\" > 80 < a href = \"/Rupee\" title = \"Rupee\" > Rupees </ a > < sup > ( < b >< span title = \"The Legend of Zelda\" > TLoZ </ span ></ b > ) </ sup > </ div > </ td > </ tr > < tr class = \"infobox-field\" > < th > Location(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Use(s) </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Strength </ th > < td > ... </ td > </ tr > < tr class = \"infobox-field\" > < th > Comparable item(s) </ th > < td > ... </ td > </ tr > </ tbody > </ table > To fetch the data I will again use a CSS selector for the name but the xpath selector to find the right element in the table for the cost. I explained in this earlier article how I figured out the right syntax to get the information. Only when the price returns an integer I will return the item. If there is no price I will ignore the item. In the future this code will most probably change to retrieve more items and different fields. def parse_item ( self , response ): \"\"\" Retrieve the item details \"\"\" name_sel = \"meta[property='og:title']::attr(content)\" price_sel = \"//tr[th//text()[contains(., 'Cost(s)')]]/td/div/text()\" name = response . css ( name_sel ) . get () price = response . xpath ( price_sel ) . get () if price and price . strip () . isdigit (): yield { \"name\" : name , \"price\" : int ( price )} Crawl the data Now the scraper is ready to be executed and retrieve the items. Run the crawler and verify that it is returning indeed the items that you would expect. There is no output that stores the items yet but the log tells me that there were 10 items that actually had a name and the cost defined ( 'item_scraped_count': 10, ). Note that I set the loglevel to INFO to prevent an information overload in the console. data-retrieval/crawl $ scrapy crawl zelda_items 2021 -01-05 21 :26:07 [ scrapy.utils.log ] INFO: Scrapy 2 .4.1 started ( bot: crawl ) 2021 -01-05 21 :26:07 [ scrapy.utils.log ] INFO: Versions: lxml 4 .6.2.0, libxml2 2 .9.10, cssselect 1 .1.0, parsel 1 .6.0, w3lib 1 .22.0, Twisted 20 .3.0, Python 3 .9.1 ( default, Dec 17 2020 , 03 :41:37 ) - [ Clang 12 .0.0 ( clang-1200.0.32.27 )] , pyOpenSSL 20 .0.1 ( OpenSSL 1 .1.1i 8 Dec 2020 ) , cryptography 3 .3.1, Platform macOS-10.15.7-x86_64-i386-64bit 2021 -01-05 21 :26:07 [ scrapy.crawler ] INFO: Overridden settings: { 'BOT_NAME' : 'crawl' , 'LOG_LEVEL' : 'INFO' , 'NEWSPIDER_MODULE' : 'crawl.spiders' , 'ROBOTSTXT_OBEY' : True, 'SPIDER_MODULES' : [ 'crawl.spiders' ]} 2021 -01-05 21 :26:07 [ scrapy.extensions.telnet ] INFO: Telnet Password: 7313d2472beec312 2021 -01-05 21 :26:07 [ scrapy.middleware ] INFO: Enabled extensions: [ 'scrapy.extensions.corestats.CoreStats' , 'scrapy.extensions.telnet.TelnetConsole' , 'scrapy.extensions.memusage.MemoryUsage' , 'scrapy.extensions.logstats.LogStats' ] 2021 -01-05 21 :26:07 [ scrapy.middleware ] INFO: Enabled downloader middlewares: [ 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware' , 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware' , 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware' , 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware' , 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' , 'scrapy.downloadermiddlewares.retry.RetryMiddleware' , 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware' , 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware' , 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware' , 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware' , 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware' , 'scrapy.downloadermiddlewares.stats.DownloaderStats' ] 2021 -01-05 21 :26:07 [ scrapy.middleware ] INFO: Enabled spider middlewares: [ 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware' , 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware' , 'scrapy.spidermiddlewares.referer.RefererMiddleware' , 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware' , 'scrapy.spidermiddlewares.depth.DepthMiddleware' ] 2021 -01-05 21 :26:09 [ scrapy.middleware ] INFO: Enabled item pipelines: [ 'crawl.pipelines.CrawlPipeline' ] 2021 -01-05 21 :26:09 [ scrapy.core.engine ] INFO: Spider opened 2021 -01-05 21 :26:09 [ scrapy.extensions.logstats ] INFO: Crawled 0 pages ( at 0 pages/min ) , scraped 0 items ( at 0 items/min ) 2021 -01-05 21 :26:09 [ scrapy.extensions.telnet ] INFO: Telnet console listening on 127 .0.0.1:6023 2021 -01-05 21 :26:13 [ scrapy.core.engine ] INFO: Closing spider ( finished ) 2021 -01-05 21 :26:13 [ scrapy.statscollectors ] INFO: Dumping Scrapy stats: { 'downloader/request_bytes' : 22501 , 'downloader/request_count' : 75 , 'downloader/request_method_count/GET' : 75 , 'downloader/response_bytes' : 1941159 , 'downloader/response_count' : 75 , 'downloader/response_status_count/200' : 37 , 'downloader/response_status_count/301' : 38 , 'dupefilter/filtered' : 1 , 'elapsed_time_seconds' : 4 .146036, 'finish_reason' : 'finished' , 'finish_time' : datetime.datetime ( 2021 , 1 , 5 , 21 , 26 , 13 , 283001 ) , 'item_scraped_count' : 10 , 'log_count/INFO' : 10 , 'memusage/max' : 73228288 , 'memusage/startup' : 73224192 , 'request_depth_max' : 1 , 'response_received_count' : 37 , 'robotstxt/request_count' : 1 , 'robotstxt/response_count' : 1 , 'robotstxt/response_status_count/200' : 1 , 'scheduler/dequeued' : 73 , 'scheduler/dequeued/memory' : 73 , 'scheduler/enqueued' : 73 , 'scheduler/enqueued/memory' : 73 , 'start_time' : datetime.datetime ( 2021 , 1 , 5 , 21 , 26 , 9 , 136965 )} 2021 -01-05 21 :26:13 [ scrapy.core.engine ] INFO: Spider closed ( finished ) Store the data First of all I define the schema of the element that I am crawling in the items.py . There is no fancy schema yet but this can obviously be improved in the future when more items are being retrieved and the actual datatypes do make a difference. \"\"\"crawl/crawl/items.py\"\"\" from scrapy import Field , Item class ZeldaItem ( Item ): \"\"\" Definition of the ZeldaItem \"\"\" name = Field () price = Field () The middlewares.py is left untouched for the project. The important bit for storing data in a database is inside models.py . As described before I use SQLAlchemy to connect to the PostgreSQL database. The database details are stored in settings.py (see below) and are used to create the SQLAlchemy engine . I define the Items model with the two fields and use the create_items_table to create the table. \"\"\"crawl/crawl/models.py\"\"\" from sqlalchemy import Column , Integer , String , create_engine from sqlalchemy.engine.base import Engine from sqlalchemy.engine.url import URL from sqlalchemy.ext.declarative import declarative_base from crawl import settings DeclarativeBase = declarative_base () def db_connect () -> Engine : \"\"\" Creates database connection using database settings from settings.py. Returns sqlalchemy engine instance \"\"\" return create_engine ( URL ( ** settings . DATABASE )) def create_items_table ( engine : Engine ): \"\"\" Create the Items table \"\"\" DeclarativeBase . metadata . create_all ( engine ) class Items ( DeclarativeBase ): \"\"\" Defines the items model \"\"\" __tablename__ = \"items\" name = Column ( \"name\" , String , primary_key = True ) price = Column ( \"price\" , Integer ) Inside the pipelines.py the spider is connected to the database. When the pipeline is started it will initalize the database and create the engine , create the table and setup a SQLAlchemy session. The process_item function is part of the default code and is executed for every yielded item in the scraper. In this case it means it will be triggered every time an item is retrieved with a name and a cost. For every item it is first checked if the item already exists in the database and in case is does not exist yet it will be added to the database. Remember to always commit() when adding (or removing) items to the table. \"\"\"crawl/crawl/pipelines.py\"\"\" from sqlalchemy.orm import sessionmaker from crawl.models import Items , create_items_table , db_connect class CrawlPipeline : def __init__ ( self ): \"\"\" Initializes database connection and sessionmaker. Creates items table. \"\"\" engine = db_connect () create_items_table ( engine ) self . Session = sessionmaker ( bind = engine ) def process_item ( self , item , spider ): \"\"\" Process the item and store to database. \"\"\" session = self . Session () instance = session . query ( Items ) . filter_by ( ** item ) . one_or_none () if instance : return instance zelda_item = Items ( ** item ) try : session . add ( zelda_item ) session . commit () except : session . rollback () raise finally : session . close () return item Finally, the settings.py is short and contains the information for the crawler. The only items I have added are the DATABASE and LOG_LEVEL variables. You could choose to add your security details in this file but I would recommend to keep them secret and store them elsewhere. \"\"\"crawl/crawl/settings.py\"\"\" import os BOT_NAME = \"crawl\" SPIDER_MODULES = [ \"crawl.spiders\" ] NEWSPIDER_MODULE = \"crawl.spiders\" ROBOTSTXT_OBEY = True ITEM_PIPELINES = { \"crawl.pipelines.CrawlPipeline\" : 300 , } DATABASE = { \"drivername\" : \"postgres\" , \"host\" : os . environ [ \"POSTGRES_HOST\" ], \"port\" : os . environ [ \"POSTGRES_PORT\" ], \"username\" : os . environ [ \"POSTGRES_USER\" ], \"password\" : os . environ [ \"POSTGRES_PASS\" ], \"database\" : os . environ [ \"POSTGRES_DB\" ], } LOG_LEVEL = \"INFO\" Verify the data As a last step in this tutorial I will double check that there are indeed ten items in the database. I have used pandas as an easy way to query the database and add the results to a table. import os import pandas as pd import sqlalchemy from sqlalchemy import create_engine USER = os . environ [ 'POSTGRES_USER' ] PASS = os . environ [ 'POSTGRES_PASS' ] HOST = os . environ [ 'POSTGRES_HOST' ] PORT = os . environ [ 'POSTGRES_PORT' ] DB = os . environ [ 'POSTGRES_DB' ] db_string = f \"postgres:// { USER } : { PASS } @ { HOST } : { PORT } / { DB } \" engine = create_engine ( db_string ) df = pd . read_sql_query ( 'SELECT * FROM items' , con = engine ) print ( df . to_string ()) # name price # 0 Boomerang 300 # 1 Heart Container 4 # 2 Blue Ring 250 # 3 Red Water of Life 68 # 4 Food 60 # 5 Blue Water of Life 40 # 6 Blue Candle 60 # 7 Arrow 80 # 8 Bow 980 # 9 Bomb 20 See the Github repo for the final code. The code most probably will change once I (slowly) continue working on this side project but I hope it might help anyone playing with data crawling and databases.", "tags": "posts", "url": "scraping-with-scrapy-and-postgres.html", "loc": "scraping-with-scrapy-and-postgres.html" }, { "title": "Setting up PostgreSQL for Python", "text": "Objective Setup PostgreSQL on Ubuntu Setup Python to connect to PostgreSQL Setup PostgreSQL For this tutorial I will be installing PostgreSQL on my VPS running Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-58-generic x86_64) . Ubuntu Packages Install the following packages to get PostgreSQL and Python running. I assume Python is already installed on the machine. postgresql postgresql-client postgresql-client-common libpq-dev jitsejan@theviji:~$ sudo apt install postgresql \\ postgresql-client \\ postgresql-client-common \\ libpq-dev Verify PostgreSQL The PostgreSQL server version can be verified with the locate tool which returns version 12 : jitsejan@theviji:~$ locate bin/postgres /usr/lib/postgresql/12/bin/postgres The PostgreSQL client version can simply be checked using psql and in this case is 12.5 : jitsejan@theviji:~$ psql -V psql ( PostgreSQL ) 12 .5 ( Ubuntu 12 .5-0ubuntu0.20.04.1 ) Create new user After installing PostgreSQL I add a new user for the project I will be working on. First login with the default postgres user and use createuser --interactive to add a user. In my case I will do a project around Zelda and therefor create a new user zelda . jitsejan@theviji:~$ sudo -i -u postgres [ sudo ] password for jitsejan: postgres@theviji:~$ createuser --interactive Enter name of role to add: zelda Shall the new role be a superuser? ( y/n ) n Shall the new role be allowed to create databases? ( y/n ) y Shall the new role be allowed to create more new roles? ( y/n ) n Default the user doesn't have a password set which means we have to login to the psql shell and add a password for the new user: postgres@theviji:~$ psql psql ( 12 .5 ( Ubuntu 12 .5-0ubuntu0.20.04.1 )) Type \"help\" for help. postgres = # \\password zelda Enter new password: Enter it again: Since we are already here lets also add a database with the same name as the user. postgres = # create database zelda; CREATE DATABASE postgres = # \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+-------------+-------------+----------------------- postgres | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | template0 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | = c/postgres + | | | | | postgres = CTc/postgres template1 | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | = c/postgres + | | | | | postgres = CTc/postgres zelda | postgres | UTF8 | en_US.UTF-8 | en_US.UTF-8 | ( 4 rows ) Additionally to adding the user to PostgreSQL I also add the user to the system. jitsejan@theviji:~$ sudo adduser zelda Adding user ` zelda ' ... Adding new group `zelda' ( 1001 ) ... Adding new user ` zelda ' (1001) with group `zelda' ... Creating home directory ` /home/zelda ' ... Copying files from `/etc/skel' ... New password: Retype new password: passwd: password updated successfully Changing the user information for zelda Enter the new value, or press ENTER for the default Full Name [] : Zelda Room Number [] : Work Phone [] : Home Phone [] : Other [] : Is the information correct? [ Y/n ] Y Now I can switch to the newly created user and start the PostgreSQL shell. The connection information shows that we are indeed connected to the new zelda database by default. jitsejan@theviji:~$ sudo -i -u zelda zelda@theviji:~$ psql zelda = > \\c onninfo You are connected to database \"zelda\" as user \"zelda\" via socket in \"/var/run/postgresql\" at port \"5432\" . Configure PostgreSQL for Python The important part to get PostgreSQL working with Python remotely is to make sure PostgreSQL allows external IPs. Modify the postgresql.conf that should be located in /etc/postgresql/12/main . Open the file as sudo with a text editor: jitsejan@theviji:~$ sudo nano /etc/postgresql/12/main/postgresql.conf And change the listen_addresses to '*' to make PostgreSQL listen to all IPs. listen_addresses = '*' # what IP address(es) to listen on; Furthermore, update the pg_hba.conf to allow a connection from any IP. The file should be located in the same folder as the postgresql.conf . jitsejan@theviji:~$ sudo nano /etc/postgresql/12/main/pg_hba.conf Add this to that the bottom of the table at the bottom of the file: host all all 0 .0.0.0/0 md5 host all all ::/0 md5 Restart postgreqsl to apply the changes: jitsejan@theviji:~$ /etc/init.d/postgresql restart In case of errors we can look at the log files located in /var/log . Use sudo tail <logfile> to check the last lines in the log file. jitsejan@theviji:~$ sudo tail /var/log/postgresql/postgresql-12-main.log Connect to PostgreSQL with Python At this point we have a running PostgreSQL server which allows external traffic. We have a dedicated user with a password that we can use to connect to the server. The Python version I will be using is 3.8.5 . jitsejan @theviji : ~ $ python3 -- version Python 3.8 . 5 Install packages Install SQLAlchemy and the psycopg2 library. SQLAlchemy is an ORM which abstracts the specific PostgreSQL code for the project. If at some point you were to switch databases you could simply update the connection but leave all the database definitions the same. pip3 install psycopg2 sqlalchemy Create database engine For safety I have added my secrets as environment variables. Update the ~/.bashrc or ~/.zshrc and add the necessary exports. # ~/.bashrc export POSTGRES_USER = zelda export POSTGRES_DB = zelda export POSTGRES_PASS = SomePass export POSTGRES_PORT = 1234 export POSTGRES_HOST = dev . jitsejan . com Make sure to reload the configuration file with source ~/.bashrc . Now that the variables are exported we can use them in the following Python script to setup the database engine. import os import sqlalchemy from sqlalchemy import create_engine USER = os . environ [ 'POSTGRES_USER' ] PASS = os . environ [ 'POSTGRES_PASS' ] HOST = os . environ [ 'POSTGRES_HOST' ] PORT = os . environ [ 'POSTGRES_PORT' ] DB = os . environ [ 'POSTGRES_DB' ] db_string = f \"postgres:// { USER } : { PASS } @ { HOST } : { PORT } / { DB } \" engine = create_engine ( db_string ) Add a table For the first project I will crawl items from a website with only two fields for simplicity. The table items contains two columns: name (string) price (integer) from sqlalchemy import Table , Column , Integer , String , MetaData meta = MetaData () items = Table ( 'items' , meta , Column ( 'name' , String , primary_key = True ), Column ( 'price' , Integer ), ) meta . create_all ( engine ) Verify the table has been created by calling the table_names function on the engine . engine . table_names () # ['items'] And that is it. PostgreSQL is up and running and we are able to interact with it using Python.", "tags": "posts", "url": "setting-up-postgres-for-python.html", "loc": "setting-up-postgres-for-python.html" }, { "title": "Scraping data based on xpaths", "text": "For a small project I need to retrieve the cost of an item from the following HTML syntax: < table > < tr > < th > Name </ th > < td > Boomerang </ td > </ tr > < tr > < th > Cost(s) </ th > < td > 300 </ td > </ tr > < tr > < th > Description </ th > < td > A wonderful boomerang </ td > </ tr > </ table > It is difficult to get to this item because not every page has the same table, and not every table has the cost mentioned. The goal is to retrieve both the name and the price of an item. To keep things tidy I will use a dataclass for the items. Here I can define that the name will be a string and the price an integer as well as overwrite the print function for a single item. @dataclass ( frozen = True ) class Item : name : str price : int def __repr__ ( self ): return ( f ' { self . __class__ . __name__ } ' f '(name= { self . name } , price= { self . price } )' ) I am crawling data from https://zelda.gamepedia.com since it has a very complete overview of all the Zelda data. I use a simple function to get the XML tree from the page source using requests with lxml.html . BeautifulSoup and other fancy packages would work too but I like to stick to more basic libraries for simplicity. import lxml.html import requests HEADERS = { 'User-Agent' : 'Mozilla/5.0' } session = requests . Session () session . headers = HEADERS def _get_tree_from_url ( url : str ) -> lxml . html . etree : resp = session . get ( url ) return lxml . html . fromstring ( resp . text ) The links to all the items are inside the gallerybox div and are easy to retrieve using cssselect . The following function is a generator that returns the links to all items mentioned on the Legend of Zelda page. def _get_item_links () -> Iterator [ str ]: items_url = f \" { BASE_URL } /Items_in_The_Legend_of_Zelda\" tree = _get_tree_from_url ( items_url ) for elem in tree . cssselect ( \"li.gallerybox .gallerytext p a\" ): yield f \" { BASE_URL }{ elem . attrib [ 'href' ] . split ( '#' )[ 0 ] } \" The next bit takes care of finding the right row in the table to retrieve the price and the name of the item. The name is retrieved from the meta tag with the og:title property by getting the content attribute. I wrote it with cssselect but this could easily be rewritten as xpath with \"//meta[@property='og:title']/@content\" . The more tricky part is to find the cell of the table where the header of the row contains a certain text. For the xpath you will need to find the tr for which the table header contains() a certain string and return the text of the div inside the cell. def get_item_details ( link : str ) -> Item : tree = _get_tree_from_url ( link ) try : name = tree . cssselect ( \"meta[property='og:title']\" )[ 0 ] . attrib [ 'content' ] price = int ( tree . xpath ( \"//tr[th//text()[contains(., 'Cost(s)')]]/td/div\" )[ 0 ] . text ) return Item ( name , price ) except : pass # No price for this item Putting it all together: from dataclasses import dataclass import lxml.html import requests from typing import Iterator BASE_URL = \"https://zelda.gamepedia.com\" HEADERS = { 'User-Agent' : 'Mozilla/5.0' } @dataclass ( frozen = True ) class Item : name : str price : int def __repr__ ( self ): return ( f ' { self . __class__ . __name__ } ' f '(name= { self . name } , price= { self . price } )' ) def _get_tree_from_url ( url : str ) -> lxml . html . etree : resp = session . get ( url ) return lxml . html . fromstring ( resp . text ) def get_item_links () -> Iterator [ str ]: items_url = f \" { BASE_URL } /Items_in_The_Legend_of_Zelda\" tree = _get_tree_from_url ( items_url ) for elem in tree . cssselect ( \"li.gallerybox .gallerytext p a\" ): yield f \" { BASE_URL }{ elem . attrib [ 'href' ] . split ( '#' )[ 0 ] } \" def get_item_details ( link : str ) -> Item : tree = _get_tree_from_url ( link ) try : name = tree . cssselect ( \"meta[property='og:title']\" )[ 0 ] . attrib [ 'content' ] price = int ( tree . xpath ( \"//tr[th//text()[contains(., 'Cost(s)')]]/td/div\" )[ 0 ] . text ) return Item ( name , price ) except : pass # No price for this item session = requests . Session () session . headers = HEADERS items = [] for link in get_item_links (): item_data = get_item_details ( link ) ( items . append ( item_data ) if item_data is not None else None ) items . sort ( key = lambda x : x . price , reverse = True ) print ( items ) # [Item(name=Bow, price=980), Item(name=Boomerang, price=300), Item(name=Blue Ring, price=250), Item(name=Arrow, price=80), Item(name=Red Water of Life, price=68), Item(name=Blue Candle, price=60), Item(name=Food, price=60), Item(name=Blue Water of Life, price=40), Item(name=Bomb, price=20), Item(name=Heart Container, price=4)]", "tags": "posts", "url": "scraping-with-xpaths.html", "loc": "scraping-with-xpaths.html" }, { "title": "Getting the lead time for Jira tickets", "text": "Goal This is a simple example on how to retrieve the changelog of your Jira tickets using Python and requests and calculate the lead time. You could also use the Python Jira module but by using plain requests the following code can easily be copied to other languages. Setting up Either you can use the Jira instance you use at work or simply create a free account at Atlassian.com . Note: setting up a new instance using GMail or another other Single Sign On method will not give you a Jira password. You do not need it if you can use the API token for logging in to your Jira instance but in case you do need a password logout of the account and on the login page click on \"Can't log in?\". Fill in the email address you used to sign up for Jira and click on \"Send recovery link\" to set a password. Authenticating In case of using the free instance that Jira provides authentication is done through the API token. Navigate to your Atlassian account > Security and click on \"Create and manage API tokens\" under API token . In case you have started from scratch there will be no API tokens yet. Click on \"Create API token\" to generate a token that you can use in the Python code to connect to the Jira instance. Make sure to save the token somewhere save, i.e. in your terminal environment. Add the API_TOKEN , API_USER and API_URL to your Python script and add them as authentication for your requests session. Note: for older Jira instances the token might not be supported and you need to authenticate with (JIRA_USER, JIRA_PASSWORD) . import requests # Authentication JIRA_TOKEN = \"abcdefg12345\" JIRA_URL = \"https://jitsejan.atlassian.net/\" JIRA_USER = \"jitsejan@gmail.com\" # Setup request session rsession = requests . Session () rsession . auth = ( JIRA_USER , JIRA_TOKEN ) Mocking data To create some data for this small example I have created four stories as part of the blog-test (BT) project. I have moved two stories to Done , one is still In progress and one is in the Backlog . Since this is a simple example I haven't added any assignee, description or other metadata to the tickets. Retrieving data Using the Jira API it is easy to retrieve the tickets as long as you know a little bit of the Jira Query Language . To create a query navigate to the All issues page and click on Advanced search. It can be as simple as querying all tickets by their project. For more advanced queries play with the Jira UI and copy the query using the Advanced search option. For simplicity reasons I will pull all the tickets based on the project with the following query: project = BT The following Python code will retrieve the tickets for the query and addtionally add the changelog. It is important to enable the changelog for the expand parameter when the data is retrieved for the tickets. Note that I use version 2 of the API and I retrieve JSON data directly. If this was production code you would obviously verify the response and build in the proper error handling. jql = \"project = BT\" params = { 'jql' : jql , 'expand' : 'changelog' , } issues = rsession . get ( f \" { JIRA_URL } /rest/api/2/search\" , params = params ) . json ()[ 'issues' ] As a base table I use only the key and the created field. import pandas as pd base = pd . json_normalize ( issues )[[ 'key' , 'fields.created' ]] base . rename ( columns = { 'fields.created' : 'created' }, inplace = True ) This will result in the following table with the four tickets that I created before. key Created BT-4 2020-08-30T21:13:05.191+0000 BT-3 2020-08-30T21:13:00.259+0000 BT-2 2020-08-30T21:12:52.512+0000 BT-1 2020-08-30T21:12:46.238+0000 To get the changelog from the issues we have to dive a little deeper into the JSON structure. Again I use json_normalize but keep the fields and key fields. To get the history you need to open the JSON path to changelog > histories and extract the items field. transitions = pd . json_normalize ( data = issues , record_path = [ 'changelog' , 'histories' ], meta = [ 'fields' , 'key' ])[[ 'created' , 'items' , 'key' ]] Running the above code will result in the following table with the items still containing a list of different elements. created items key 2020-08-30T21:25:13.454+0000 [{'field': 'status', 'fieldtype': 'jira', .. ] BT-3 2020-08-30T21:17:14.147+0000 [{'field': 'status', 'fieldtype': 'jira', .. ] BT-3 2020-08-30T21:21:39.042+0000 [{'field': 'resolution', 'fieldtype': 'jira', .. ] BT-2 2020-08-30T21:18:59.912+0000 [{'field': 'status', 'fieldtype': 'jira', .. ] BT-2 2020-08-30T21:17:08.462+0000 [{'field': 'status', 'fieldtype': 'jira', .. ] BT-2 2020-08-30T21:21:41.059+0000 [{'field': 'resolution', 'fieldtype': 'jira', .. ] BT-1 2020-08-30T21:18:58.145+0000 [{'field': 'status', 'fieldtype': 'jira', .. ] BT-1 2020-08-30T21:14:17.086+0000 [{'field': 'status', 'fieldtype': 'jira', .. ] BT-1 The next transformation involves a few steps: Explode the items column to have a row for each item in the list Extract the columns from the items column (i.e. field and fieldtype ) using pd.Series Filter out the transitions that involve a status change Select only the status change date created , the key and the toString (final state of the transition) transitions = transitions . join ( transitions [ 'items' ] \\ . explode () \\ . apply ( pd . Series ) ) \\ . query ( \"field == 'status'\" ) \\ . drop ( \"items\" , axis = 1 )[[ 'created' , 'key' , 'toString' ]] The table looks cleaner now and shows when a ticket was put in a certain state. created key toString 2020-08-30T21:25:13.454+0000 BT-3 In Progress 2020-08-30T21:17:14.147+0000 BT-3 Selected for Development 2020-08-30T21:21:39.042+0000 BT-2 Done 2020-08-30T21:18:59.912+0000 BT-2 In Progress 2020-08-30T21:17:08.462+0000 BT-2 Selected for Development 2020-08-30T21:21:41.059+0000 BT-1 Done 2020-08-30T21:18:58.145+0000 BT-1 In Progress 2020-08-30T21:14:17.086+0000 BT-1 Selected for Development In the previous table it should be observed that there are no transitions for the fourth ticket BT-4 . This makes sense since the ticket was only created but was never put in another column on the Jira board. To calculate ticket metrics it is important to also include the transition to when a ticket was added. In order to do this I will join the transitions table with the created column from the base table. First I will create a copy of the base dataframe and add an additional column with the transition to match the schema of the transitions table. created_transitions = base . copy () # Add column created_transitions [ 'toString' ] = 'Backlog' This will show the four keys with the timestamp and the toString information. key created toString BT-4 2020-08-30T21:13:05.191+0000 Backlog BT-3 2020-08-30T21:13:00.259+0000 Backlog BT-2 2020-08-30T21:12:52.512+0000 Backlog BT-1 2020-08-30T21:12:46.238+0000 Backlog After combining the two tables with transitions = pd . concat ([ transitions , create_transitions ]) the table is complete and contains all the information we need. Now we do have a transition for BT-4 . created key toString 2020-08-30T21:12:46.238+0000 BT-1 Backlog 2020-08-30T21:14:17.086+0000 BT-1 Selected for Development 2020-08-30T21:18:58.145+0000 BT-1 In Progress 2020-08-30T21:21:41.059+0000 BT-1 Done 2020-08-30T21:12:52.512+0000 BT-2 Backlog 2020-08-30T21:17:08.462+0000 BT-2 Selected for Development 2020-08-30T21:18:59.912+0000 BT-2 In Progress 2020-08-30T21:21:39.042+0000 BT-2 Done 2020-08-30T21:13:00.259+0000 BT-3 Backlog 2020-08-30T21:17:14.147+0000 BT-3 Selected for Development 2020-08-30T21:25:13.454+0000 BT-3 In Progress 2020-08-30T21:13:05.191+0000 BT-4 Backlog To calculate the time in each status we will need to pivot the table to get the tickets as rows and the statuses as columns. statuses = transitions . pivot ( index = 'key' , values = 'created' , columns = 'toString' ) # Correct the datatype for the dataframe statuses = statuses . astype ( 'datetime64[ns]' ) # Rename the column The table now looks like this which makes it easy to see that BT-3 is not done yet and BT-4 never moved. key Backlog Selected for Development In Progress Done BT-1 2020-08-30T21:12:46.238 2020-08-30T21:14:17.086 2020-08-30T21:18:58.145 2020-08-30T21:21:41.059 BT-2 2020-08-30T21:12:52.512 2020-08-30T21:17:08.462 2020-08-30T21:18:59.912 2020-08-30T21:21:39.042 BT-3 2020-08-30T21:13:00.259 2020-08-30T21:17:14.147 2020-08-30T21:25:13.454 NaT BT-4 2020-08-30T21:13:05.191 NaT NaT NaT Now to calculate the lead time (time from Backlog to Done) it is straightforward to substract the two columns and get the timedelta . Since I moved tickets manually in a short period of time I will calculate the delta in seconds. statuses [ 'duration [s]' ] = ( statuses [ 'Done' ] - statuses [ 'Backlog' ]) . apply ( lambda x : x . seconds ) The very impressive result will be this small table: key duration [s] BT-1 534 BT-2 526 BT-3 nan BT-4 nan Check the final gist here . Hope this helps anyone playing with Jira and Python!", "tags": "posts", "url": "getting-lead-time-for-jira-tickets.html", "loc": "getting-lead-time-for-jira-tickets.html" }, { "title": "Hosting a static website with IPFS", "text": "In this article I will upload a static website to IPFS to get myself familiar with the steps it takes to link an .eth domain to content hosted on the distributed web. Prerequisites Mac Install IPFS Desktop according to the install instructions . This will add IPFS to the toolbar (the cube icon). Verify that IPFS is working properly by clicking the icon. Test ipfs in the command line after enabling Command Line Tools in the Preferences : ~/code/ipfs-static-website $ ❯ ipfs --version ipfs version 0 .6.0 By installing the Desktop will already start the daemon, so running ipfs daemon is not necessary. VPS Install the go-ipfs by retrieving the TAR-ball, extracing it and running the installation script. ~/ $ wget https://github.com/ipfs/go-ipfs/releases/download/v0.5.1/go-ipfs_v0.5.1_linux-amd64.tar.gz ~/ $ tar -xvzf go-ipfs_v0.5.1_linux-amd64.tar.gz ~/ $ cd go-ipfs ~/go-ipfs $ sudo bash install.sh ~/go-ipfs $ ipfs --version ipfs version 0 .5.1 Initialization will start the node in a local folder. Once you have been added as a node, the daemon can be started. $ ipfs init --profile server initializing IPFS node at /home/jitsejan/.ipfs generating 2048 -bit RSA keypair...done peer identity: QmSztWC9dxLzUV7Ph5ZJLwhGW5aLRG2Pwptis3cw6cfK53 to get started, enter: ipfs cat /ipfs/QmQPeNsJPyVWPFDVHb77w8G42Fvo15z4bG2X8D2GhfbSXc/readme $ ipfs daemon Initializing daemon... go-ipfs version: 0 .5.1 Repo version: 9 System version: amd64/linux Golang version: go1.13.10 Swarm listening on /ip4/127.0.0.1/tcp/4001 Swarm listening on /ip4/172.17.0.1/tcp/4001 Swarm listening on /ip4/172.21.0.1/tcp/4001 Swarm listening on /ip4/209.182.238.29/tcp/4001 Swarm listening on /ip6/::1/tcp/4001 Swarm listening on /p2p-circuit Swarm announcing /ip4/127.0.0.1/tcp/4001 Swarm announcing /ip4/209.182.238.29/tcp/4001 Swarm announcing /ip6/::1/tcp/4001 API server listening on /ip4/127.0.0.1/tcp/5001 WebUI: http://127.0.0.1:5001/webui Gateway ( readonly ) server listening on /ip4/127.0.0.1/tcp/8080 Daemon is ready Verify the peers that are connected. $ ipfs swarm peers /ip4/104.131.131.82/tcp/4001/p2p/QmaCpDMGvV2BGHeYERUEnRQAwe3N8SzbUtfsmvsqQLuvuJ /ip4/111.229.117.28/tcp/4001/p2p/QmXUKFSAKB4K9mSVMmtjJw55CkcyXajwTLXdmxvTC4kYy6 /ip4/113.255.3.43/tcp/44244/p2p/QmYC6H9pD26iAMnSiDgn1Rtz8g7Kmiv7mMjoCPRUGkQMLk /ip4/139.162.58.12/tcp/4001/p2p/QmXYaskeTJHGooCG32wP8tY8yfPYiQbC6yeD9RbrYref67 /ip4/147.75.109.213/tcp/4001/p2p/QmNnooDu7bfjPFoTZYxMNLWUQJyrVwtbZg5gBMjTezGAJN /ip4/147.75.77.187/tcp/4001/p2p/QmQCU2EcMqAqQPR2i9bChDtGNJchTbq5TbXJJ16u19uLTa /ip4/147.75.94.115/tcp/4001/p2p/QmcZf59bWwK5XFi76CZX8cbJ4BhTzzA3gU1ZjYZcYW3dwt /ip4/159.65.73.69/tcp/31564/p2p/12D3KooWQC15gyTUwDUob18c5EQBLCDbHrp8WymrrHoMgdPwLFqW /ip4/172.104.103.157/tcp/4001/p2p/QmYRk9rftMorXbRPMW26on6kw1ZPkf5hPQVvmd4aumT8JV /ip4/206.189.69.250/tcp/30315/p2p/12D3KooWJDNVGavZMo5WzgqPZCNBdrKu1DxqoER5wAQ61PHaFnrv /ip4/207.148.19.196/tcp/20010/p2p/12D3KooWMRXRibgUrCY9FDEXG8DFX3RtqwDKLQT98dgAZP25jvRu /ip4/49.234.193.176/tcp/4001/p2p/QmakhXhhfcpKpy1LY9FgBqZ6WcMMgeZqPzXjZSDEcFAmQ2 /ip4/73.95.18.162/tcp/51238/p2p/QmWMZpfbMfwRumwrrDjicWsBzHCGzq91AR285dtkrAQB9D Execute the sample test by adding a string to IPFS and querying it using curl . $ hash = ` echo \"I <3 IPFS - $( whoami ) \" | ipfs add -q ` $ echo $hash QmR8yeru6tqJis2WR5YV6xmgAQHTBwPKxN8DoJK7uhK4Z3 $ curl \"https://ipfs.io/ipfs/ $hash \" I < 3 IPFS -jitsejan Create a basic website The website that we will upload has the following structure. I chose to use subfolders for css and images to make sure this is also supported by IPFS. In the future I want to upload more complex websites to IPFS, for example my personal blog that is statis website created using Pelican . From my understanding of IPFS it is not possible to upload dynamic content at this point. ~/code/ipfs-static-website $ ❯ tree . ├── README.md ├── css │ └── style.css ├── images │ └── blockchain.jpg └── index.html The template for the website has a link to the stylesheet, some content and an image. index.html <!DOCTYPE html> < html > < head > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > < title > My first IPFS site! </ title > </ head > < body > < div class = 'main-container' > < img src = \"images/blockchain.jpg\" alt = \"A random Blockchain image\" /> < div class = \"content\" > IPFS is the < b > future </ b > ! </ div > </ div > </ body > </ html > The stylesheet sets the background for the page and makes sure the image is aligned in the center. style.css body { background-color : black ; } . main-container { margin : 50 px auto 0 px auto ; width : 500 px ; } . main-container img { width : 100 % ; } . main-container . content { color : white ; text-align : center ; } Add files to IPFS Now that the files have been created the website should be added to the filesystem. Run ipfs add -r in the folder that should be uploaded to recursively add the files to IPFS. ~/code/ipfs-static-website $ ❯ ipfs add -r . added QmNfVVQsXyekrNiM2dK35oQXg2dGqQ97Gz2PDBxUH6Piqu ipfs-static-website/README.md added QmZEZJRrb6WxoenKzuXtu9jUgmCxGpg777Y2zqcFwifGNS ipfs-static-website/css/style.css added QmQaYCUpUzHLiefmonVSHFnBhWw9bHTi1Js3QMddMxStKE ipfs-static-website/images/blockchain.jpg added QmSFgajgQq1XtjsLxxDupPH1Ys1tAuJ8DQoPScEUEZZX2U ipfs-static-website/index.html added QmUnsAAQ5vpH4gX1ypR5WaQJb1KDP7KsTEomiijJzetUvM ipfs-static-website/css added QmYBwq5uLGX6zfEBwjQ7UHgEv3Ton9hX4QjxxkTfg1tdVx ipfs-static-website/images added Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u ipfs-static-website 77 .86 KiB / 178 .67 KiB [=========================== >---------------------------------- ] 43 .58% The SITE_ID would be Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u as it is the hash of the main folder (bottom element). Opening up the file browser in IPFS desktop and searching for the QmHash will show the files from the repository. To confirm that the files are correct you can view the files. For example, you can navigate to images and open the blockchain.jpg to see the actual content. Verify the content In the previous step we found the hash of the main folder of the website. Since the ipfs daemon is running, we can view the files locally by navigating to http://localhost:8080/ipfs/ and adding the hash. Opening the hash in the browser http://localhost:8080/ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u/ will show the page as we expect: Use IPNS to host content A downside of using IPFS is that every time the website changes the corresponding hashes will be updated. If you link directly to the IPFS hash with your DNS this will break the next time you update the website. To avoid manually updating the hash with every update we could use IPNS instead. The IPNS hash should remain the same even though the website gets updated. In order to get the IPNS hash we will need to publish using ipfs name publish <IPFS_HASH> and wait for the IPFS hash to be returned. In my case it took a minute before the publishing was completed. ❯ ipfs name publish Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u Published to QmSztWC9dxLzUV7Ph5ZJLwhGW5aLRG2Pwptis3cw6cfK53: /ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u The return value contains the hash for IPNS which again we can verify using our localhost. The $PEER_ID will be QmSztWC9dxLzUV7Ph5ZJLwhGW5aLRG2Pwptis3cw6cfK53 and can be appended to http://localhost:8080/ipns/ . Indeed opening http://localhost:8080/ipns/QmXwD1dj6ywm3pNQPY2vuEzjdxz1zvrnVe7DrJp56yBnPU/ shows again the basic website. Setup DNS with Cloudflare Cloudflare is a service I use for my DNS management and security of my website. Cloudfare also supports an IPFS gateway which means we can setup the DNS to the IPFS content with this service. In order to link a domain name to the IPFS content we need to add two elements. In my case I want to link the IPFS content to https://ipfs.jitsejan.com . CNAME containing the subdomain with a target to cloudflare-ipfs.com. TXT with the name following the pattern _dnslink.<subdomain> with the dnslink=/ipfs/<IPFS_HASH> as content. CNAME: DNS link: These settings can be verified with dig by checking the answer to a call to _dnslink.<subdomain>.<domain> . This should return the dnslink with the correct IPFS hash. ❯ dig +noall +answer TXT _dnslink.ipfs.jitsejan.com _dnslink.ipfs.jitsejan.com. 300 IN TXT \"dnslink=/ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u\" As an additional step we can add a certificate to the domain by navigating to https://www.cloudflare.com/distributed-web-gateway/ and scrolling down to the bottom: After a few seconds this should complete: Before checking if my domain is working I will verify the content on the IPFS server of Cloudflare. By navigating to https://cloudflare-ipfs.com/ipfs/Qmeg3LpczHYLWFhQ9htz6qjizkf2aPKm3qZEyW4autpo5u/ I can check if the website is available. Initially it showed me the right page, but without the image. After waiting a couple of minutes the image shows too: Setup ENS I have registered my eth domain with https://app.ens.domains to make sure nobody would take jitsejan.eth . To see my domains I will need to connect to my wallet which was used to buy the domains. Because I bought the domain on my phone using the Cipher Browser some time ago and since then Cipher Browser got acquired by Coinbase the Cipher app was disabled. I did not have access to the Ethereum network and could not access https://app.ens.domains . Using the recovery phrase I was able to import my wallet into Firefox with MetaMask on my laptop and manage my domains. Navigate to your eth domain page, for example https://app.ens.domains/name/jitsejan.eth , and by clicking the + under Records add the content with your IPFS link: Note: Adding content to the network will cost gas! After being very patience the website will be visible on http://jitsejan.eth.link . Note the .link in the end to make sure the DNS can handle the content hosted on ENS (info: http://eth.link/). Viewing the page is free, so refreshing every second wouldn't cost you money. Future work Add a Pelican blog to IPFS Use IPNS in combination with the .eth domain Sources https://docs.ipfs.io/how-to/host-single-page-site/#create-your-site https://www.cloudflare.com/distributed-web-gateway/", "tags": "posts", "url": "hosting-static-website-with-ipfs.html", "loc": "hosting-static-website-with-ipfs.html" }, { "title": "Forward filling in Spark", "text": "Recently I had the challenge to figure out the status of a certain person in our database on any possible date, while the only thing we store is when the status of a person changes . The query I need to answer is similar to the following SQL statement: SELECT status FROM person_statuses WHERE person = \"John Doe\" AND time = current_date () Step 1 - Create the data To create the dummy data I will create an event generator. The generated events will be the input for the forward fill exercise. I will use the code I have written for the data-pipeline-project from this repo . In [1]: from datetime import datetime , timedelta from faker import Faker import json DATE_END = datetime . now () DATE_START = DATE_END - timedelta ( days = 31 ) NUM_EVENTS = 10 class EventGenerator : \"\"\" Defines the EventGenerator \"\"\" MIN_LIVES = 1 MAX_LIVES = 99 CHARACTERS = [ \"Mario\" , \"Luigi\" , \"Peach\" , \"Toad\" ] def __init__ ( self , num_events , output_type , start_date , end_date , output_file = None ): \"\"\" Initialize the EventGenerator \"\"\" self . faker = Faker () self . num_events = num_events self . output_type = output_type self . output_file = output_file self . start_date = start_date self . end_date = end_date def _get_date_between ( self , date_start , date_end ): \"\"\" Get a date between start and end date \"\"\" return self . faker . date_between_dates ( date_start = date_start , date_end = date_end ) def _generate_events ( self ): \"\"\" Generate the metric data \"\"\" for _ in range ( self . num_events ): yield { \"character\" : self . faker . random_element ( self . CHARACTERS ), \"world\" : self . faker . random_int ( min = 1 , max = 8 , step = 1 ), \"level\" : self . faker . random_int ( min = 1 , max = 4 , step = 1 ), \"lives\" : self . faker . random_int ( min = self . MIN_LIVES , max = self . MAX_LIVES , step = 1 ), \"time\" : str ( self . _get_date_between ( self . start_date , self . end_date )), } def store_events ( self ): if self . output_type == \"jl\" : with open ( self . output_file , \"w\" ) as outputfile : for event in self . _generate_events (): outputfile . write ( f \" { json . dumps ( event ) } \\n \" ) elif self . output_type == \"list\" : return list ( self . _generate_events ()) I only want 10 events to keep the dataframe we use in Spark small. In [2]: params = { \"num_events\" : NUM_EVENTS , \"output_type\" : \"list\" , \"start_date\" : DATE_START , \"end_date\" : DATE_END , } # Create the event generator generator = EventGenerator ( ** params ) # Create and store the events events = generator . store_events () Step 2 - Analyze the data The events represent the persons (Nintendo characters) and their status (current world and level). From looking at the data it is obvious there are big gaps in time before the characters advance to the next level. How do we know where Mario was without storing the daily world/level status? In [3]: import pandas as pd pd . DataFrame ( events ) . sort_values ([ \"character\" , \"time\" ]) Out[3]: character world level lives time 9 Luigi 4 3 9 2020-04-24 8 Luigi 7 3 42 2020-04-30 5 Luigi 5 2 53 2020-05-14 0 Luigi 6 4 99 2020-05-22 3 Luigi 2 1 41 2020-05-23 1 Mario 5 3 13 2020-05-02 2 Mario 4 4 80 2020-05-06 7 Peach 8 4 55 2020-04-29 4 Toad 6 3 40 2020-05-11 6 Toad 6 3 58 2020-05-14 Step 3 - Create Spark Dataframe To create the Spark Dataframe a SparkSession is used. I don't use any external libraries right now, since we will use plain (Py)Spark. In [4]: from pyspark.sql import SparkSession , SQLContext from pyspark.sql import functions as F from pyspark.sql.window import Window import os os . environ [ 'PYSPARK_PYTHON' ] = \"/data/jupyter/bin/python\" spark = ( SparkSession . builder . appName ( \"Spark Forward Fill\" ) . getOrCreate ()) sc = SQLContext ( spark ) In [5]: df = sc . createDataFrame ( events ) /data/jupyter/lib/python3.6/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead warnings.warn(\"inferring schema from dict is deprecated,\" In [6]: df . show () +---------+-----+-----+----------+-----+ |character|level|lives| time|world| +---------+-----+-----+----------+-----+ | Luigi| 4| 99|2020-05-22| 6| | Mario| 3| 13|2020-05-02| 5| | Mario| 4| 80|2020-05-06| 4| | Luigi| 1| 41|2020-05-23| 2| | Toad| 3| 40|2020-05-11| 6| | Luigi| 2| 53|2020-05-14| 5| | Toad| 3| 58|2020-05-14| 6| | Peach| 4| 55|2020-04-29| 8| | Luigi| 3| 42|2020-04-30| 7| | Luigi| 3| 9|2020-04-24| 4| +---------+-----+-----+----------+-----+ Step 4 - Correct the data Ensure we have the correct datatypes: In [7]: df . dtypes Out[7]: [('character', 'string'), ('level', 'bigint'), ('lives', 'bigint'), ('time', 'string'), ('world', 'bigint')] In [8]: df = df . withColumn ( \"time\" , F . to_date ( F . to_timestamp ( \"time\" ))) In [9]: df . dtypes Out[9]: [('character', 'string'), ('level', 'bigint'), ('lives', 'bigint'), ('time', 'date'), ('world', 'bigint')] As a status I combine the world and the level: In [10]: df = df . withColumn ( \"status\" , F . concat ( F . col ( 'world' ), F . lit ( '-' ), F . col ( 'level' ))) df . show () +---------+-----+-----+----------+-----+------+ |character|level|lives| time|world|status| +---------+-----+-----+----------+-----+------+ | Luigi| 4| 99|2020-05-22| 6| 6-4| | Mario| 3| 13|2020-05-02| 5| 5-3| | Mario| 4| 80|2020-05-06| 4| 4-4| | Luigi| 1| 41|2020-05-23| 2| 2-1| | Toad| 3| 40|2020-05-11| 6| 6-3| | Luigi| 2| 53|2020-05-14| 5| 5-2| | Toad| 3| 58|2020-05-14| 6| 6-3| | Peach| 4| 55|2020-04-29| 8| 8-4| | Luigi| 3| 42|2020-04-30| 7| 7-3| | Luigi| 3| 9|2020-04-24| 4| 4-3| +---------+-----+-----+----------+-----+------+ Step 5 - Forward fill the data The data should be partitioned by character and ordered by the time. For this I will use a simple window function to go through the data. In [11]: w = Window () . partitionBy ( \"character\" ) . orderBy ( \"time\" ) Apply the window to create a new column where we add the time of the next status update per character. In [12]: df = df . withColumn ( \"next_status\" , F . lead ( \"time\" ) . over ( w )) df . show () +---------+-----+-----+----------+-----+------+-----------+ |character|level|lives| time|world|status|next_status| +---------+-----+-----+----------+-----+------+-----------+ | Peach| 4| 55|2020-04-29| 8| 8-4| null| | Toad| 3| 40|2020-05-11| 6| 6-3| 2020-05-14| | Toad| 3| 58|2020-05-14| 6| 6-3| null| | Mario| 3| 13|2020-05-02| 5| 5-3| 2020-05-06| | Mario| 4| 80|2020-05-06| 4| 4-4| null| | Luigi| 3| 9|2020-04-24| 4| 4-3| 2020-04-30| | Luigi| 3| 42|2020-04-30| 7| 7-3| 2020-05-14| | Luigi| 2| 53|2020-05-14| 5| 5-2| 2020-05-22| | Luigi| 4| 99|2020-05-22| 6| 6-4| 2020-05-23| | Luigi| 1| 41|2020-05-23| 2| 2-1| null| +---------+-----+-----+----------+-----+------+-----------+ With the sequence I create the column containing the dates between the time of the status change and the time of the next_status . The sequence column will contain a list of dates that can be exploded to create a row for each date in the list. In [13]: df = df . withColumn ( \"sequence\" , F . when ( F . col ( \"next_status\" ) . isNotNull (), F . expr ( \"sequence(to_date(time), date_sub(to_date(next_status),1), interval 1 day)\" )) \\ . otherwise ( F . array ( \"time\" ))) df . show () +---------+-----+-----+----------+-----+------+-----------+--------------------+ |character|level|lives| time|world|status|next_status| sequence| +---------+-----+-----+----------+-----+------+-----------+--------------------+ | Peach| 4| 55|2020-04-29| 8| 8-4| null| [2020-04-29]| | Toad| 3| 40|2020-05-11| 6| 6-3| 2020-05-14|[2020-05-11, 2020...| | Toad| 3| 58|2020-05-14| 6| 6-3| null| [2020-05-14]| | Mario| 3| 13|2020-05-02| 5| 5-3| 2020-05-06|[2020-05-02, 2020...| | Mario| 4| 80|2020-05-06| 4| 4-4| null| [2020-05-06]| | Luigi| 3| 9|2020-04-24| 4| 4-3| 2020-04-30|[2020-04-24, 2020...| | Luigi| 3| 42|2020-04-30| 7| 7-3| 2020-05-14|[2020-04-30, 2020...| | Luigi| 2| 53|2020-05-14| 5| 5-2| 2020-05-22|[2020-05-14, 2020...| | Luigi| 4| 99|2020-05-22| 6| 6-4| 2020-05-23| [2020-05-22]| | Luigi| 1| 41|2020-05-23| 2| 2-1| null| [2020-05-23]| +---------+-----+-----+----------+-----+------+-----------+--------------------+ Now explode is applied to the sequence column which gives the result shown below. As we can see the events are filled from the first status change until the latest status change. In [14]: df . select ( \"character\" , \"status\" , F . explode ( \"sequence\" ) . alias ( \"time\" )) . show () +---------+------+----------+ |character|status| time| +---------+------+----------+ | Peach| 8-4|2020-04-29| | Toad| 6-3|2020-05-11| | Toad| 6-3|2020-05-12| | Toad| 6-3|2020-05-13| | Toad| 6-3|2020-05-14| | Mario| 5-3|2020-05-02| | Mario| 5-3|2020-05-03| | Mario| 5-3|2020-05-04| | Mario| 5-3|2020-05-05| | Mario| 4-4|2020-05-06| | Luigi| 4-3|2020-04-24| | Luigi| 4-3|2020-04-25| | Luigi| 4-3|2020-04-26| | Luigi| 4-3|2020-04-27| | Luigi| 4-3|2020-04-28| | Luigi| 4-3|2020-04-29| | Luigi| 7-3|2020-04-30| | Luigi| 7-3|2020-05-01| | Luigi| 7-3|2020-05-02| | Luigi| 7-3|2020-05-03| +---------+------+----------+ only showing top 20 rows One thing missing in the approach above is that we only know the status until the latest change for each character. What I need is to know what the status is today, so I need to modify the code and create an artifical end date when there is no next status, or the next status is in the past. That way I can fill dates after the last status up until today with that last status. For completeness I will repeat the code that I have created before. In [15]: # Create the dataframe df = sc . createDataFrame ( events ) # Modify the data type df = df . withColumn ( \"time\" , F . to_date ( F . to_timestamp ( \"time\" ))) # Create the status column df = df . withColumn ( \"status\" , F . concat ( F . col ( 'world' ), F . lit ( '-' ), F . col ( 'level' ))) # Apply the window df = df . withColumn ( \"next_status\" , F . lead ( \"time\" ) . over ( w )) # Fill in the empty `next_status` column with today's date df = df . withColumn ( \"next_status\" , F . when ( F . col ( \"next_status\" ) . isNull (), F . expr ( \"current_date()\" )) \\ . otherwise ( F . col ( \"next_status\" ))) # Apply the sequence df = df . withColumn ( \"sequence\" , F . when ( F . col ( \"next_status\" ) . isNotNull (), F . expr ( \"sequence(to_date(time), date_sub(to_date(next_status), 1), interval 1 day)\" )) \\ . otherwise ( F . array ( \"time\" ))) # Select the columns and explore the sequence df . select ( \"character\" , \"status\" , F . explode ( \"sequence\" ) . alias ( \"time\" )) . show () +---------+------+----------+ |character|status| time| +---------+------+----------+ | Peach| 8-4|2020-04-29| | Peach| 8-4|2020-04-30| | Peach| 8-4|2020-05-01| | Peach| 8-4|2020-05-02| | Peach| 8-4|2020-05-03| | Peach| 8-4|2020-05-04| | Peach| 8-4|2020-05-05| | Peach| 8-4|2020-05-06| | Peach| 8-4|2020-05-07| | Peach| 8-4|2020-05-08| | Peach| 8-4|2020-05-09| | Peach| 8-4|2020-05-10| | Peach| 8-4|2020-05-11| | Peach| 8-4|2020-05-12| | Peach| 8-4|2020-05-13| | Peach| 8-4|2020-05-14| | Peach| 8-4|2020-05-15| | Peach| 8-4|2020-05-16| | Peach| 8-4|2020-05-17| | Peach| 8-4|2020-05-18| +---------+------+----------+ only showing top 20 rows Step 6 - Avoid forward filling! From this small test it is clear it is not wise to use the sequence to create an event for every date for every character. This will easily become massive and should not be used in reality. The main idea behind trying to do the forward filling is to compare the complexity with Python pandas . In Pandas you can forward fill by simply using ffill . I used the following code to retrieve the same result as in the Spark script above. In [16]: pdf = pd . DataFrame ( events ) # Convert datatypes pdf [ 'time' ] = pd . to_datetime ( pdf [ 'time' ]) # Add status column pdf [ 'status' ] = pdf . apply ( lambda x : f \" { x [ 'world' ] } - { x [ 'level' ] } \" , axis = 1 ) # Create full time range as frame timeframe = pd . date_range ( start = min ( pdf [ 'time' ]), end = datetime . now () . date ()) . to_frame () . reset_index ( drop = True ) . rename ( columns = { 0 : 'time' }) # Merge timeframe into original frame pdf = pdf . merge ( timeframe , left_on = 'time' , right_on = 'time' , how = 'right' ) # 1. Pivot to get dates on rows and characters as columns # 2. Forward fill values per character # 3. Fill remaining NaNs with False pdf = pdf . pivot ( index = 'time' , columns = 'character' , values = 'status' ) pdf = pdf . fillna ( method = 'ffill' ) # Drop NaN column and reset the index pdf = pdf . loc [:, pdf . columns . notnull ()] . reset_index () # Melt the columns back pdf = pd . melt ( pdf , id_vars = 'time' , value_name = 'status' ) print ( f \"Original length: { len ( events ) } , new length: { len ( pdf ) } \" ) pdf . head ( 10 ) Original length: 10, new length: 128 Out[16]: time character status 0 2020-04-24 Luigi 4-3 1 2020-04-25 Luigi 4-3 2 2020-04-26 Luigi 4-3 3 2020-04-27 Luigi 4-3 4 2020-04-28 Luigi 4-3 5 2020-04-29 Luigi 4-3 6 2020-04-30 Luigi 7-3 7 2020-05-01 Luigi 7-3 8 2020-05-02 Luigi 7-3 9 2020-05-03 Luigi 7-3 Step 7 - Be smarter Instead of using the forward fill approach to create all this data, it is a better idea to only add one more column to the table where the end of the status is recorded. For example, if Mario was in level 1-1 seven days ago and today he finally made it to level 1-2, there is no need to create six events containing level 1-1 and one event for level 1-2. Instead it would be better that one event contains 1-1 with start date seven days ago and end date yesterday, plus an event for level 1-2 with start date today and future date somewhere in the future. Coming back to the initial SQL query I can rewrite this easily to a query with BETWEEN : SELECT status FROM person_statuses WHERE person = \"John Doe\" AND time = current_date () is rewritten to SELECT status FROM person_statuses WHERE person = \"John Doe\" AND current_date () BETWEEN time AND endtime I use a similar script, but will extract one day from the endtime column. There is no explode needed with this approach. In [17]: # Create the dataframe df = sc . createDataFrame ( events ) # Modify the data type df = df . withColumn ( \"time\" , F . to_date ( F . to_timestamp ( \"time\" ))) # Create the status column df = df . withColumn ( \"status\" , F . concat ( F . col ( 'world' ), F . lit ( '-' ), F . col ( 'level' ))) # Apply the window df = df . withColumn ( \"endtime\" , F . lead ( \"time\" ) . over ( w )) # Substract one day from the endtime df = df . withColumn ( \"endtime\" , F . expr ( \"date_sub(to_date(endtime), 1)\" )) # Fill in the empty `endtime` column with today's date df = df . withColumn ( \"endtime\" , F . when ( F . col ( \"endtime\" ) . isNull (), F . expr ( \"current_date()\" )) \\ . otherwise ( F . col ( \"endtime\" ))) # Select the columns and explore the sequence df . select ( \"character\" , \"status\" , \"time\" , \"endtime\" ) . show () +---------+------+----------+----------+ |character|status| time| endtime| +---------+------+----------+----------+ | Peach| 8-4|2020-04-29|2020-05-25| | Toad| 6-3|2020-05-11|2020-05-13| | Toad| 6-3|2020-05-14|2020-05-25| | Mario| 5-3|2020-05-02|2020-05-05| | Mario| 4-4|2020-05-06|2020-05-25| | Luigi| 4-3|2020-04-24|2020-04-29| | Luigi| 7-3|2020-04-30|2020-05-13| | Luigi| 5-2|2020-05-14|2020-05-21| | Luigi| 6-4|2020-05-22|2020-05-22| | Luigi| 2-1|2020-05-23|2020-05-25| +---------+------+----------+----------+ Store the dataframe as temporary view such that I can use SQL to query it with Spark: In [18]: df . createOrReplaceTempView ( \"df\" ) In [19]: sc . sql ( \"SELECT status, time, endtime FROM df WHERE character = 'Peach'\" ) . show () +------+----------+----------+ |status| time| endtime| +------+----------+----------+ | 8-4|2020-04-29|2020-05-25| +------+----------+----------+ In [20]: sc . sql ( \"SELECT status FROM df WHERE character = 'Peach' AND '2020-05-24' BETWEEN time AND endtime\" ) . show () +------+ |status| +------+ | 8-4| +------+ Instead of using PySpark to create the dataframe the same can be achieved using SQL. In [21]: # Create the events view eventsdf = sc . createDataFrame ( events ) eventsdf . createOrReplaceTempView ( \"events\" ) In [22]: sc . sql ( \"\"\" WITH statuses AS ( SELECT character, CONCAT(world, '-', level) AS status, to_date(time) AS start, to_date(DATE_SUB(LEAD(time) OVER (PARTITION BY character ORDER BY time), 1)) AS end FROM events ) SELECT character, status, start, IF(end IS NOT NULL, end, current_date()) AS end FROM statuses \"\"\" ) . show () +---------+------+----------+----------+ |character|status| start| end| +---------+------+----------+----------+ | Peach| 8-4|2020-04-29|2020-05-25| | Toad| 6-3|2020-05-11|2020-05-13| | Toad| 6-3|2020-05-14|2020-05-25| | Mario| 5-3|2020-05-02|2020-05-05| | Mario| 4-4|2020-05-06|2020-05-25| | Luigi| 4-3|2020-04-24|2020-04-29| | Luigi| 7-3|2020-04-30|2020-05-13| | Luigi| 5-2|2020-05-14|2020-05-21| | Luigi| 6-4|2020-05-22|2020-05-22| | Luigi| 2-1|2020-05-23|2020-05-25| +---------+------+----------+----------+ And that's it. This was a good exercise to understand how easy it is to make things too complex..", "tags": "posts", "url": "forward-filling-in-spark.html", "loc": "forward-filling-in-spark.html" }, { "title": "GraphQL with Flask and MongoDB", "text": "In this project I will add data to a MongoDB database and make it accessible using GraphQL. Prerequisites Python For this project I will be using Python 3.7 and pipenv as my virtual environment. MongoDB Install MongoDB on your machine or VPS, or use a free hosted cluster. On Mac , simply use brew to install the database. ❯ brew tap mongodb / brew ❯ brew install mongodb - community @4.2 I went with Atlas and tried one of the free clusters they offer. After selecting the region and the name, simply click on create and wait for the cluster to be ready. 3 minutes later the cluster will be ready. Go to Security -> Database Access and create a new Database User and give Admin or Read and Write access to the database. As an optional safety guard it is smart to enable the IP whitelist and add your own IP to the list to make sure no other IPs can access your database. Finally go back to the Clusters overview and click on Connect . Choose the appropiate connection mechanism for your application. In my case this will be with Python 3.7. This should give you enough information to get started with MongoDB! [Optional] MongoDB Compass To visualize your MongoDB instance download MongoDB Compass from their website. Use the connection string from the previous step if you want to connect to the hosted cluster directly. Once you have connected to the database the page will show your cluster, the hosts and the available data. Since we have not added anything yet it will show the default admin and local databases. Creating the dataset As a dataset I will be using information from https://www.mariowiki.com/ . More specifcally I will be crawling data from Super Mario 1 levels from https://www.mariowiki.com/Category:Super_Mario_Bros._Levels . Each page with the level details has a table containing the Enemies and the Level statistics . I will just show the reduced version of the script here, but the whole script is availabe in my Github repo. The script will run through the links on the website and retrieves meta data, the description, enemies and the statistics. The files are stored in a JSON file. def get_all_tables (): \"\"\" Retrieve all the tables \"\"\" tree = get_lxml_tree_from_url ( SMB_LEVEL_URL ) for elem in tree . cssselect ( '#mw-pages a' ): url = f \" { BASE_URL }{ elem . get ( 'href' ) } \" print ( f \"Crawling data for ` { url } `\" ) if 'Minus' not in url : subtree = get_lxml_tree_from_url ( url ) yield { 'table_data' : _get_table_data ( subtree ), 'description' : _get_description ( subtree ), 'enemies' : _get_enemies ( subtree ), 'statistics' : _get_level_statistics ( subtree ), } def main (): \"\"\" Main function \"\"\" df = pd . DataFrame . from_dict ( get_all_tables ()) print ( f \"Found { len ( df ) } results\" ) df . to_json ( 'smb.json' , orient = 'records' ) if __name__ == \"__main__\" : main () The JSON for one item will look like the following: { \"description\" : \"World 1-1 is the first level of World 1 in Super Mario Bros., and the first level overall in said game; thus, it is the first level in general of the entire Super Mario series. The first screen of the level is also the game's title screen when starting it up. It contains the basics of the Super Mario Bros. game, getting the player ready for the journey ahead. The level consists of Magic Mushrooms, standard enemies such as Little Goombas and Koopa Troopas, a lot of coins, a hidden secret bonus area that allows the player to skip most of the level, Fire Flowers, pits, and a flagpole at the end. According to Shigeru Miyamoto, World 1-1 was one of the later levels created, due to the \\\"fun\\\" courses created first being more suited for late game, where players were more familiar with how Super Mario Bros. works.\" , \"enemies\" : [ { \"name\" : \"Little Goomba\" , \"amount\" : \"16\" }, { \"name\" : \"Green Koopa Troopa\" , \"amount\" : \"1\" } ], \"statistics\" : [ { \"name\" : \"Coin\" , \"amount\" : 39 }, { \"name\" : \"Magic Mushroom\" , \"amount\" : 3 }, { \"name\" : \"Fire Flower\" , \"amount\" : 3 }, { \"name\" : \"Starman\" , \"amount\" : 1 }, { \"name\" : \"1 up Mushroom\" , \"amount\" : 1 } ], \"table_data\" : { \"World-Level\" : \"World 1-1\" , \"World\" : \"World 1\" , \"Game\" : \"Super Mario Bros.\" , \"Time limit\" : \"400 seconds\" } } Storing the dataset Creating the schemas In the previous step I have stored the result of the crawler to smb.json . To store the data in MongoDB I need to define the schema for the different tables. This is done in models.py using mongoengine as shown below. I want to store the following four tables. Games Powerups Enemies Levels The first collection Game only has a name , Powerup and Enemy have a name and amount and the Level collection will have more fields. A Level references a Game document, has a list of enemies and powerups, but also information on the time limit, boss, world and a description. \"\"\" models.py \"\"\" from mongoengine import Document , EmbeddedDocument from mongoengine.fields import ( DateTimeField , ListField , ReferenceField , StringField , IntField , ) class Game ( Document ): meta = { \"collection\" : \"game\" } name = StringField () class Powerup ( Document ): meta = { \"collection\" : \"powerup\" } name = StringField () amount = IntField () class Enemy ( Document ): meta = { \"collection\" : \"enemy\" } name = StringField () amount = IntField () class Level ( Document ): meta = { \"collection\" : \"level\" } game = ReferenceField ( Game ) name = StringField () description = StringField () world = StringField () time_limit = IntField () boss = StringField () enemies = ListField ( ReferenceField ( Enemy )) powerups = ListField ( ReferenceField ( Powerup )) Loading the data into MongoDB Now the schemas for the documents has been created, I can load the data into the MongoDB cluster that I have created before. I open the JSON file and iterate through the data. Note that I have used different methods to get data from the JSON object. One way is to use multiple get s to get a nest field. game = Game ( name = data [ 0 ] . get ( 'table_data' ) . get ( 'Game' )) A more readable way is to use jsonpath like I did here, but it requires an extra import. name = jsonpath ( row , 'table_data.World-Level' )[ 0 ] The final script to load the data is database.py . Since I am doing this project as a proof of concept I will always wipe the database first before adding new data. \"\"\" database.py \"\"\" import json from jsonpath import jsonpath from mongoengine import connect import os from models import Enemy , Level , Game , Powerup DATABASE = \"flask-mongodb-graphene\" PASSWORD = os . environ . get ( \"MONGODB_PASSWORD\" ) client = connect ( DATABASE , host = f \"mongodb+srv://mongograph: { PASSWORD } @clusterjj-gazky.mongodb.net/?ssl=true&ssl_cert_reqs=CERT_NONE\" , alias = \"default\" , ) client . drop_database ( DATABASE ) def init_db (): with open ( \"smb.json\" , \"r\" ) as file : data = json . loads ( file . read ()) game = Game ( name = data [ 0 ] . get ( \"table_data\" ) . get ( \"Game\" )) game . save () for row in data : enemies = [] for elem in row [ \"enemies\" ]: amount = elem [ \"amount\" ] if isinstance ( elem [ \"amount\" ], int ) else 1 enemy = Enemy ( name = elem [ \"name\" ], amount = amount ) enemy . save () enemies . append ( enemy ) powerups = [] for elem in row [ \"statistics\" ]: powerup = Powerup ( name = elem [ \"name\" ], amount = elem [ \"amount\" ]) powerup . save () powerups . append ( powerup ) level = Level ( description = row . get ( \"description\" ), name = jsonpath ( row , \"table_data.World-Level\" )[ 0 ], world = jsonpath ( row , \"table_data.World\" )[ 0 ], time_limit = jsonpath ( row , \"table_data.Time limit\" )[ 0 ] . split ( \" \" )[ 0 ], boss = row . get ( \"table_data\" ) . get ( \"Boss\" ), enemies = enemies , game = game , powerups = powerups , ) level . save () init_db () To populate the database run the script: ❯ python database.py Verifying the dataset Option 1. Verifying it using the website Option 2. Verify using MongoDB Compass Database overview Collection overview Detailed collection view Option 3. Using Python \"\"\" verify.py \"\"\" from database import client from models import Powerup for powerup in Powerup . objects : print ( powerup . name ) # OUTPUT # Coin # Magic Mushroom # Fire Flower # Starman # 1 up Mushroom # Coin # Magic Mushroom # Fire Flower # Starman # 1 up Mushroom # Coin # Magic Mushroom # Fire Flower Setting up GraphQL To make the data accessible with GraphQL I need to convert models from the previous step to a GraphQL schema. Firstly, I need the graphene dependencies to create the schema specifically for a MongoDB connection. After importing the dependencies I import all the models from my models.py . Now each model has to be setup as a node in the GraphQL Schema where Query is the top of the graph. In the Query class I have defined three different queries: Get all levels Get all enemies Get all powerups \"\"\" schema.py \"\"\" import graphene from graphene.relay import Node from graphene_mongo import MongoengineConnectionField , MongoengineObjectType from models import Game as GameModel from models import Powerup as PowerupModel from models import Enemy as EnemyModel from models import Level as LevelModel class Game ( MongoengineObjectType ): class Meta : description = \"Game\" model = GameModel interfaces = ( Node ,) class Powerup ( MongoengineObjectType ): class Meta : description = \"Power-ups\" model = PowerupModel interfaces = ( Node ,) class Enemy ( MongoengineObjectType ): class Meta : description = \"Enemies\" model = EnemyModel interfaces = ( Node ,) class Level ( MongoengineObjectType ): class Meta : description = \"Levels\" model = LevelModel interfaces = ( Node ,) class Query ( graphene . ObjectType ): node = Node . Field () all_levels = MongoengineConnectionField ( Level ) all_enemies = MongoengineConnectionField ( Enemy ) all_powerups = MongoengineConnectionField ( Powerup ) schema = graphene . Schema ( query = Query , types = [ Powerup , Level , Enemy , Game ]) To start the server now the schema is defined we need to create the application. Flask is used as web application with one path (rule) to the GraphQL endpoint. I connect to the MongoDB database and set the database. The server is started on port 5002. \"\"\" app.py \"\"\" from flask import Flask from flask_graphql import GraphQLView from mongoengine import connect import os from schema import schema DATABASE = 'flask-mongodb-graphene' PASSWORD = os . environ . get ( \"MONGODB_PASSWORD\" ) client = connect ( DATABASE , host = f 'mongodb+srv://mongograph: { PASSWORD } @clusterjj-gazky.mongodb.net/?ssl=true&ssl_cert_reqs=CERT_NONE' , alias = 'default' ) app = Flask ( __name__ ) app . debug = True app . add_url_rule ( '/graphql' , view_func = GraphQLView . as_view ( 'graphql' , schema = schema , graphiql = True )) if __name__ == '__main__' : app . run ( port = 5002 ) Go to your terminal and run the webapp: ❯ python app.py Verifying GraphQL Option 1. Verify using the Flask version of GraphiQL Go to localhost:5002/graphql and run the allPowerups query to get back the names of all the power-ups. Option 2. Verify using GraphiQL application Using the standalone application GraphiQL it is easy to test the GraphQL endpoint. Using the allEnemies query defined in schema.py we get back all the enemies and their amounts. Option 3. Using Postman I have used Postman for a long time for testing my REST APIs and fortunately it also supports GraphQL APIs. And that should do it. Check my GitHub for the code. In another post I want to explore GraphQL further and implement filtering and pagination. Sources https://graphene-mongo.readthedocs.io https://jeffersonheard.github.io", "tags": "posts", "url": "graphql-with-flask-and-mongodb.html", "loc": "graphql-with-flask-and-mongodb.html" }, { "title": "Integrating PySpark with Salesforce", "text": "To get a connection in Spark with Salesforce the advice is to use the spark-salesforce library. In order to make this work several dependencies need to be added. Make sure the core libraries to support XML are also downloaded. $ wget https://repo1.maven.org/maven2/com/springml/spark-salesforce_2.11/1.1.1/spark-salesforce_2.11-1.1.1.jar $ wget https://repo1.maven.org/maven2/com/springml/salesforce-wave-api/1.0.9/salesforce-wave-api-1.0.9.jar $ wget https://repo1.maven.org/maven2/com/force/api/force-partner-api/40.0.0/force-partner-api-40.0.0.jar $ wget https://repo1.maven.org/maven2/com/force/api/force-wsc/40.0.0/force-wsc-40.0.0.jar $ wget https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-xml/2.10.3/jackson-dataformat-xml-2.10.3.jar $ wget https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.jar The configuration is saved in config.ini with the following fields: [salesforce] username = mail@jitsejan.com password = securePassw0rd token = sal3sforceT0ken Loading the configuration is done using configparser : from configparser import ConfigParser config = ConfigParser () config . read ( 'config.ini' ) When creating the SparkSession make sure the paths to the differents JARs are correctly set: from pyspark import SparkSession jars = [ 'spark-salesforce_2.11-1.1.1.jar' , 'salesforce-wave-api-1.0.9.jar' , 'force-partner-api-40.0.0.jar' , 'force-wsc-40.0.0.jar' , 'jackson-dataformat-xml-2.10.3.jar' , 'jackson-core-2.10.3.jar' , ] spark = ( SparkSession . builder . appName ( \"PySpark with Salesforce\" ) . config ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . getOrCreate ()) The session is created and we are ready to pull some data: soql = \"SELECT name, industry, type, billingaddress, sic FROM account\" df = spark \\ . read \\ . format ( \"com.springml.spark.salesforce\" ) \\ . option ( \"username\" , config . get ( 'salesforce' , 'username' )) \\ . option ( \"password\" , f \" { config . get ( 'salesforce' , 'password' ) }{ config . get ( 'salesforce' , 'token' ) } \" ) \\ . option ( \"soql\" , soql ) \\ . load ()", "tags": "posts", "url": "integrating-pyspark-with-salesforce.html", "loc": "integrating-pyspark-with-salesforce.html" }, { "title": "Integrating PySpark with SQL server using JDBC", "text": "First of all I need the JDBC driver for Spark in order to make the connection to a Microsoft SQL server. $ wget https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/6.4.0.jre8/mssql-jdbc-6.4.0.jre8.jar -P /opt/notebooks/ The configuration is saved in config.ini with the following fields: [mydb] database = mydb host = mydb-server.database.windows.net username = readonly password = mypassword port = 1433 Loading the configuration is simple with configparser : from configparser import ConfigParser config = ConfigParser () config . read ( 'config.ini' ) Set the values for the connection: jdbc_url = f \"jdbc:sqlserver:// { config . get ( 'mydb' , 'host' ) } : { config . get ( 'mydb' , 'port' ) } ;database= { config . get ( 'mydb' , 'database' ) } \" connection_properties = { \"user\" : config . get ( 'mydb' , 'username' ), \"password\" : config . get ( 'mydb' , 'password' ) } When creating the SparkSession make sure the path to the JAR is correctly set: from pyspark.sql import SparkSession jars = [ \"mssql-jdbc-6.4.0.jre8.jar\" , ] spark = ( SparkSession . builder . appName ( \"PySpark with SQL server\" ) . config ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . getOrCreate ()) The session is created and we can query the actual database: schema = 'dbo' table = 'users' The reading is done using the jdbc read option and specifying the connection details: df = spark \\ . read \\ . jdbc ( jdbc_url , f \" { schema } . { table } \" , properties = connection_properties ) An alternative approach is to use the same syntax as for the Redshift article by omitting the connection_properties and use a more explicit notation. df = spark . read \\ . format ( \"jdbc\" ) \\ . option ( \"url\" , jdbc_url ) \\ . option ( \"dbtable\" , f \" { schema } . { table } \" ) \\ . option ( \"user\" , config . get ( 'mydb' , 'username' )) \\ . option ( \"password\" , config . get ( 'mydb' , 'password' )) \\ . load ()", "tags": "posts", "url": "integrating-pyspark-with-sql-server-using-jdbc.html", "loc": "integrating-pyspark-with-sql-server-using-jdbc.html" }, { "title": "Using Powerlevel10K as Zsh theme", "text": "Previously I was using powerlevel9k as theme for my iTerm2 Zsh configuration. Recently I had to install a new MacBook and found an easier way to make the terminal look fancier. powerlevel10k is the better version of powerlevel9k , especially since it has a configuration prompt where the installer guides you through all the changes you can make to the style. For Mac it is as simple as the following few lines, assuming you have brew installed. $ brew install romkatv/powerlevel10k/powerlevel10k $ echo 'source /usr/local/opt/powerlevel10k/powerlevel10k.zsh-theme' >>! ~/.zshrc $ p10k configure", "tags": "posts", "url": "using-powerlevel10k-as-zsh-theme.html", "loc": "using-powerlevel10k-as-zsh-theme.html" }, { "title": "Using parametrize with PyTest", "text": "Recap In my previous post I showed the function to test the access to the castle based on the powerup of the character . It takes a test for the case that the character has_access and a test to verify the character does not have access without the Super Mushroom. Both the castle and character are set as a fixture in the conftest.py . Snippet - Fixtures for castle and character import pytest ... @pytest . fixture ( scope = \"class\" ) def castle (): return Castle ( CASTLE_NAME ) @pytest . fixture ( scope = \"class\" ) def character (): return Character ( CHARACTER_NAME ) ... Snippet - Tests to validate access to castle. ... class TestCastleClass : \"\"\" Defines the tests for the Castle class \"\"\" ... def test_has_access_true_with_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns True for Super Mushroom \"\"\" character . powerup = 'Super Mushroom' assert castle . has_access ( character ) def test_has_access_false_without_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns False for other powerups \"\"\" character . powerup = 'Not a mushroom' assert not castle . has_access ( character ) ... Running this in the console looks like this: $ pytest -k has_access -v ============================================================ test session starts ============================================================= platform darwin -- Python 3 .7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /Users/jitsejan/.local/share/virtualenvs/blog-testing-KMgUXSdn/bin/python3.7m cachedir: .pytest_cache rootdir: /Users/jitsejan/code/blog-testing plugins: mock-1.11.1 collected 17 items / 15 deselected / 2 selected tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom PASSED [ 50 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_false_without_super_mushroom PASSED [ 100 % ] Introducing parametrize Using parametrize writing tests becomes significantly easier. Instead of writing a test for each combination of parameters I can write one test with a list of different sets of parameters. For each set of parameters the same test case will be executed, hence the two test cases above can be replaced by: @pytest . mark . parametrize ( 'powerup,has_access' , [ ( \"Super Mushroom\" , True ), ( \"Not a mushroom\" , False ), ], ids = [ 'successful' , 'failure-without-super-mushroom' ]) def test_has_access_true_with_super_mushroom ( self , castle , character , powerup , has_access ): \"\"\" Test that has_access returns True for Super Mushroom \"\"\" character . powerup = powerup assert castle . has_access ( character ) == has_access Running the same selection of tests again still returns two results but now it is indicated by the ID provided as argument for the parametrize . $ pytest -k has_access -v ============================================================ test session starts ============================================================= platform darwin -- Python 3 .7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /Users/jitsejan/.local/share/virtualenvs/blog-testing-KMgUXSdn/bin/python3.7m cachedir: .pytest_cache rootdir: /Users/jitsejan/code/blog-testing plugins: mock-1.11.1 collected 17 items / 15 deselected / 2 selected tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom [ successful ] PASSED [ 50 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom [ failure-without-super-mushroom ] PASSED [ 100 % ] The other test cases in the repo don't lend themselves to be used for parametrization, but it has helped me to reduce the number of test cases in our data platform repo by half. Give it a try :)", "tags": "posts", "url": "using-parametrize-with-pytest.html", "loc": "using-parametrize-with-pytest.html" }, { "title": "Integrating PySpark with Redshift", "text": "In my article on how to connect to S3 from PySpark I showed how to setup Spark with the right libraries to be able to connect to read and right from AWS S3. In the following article I show a quick example how I connect to Redshift and use the S3 setup to write the table to file. First of all I need the Postgres driver for Spark in order to make connecting to Redshift possible. $ wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.6/postgresql-42.2.6.jar -P /opt/notebooks/ I have saved my configuration in the following variable for testing purposes. Of course it would be wise to store the details in environment variables or in a proper configuration file. config = { 'aws_access_key' : 'aaaaaa' , 'aws_secret_key' : 'bbbbb' , 'aws_region' : 'eu-west-2' , 'aws_bucket' : 'my-bucket' , 'redshift_user' : 'user' , 'redshift_pass' : 'pass' , 'redshift_port' : 1234 , 'redshift_db' : 'mydatabase' , 'redshift_host' : 'myhost' , } Setting up the Spark context is straightforward. Make sure the Postgres library is available by adding it to extraClassPath , or copy it to the jars folder in the Spark installation location ( SPARK_HOME ). from pyspark import SparkContext , SparkConf , SQLContext jars = [ \"/opt/notebooks/postgresql-42.2.6.jar\" ] conf = ( SparkConf () . setAppName ( \"S3 with Redshift\" ) . set ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . set ( \"spark.hadoop.fs.s3a.access.key\" , config . get ( 'aws_access_key' )) . set ( \"spark.hadoop.fs.s3a.secret.key\" , config . get ( 'aws_secret_key' )) . set ( \"spark.hadoop.fs.s3a.path.style.access\" , True ) . set ( \"spark.hadoop.fs.s3a.impl\" , \"org.apache.hadoop.fs.s3a.S3AFileSystem\" ) . set ( \"com.amazonaws.services.s3.enableV4\" , True ) . set ( \"spark.hadoop.fs.s3a.endpoint\" , f \"s3- { config . get ( 'region' ) } .amazonaws.com\" ) . set ( \"spark.executor.extraJavaOptions\" , \"-Dcom.amazonaws.services.s3.enableV4=true\" ) . set ( \"spark.driver.extraJavaOptions\" , \"-Dcom.amazonaws.services.s3.enableV4=true\" ) ) sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) Now the Spark context is set I specify the schema and the table that I want to read from Redshift and write to S3. schema = 'custom' table = 'postcodes' The reading is done using the jdbc format and specifying the Redshift details: df = sqlContext . read \\ . format ( \"jdbc\" ) \\ . option ( \"url\" , f \"jdbc:postgresql:// { config . get ( 'redshift_host' ) } .redshift.amazonaws.com: { config . get ( 'redshift_port' ) } / { config . get ( 'redshift_db' ) } \" ) \\ . option ( \"dbtable\" , f \" { schema } . { table } \" ) \\ . option ( \"user\" , config . get ( 'redshift_user' )) \\ . option ( \"password\" , config . get ( 'redshift_pass' )) \\ . load () Writing is easy since I specified the S3 details in the Spark configuration. df . write . mode ( 'overwrite' ) . parquet ( \"s3a://{config.get('aws_bucket')}/raw/ {schema} / {table} )", "tags": "posts", "url": "integrating-pyspark-with-redshift.html", "loc": "integrating-pyspark-with-redshift.html" }, { "title": "Integrating PySpark notebook with S3", "text": "Introduction In my post Using Spark to read from S3 I explained how I was able to connect Spark to AWS S3 on a Ubuntu machine. Last week I was trying to connect to S3 again using Spark on my local machine, but I wasn't able to read data from our datalake. Our datalake is hosted in the eu-west-2 region which apparently requires you to specify the version of authentication. Instead of setting up the right environment on my machine and reconfigure everything, I chose to update the Docker image from my notebook repo so I could test locally on my Mac before pushing it to my server. Instead of configuring both my local and remote environment I can simply spin up the Docker container and have two identical environments. Implementation Rather than providing the AWS credentials in the Spark config, I want to keep things simple and only have one credentials file from where I will read the important information. The contents of ~/.aws/credentials specify just one account in this example, but this is where I have specified all the different AWS accounts we are using. This file will be copied to the Docker container by mounting the local aws folder inside the Docker instance. [prod] aws_access_key_id = xxxxxxyyyyyyy aws_secret_access_key = zzzzzzzzyyyyyyy region = eu-west-2 The Dockerfile consists of different steps. I have stripped down the Dockerfile to only install the essentials to get Spark working with S3 and a few extra libraries (like nltk ) to play with some data. A few things to note: The base image is the pyspark-notebook provided by Jupyter . Some packages are installed to be able to install the rest of the Python requirements. The Jupyter configuration (see below) is copied to the Docker image. Two libraries for Spark are downloaded to interact with AWS. These particular versions seem to work well, where newer versions caused different issues during my testing. The Python packages are installed defined in the requirements.txt . The Jupyter packages and extensions are installed and enabled. The notebook is started in Jupyter lab mode. FROM jupyter/pyspark-notebook USER root # Add essential packages RUN apt-get update && apt-get install -y build-essential curl git gnupg2 nano apt-transport-https software-properties-common # Set locale RUN apt-get update && apt-get install -y locales \\ && echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen \\ && locale-gen # Add config to Jupyter notebook COPY jupyter/jupyter_notebook_config.py /home/jovyan/.jupyter/ RUN chmod -R 777 /home/jovyan/ # Spark libraries RUN wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar -P $SPARK_HOME /jars/ RUN wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P $SPARK_HOME /jars/ USER $NB_USER # Install Python requirements COPY requirements.txt /home/jovyan/ RUN pip install -r /home/jovyan/requirements.txt # Install NLTK RUN python -c \"import nltk; nltk.download('popular')\" # Custom styling RUN mkdir -p /home/jovyan/.jupyter/custom COPY custom/custom.css /home/jovyan/.jupyter/custom/ # NB extensions RUN jupyter contrib nbextension install --user RUN jupyter nbextensions_configurator enable --user # Run the notebook CMD [ \"/opt/conda/bin/jupyter\" , \"lab\" , \"--allow-root\" ] The Jupyter configuration file sets up the notebook environment. In my case I set the password, the startup directory and the IP restrictions. \"\"\" jupyter_notebook_config.py \"\"\" c = get_config () c . InteractiveShell . ast_node_interactivity = \"all\" c . NotebookApp . allow_origin = '*' c . NotebookApp . ip = '*' c . NotebookApp . notebook_dir = '/opt/notebooks/' c . NotebookApp . open_browser = False c . NotebookApp . password = u 'sha1:a123:345345' c . NotebookApp . port = 8558 The docker-compose.yml contains the setup of the Docker instance. The most important parts of the compose file are: The notebook and data folder are mapped from the Docker instance to a local folder. The credential file is mapped from the Docker machine to the local machine. The public port is set to 8558. version : '3' services : jitsejan-pyspark : user : root privileged : true image : jitsejan/pyspark-notebook restart : always volumes : - ./notebooks:/opt/notebooks - ./data:/opt/data - $HOME/.aws/credentials:/home/jovyan/.aws/credentials:ro environment : - GRANT_SUDO=yes ports : - \"8558:8558\" Execution I am running Docker version 19.03.5 at the time of writing. ~/code/notebooks > master $ docker version Client: Docker Engine - Community Version: 19 .03.5 API version: 1 .40 Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07 :22:34 2019 OS/Arch: darwin/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19 .03.5 API version: 1 .40 ( minimum version 1 .12 ) Go version: go1.12.12 Git commit: 633a0ea Built: Wed Nov 13 07 :29:19 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: v1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1 .0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0 .18.0 GitCommit: fec3683 Run docker-compose up after creating the compose file to spin up the notebook. ~/code/notebooks > master $ docker-compose up Creating network \"notebooks_default\" with the default driver Creating notebooks_jitsejan-pyspark_1 ... done Attaching to notebooks_jitsejan-pyspark_1 jitsejan-pyspark_1 | [ I 13 :38:47.211 LabApp ] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret jitsejan-pyspark_1 | [ W 13 :38:47.474 LabApp ] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended. jitsejan-pyspark_1 | [ I 13 :38:47.516 LabApp ] [ jupyter_nbextensions_configurator ] enabled 0 .4.1 jitsejan-pyspark_1 | [ I 13 :38:48.766 LabApp ] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab jitsejan-pyspark_1 | [ I 13 :38:48.767 LabApp ] JupyterLab application directory is /opt/conda/share/jupyter/lab jitsejan-pyspark_1 | [ I 13 :38:49.802 LabApp ] Serving notebooks from local directory: /opt/notebooks jitsejan-pyspark_1 | [ I 13 :38:49.802 LabApp ] The Jupyter Notebook is running at: jitsejan-pyspark_1 | [ I 13 :38:49.802 LabApp ] http://ade618e362da:8558/ jitsejan-pyspark_1 | [ I 13 :38:49.803 LabApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . The container is now running on port 8558 : ~/code/notebooks > master $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ade618e362da jitsejan/pyspark-notebook \"tini -g -- /opt/con…\" 2 minutes ago Up 2 minutes 0 .0.0.0:8558->8558/tcp, 8888 /tcp notebooks_jitsejan-pyspark_1 For convenience I am running Portainer because it is easier to get an overview of the containers running in Docker instead of using the CLI.. It also helps to clean up all the orphan images. Code To connect Spark to S3 I will use the credentials file as configuration file and use the configparser library to read the parameters. from configparser import ConfigParser config_object = ConfigParser () config_object . read ( \"/home/jovyan/.aws/credentials\" ) profile_info = config_object [ \"prod\" ] Since the configuration now contains the production account data I can use it so set the parameters for the Spark context. Note that all these parameters are required to connect to the data on S3. The access key and secret are always mentioned in tutorials, but it took me a while to figure out I need to specify the endpoint and enable V4. from pyspark import SparkContext , SparkConf , SQLContext conf = ( SparkConf () . set ( \"spark.hadoop.fs.s3a.path.style.access\" , True ) . set ( \"spark.hadoop.fs.s3a.access.key\" , profile_info . get ( 'aws_access_key_id' )) . set ( \"spark.hadoop.fs.s3a.secret.key\" , profile_info . get ( 'aws_secret_access_key' )) . set ( \"spark.hadoop.fs.s3a.endpoint\" , f \"s3- { profile_info . get ( 'region' ) } .amazonaws.com\" ) . set ( \"spark.hadoop.fs.s3a.impl\" , \"org.apache.hadoop.fs.s3a.S3AFileSystem\" ) . set ( \"com.amazonaws.services.s3.enableV4\" , True ) . set ( \"spark.driver.extraJavaOptions\" , \"-Dcom.amazonaws.services.s3.enableV4=true\" ) ) With the above configuration I initialize the Spark context and can read from our datalake. sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) df = sqlContext . read . parquet ( \"s3a://mi-datalake-prod/warehouse/platform/company_list/\" ) Hope this helps!", "tags": "posts", "url": "integrating-pyspark-notebook-with-s3.html", "loc": "integrating-pyspark-notebook-with-s3.html" }, { "title": "Casting a PySpark DataFrame column to a specific datatype", "text": "import pyspark.sql.functions as F from pyspark.sql.types import IntegerType # Cast the count column to an integer dataframe . withColumn ( \"count\" , F . col ( \"count\" ) . cast ( IntegerType ()))", "tags": "posts", "url": "casting-a-pyspark-column-datatype.html", "loc": "casting-a-pyspark-column-datatype.html" }, { "title": "Run methods dynamically by name", "text": "In this project I want to verify the availability of the APIs that we use to ingest data into our data platform. In the example I will use Jira, Workable and HiBob since they offer clean APIs without too much configuration. First I will create a test suite to verify the availability and once this works move it to a Lambda function that could be scheduled with CloudWatch on a fixed schedule. Prerequisites Make sure the following environment variables are set. Change it to the correct profile and region for the AWS account you want to run the tests for. The profile should be available in ~/.aws/credentials . AWS_PROFILE=prod AWS_DEFAULT_REGION=eu-west-1 ENV=prod Testing One of the main reasons I switched from unittest to pytest is the ease of use of fixtures . You can define the functions or variables that you will need throughout the whole test set. Fixture class In this case I define a FixtureClass that contains a function to retrieve the parameters from AWS SSM , a key-value store where we store our API keys and other secrets. Note that in the following function I only retrieve the parameters for the base path / . The class has a Boto3 session and ssm_client . class FixtureClass : \"\"\" Defines the FixtureClass \"\"\" def get_ssm_parameters ( self ): \"\"\" Returns the SSM parameters \"\"\" paginator = self . ssm_client . get_paginator ( \"get_parameters_by_path\" ) iterator = paginator . paginate ( Path = \"/\" , WithDecryption = True ) params = {} for page in iterator : for param in page . get ( \"Parameters\" , []): params [ param . get ( \"Name\" )] = param . get ( \"Value\" ) return params @property def session ( self ): return boto3 . session . Session () @property def ssm_client ( self ): return self . session . client ( \"ssm\" ) Set the fixture Now I can create an instance of the FixtureClass and add the ssm_parameters as fixture for my tests. Define them either on conftest.py next to your test files, or add them on top of the file where you will write the tests. fc = FixtureClass () @pytest . fixture ( scope = \"session\" ) def ssm_parameters (): return fc . get_ssm_parameters () Add fixture to test class In order to make the fixture available for all the tests I add the fixture as attribute to the test class. By using autouse=True the fixtures become available in the setup_class method. This method is only called once per execution of the tests. \"\"\" testsourceavailability.py \"\"\" class TestSourceAvailability : \"\"\" Defines the tests to verify the source availability \"\"\" @pytest . fixture ( autouse = True ) def setup_class ( self , ssm_parameters ): \"\"\" Setup the test class \"\"\" self . ssm_parameters = ssm_parameters self . session = requests . Session () def _get_param ( self , key ): return self . ssm_parameters . get ( key ) Create the tests Below I have defined five different tests to validate the availability of three different sources. Three of the tests are to verify the availability (good weather) and two of them ensure that the API does not work with the wrong parameters (bad weather). HiBob (tool used by HR) Jira (tool used by Tech) Workable (tool used by Recruiting) Every tests consists of the following steps: Retrieve the parameters from SSM Set the arguments for the API call Assert the status code of the API call # Append to previous TestSourceAvailability file. def test_hibob_is_available ( self ): \"\"\" Test that the HiBob API is available \"\"\" api_key = self . _get_param ( \"HIBOB_API_KEY\" ) api_url = self . _get_param ( \"HIBOB_API_URL\" ) kwargs = { 'method' : 'get' , 'url' : api_url , 'headers' : { \"Authorization\" : api_key } } assert self . session . request ( ** kwargs ) . status_code == 200 def test_jira_is_available ( self ): \"\"\" Test that the Jira API is available \"\"\" api_key = self . _get_param ( \"JIRA_API_KEY\" ) api_url = self . _get_param ( \"JIRA_URL\" ) jira_user = self . _get_param ( \"JIRA_USER\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } /rest/api/2/project\" , 'auth' : ( jira_user , api_key ) } assert self . session . request ( ** kwargs ) . status_code == 200 def test_jira_is_unavailable_without_api_key ( self ): \"\"\" Test that the Jira API is unavailable without API key \"\"\" api_key = None api_url = self . _get_param ( \"JIRA_URL\" ) jira_user = self . _get_param ( \"JIRA_USER\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } /rest/api/2/project\" , 'auth' : ( jira_user , \"\" ) } assert self . session . request ( ** kwargs ) . status_code == 401 def test_workable_api_is_available ( self ): \"\"\" Test that the Workable API is available \"\"\" api_key = self . _get_param ( \"WORKABLE_API_KEY\" ) api_url = self . _get_param ( \"WORKABLE_API_URL\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } jobs\" , 'headers' : { \"Authorization\" : \"Bearer {} \" . format ( api_key )}, 'params' : { \"limit\" : 1 , \"include_fields\" : \"description\" } } assert self . session . request ( ** kwargs ) . status_code == 200 def test_workable_api_is_not_available_for_wrong_credentials ( self ): \"\"\" Test that the Workable API is not available for wrong credentials \"\"\" api_key = \"fake_key\" api_url = self . _get_param ( \"WORKABLE_API_URL\" ) kwargs = { 'method' : 'get' , 'url' : f \" { api_url } jobs\" , 'headers' : { \"Authorization\" : \"Bearer {} \" . format ( api_key )}, 'params' : { \"limit\" : 1 , \"include_fields\" : \"description\" } } assert self . session . request ( ** kwargs ) . status_code == 401 Execution of the tests Make sure pytest is installed on your machine. Run the file we've created before and add verbosity if you wish. Note that I use Python 3.7 and pytest 5.3.1. In my case all tests are green, so we can continue! $ pytest testsourceavailability.py -v ============================= test session starts ============================== platform darwin -- Python 3 .7.4, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 cachedir: .pytest_cache rootdir: /Users/jitsejan/Documents collected 5 items testsourceavailability.py::TestSourceAvailability::test_hibob_is_available PASSED [ 20 % ] testsourceavailability.py::TestSourceAvailability::test_jira_is_available PASSED [ 40 % ] testsourceavailability.py::TestSourceAvailability::test_jira_is_unavailable_without_api_key PASSED [ 60 % ] testsourceavailability.py::TestSourceAvailability::test_workable_api_is_available PASSED [ 80 % ] testsourceavailability.py::TestSourceAvailability::test_workable_api_is_not_available_for_wrong_credentials PASSED [ 100 % ] Lambda function Because I do not only want to verify availability at the test stage, I will create a Lambda function that I can schedule to periodically check that the APIs are still available. AvailabilityChecker class The class is initialized with the ssm_client to access the parameters stored in SSM and again the requests.Session that is used to call the API. The function to get the parameter by key has the same name, but the underlying logic is of course different compared to the one I used in the tests before with a fixture. By keeping the function name the same it is slightly easier to copy the tests to this Lambda function. \"\"\" availabilitychecker.py \"\"\" class AvailabilityChecker : def __init__ ( self ): self . ssm_client = boto3 . client ( \"ssm\" ) self . session = requests . Session () def _get_param ( self , key ): \"\"\" Return the SSM parameter \"\"\" return self . ssm_client . get_parameter ( Name = key , WithDecryption = True )[ \"Parameter\" ][ \"Value\" ] Verify functions I will only add the good weather tests from the previous test set, hence I will verify HiBob, Jira and Workable, but I don't check for the negative cases. # continue availabilitychecker.py def verify_hibob_is_available ( self ): \"\"\" Verify that the HiBob API is available \"\"\" api_key = self . _get_param ( \"HIBOB_API_KEY\" ) api_url = self . _get_param ( \"HIBOB_API_URL\" ) arguments = { 'method' : 'get' , 'url' : api_url , 'headers' : { \"Authorization\" : api_key } } return self . session . request ( ** arguments ) . status_code == 200 def verify_jira_is_available ( self ): \"\"\" Verify that the Jira API is available \"\"\" api_key = self . _get_param ( \"JIRA_API_KEY\" ) api_url = self . _get_param ( \"JIRA_URL\" ) jira_user = self . _get_param ( \"JIRA_USER\" ) arguments = { 'method' : 'get' , 'url' : f \" { api_url } /rest/api/2/project\" , 'auth' : ( jira_user , api_key ) } return self . session . request ( ** arguments ) . status_code == 200 def verify_workable_api_is_available ( self ): \"\"\" Verify that the Workable API is available \"\"\" api_key = self . _get_param ( \"WORKABLE_API_KEY\" ) api_url = self . _get_param ( \"WORKABLE_API_URL\" ) arguments = { 'method' : 'get' , 'url' : f \" { api_url } jobs\" , 'headers' : { \"Authorization\" : \"Bearer {} \" . format ( api_key ) }, 'params' : { \"limit\" : 1 , \"include_fields\" : \"description\" } } return self . session . request ( ** arguments ) . status_code == 200 Retrieve verify methods automatically Because in reality this file is way larger since I need to test way more sources, I do not want to write out all the verify functions explicitly in my Lambda function like below. def lambda_handler ( event , context ): avc = AvailabilityChecker () avc . verify_hibob_is_available () avc . verify_jira_is_available () avc . verify_workable_is_available () Instead, I want to dynamically retrieve these functions by iterating through the methods of the class. def _get_verify_functions ( self ): \"\"\" Return verify functions inside this class \"\"\" return [ func for func in dir ( self ) if callable ( getattr ( self , func )) and func . startswith ( \"verify\" )] This method will loop through the callable functions, check if it starts with verify and return a list of functions. Final Lambda I have updated the lambda_handler to retrieve the functions, iterate through them and execute the method to validate for the different sources if the API is available. Of course this code is a bit longer, but when I add more verify functions to the class, I do not have to change any other code! def lambda_handler ( event , context ): avc = AvailabilityChecker () for method in avc . _get_verify_functions (): is_available = getattr ( avc , method )() source = ' ' . join ( method . split ( '_' )[ 1 : - 2 ]) . title () if not is_available : print ( f \"[NOK] Please check availability for ` { source } `.\" ) else : print ( f \"[OK] ` { source } `\" ) Running it will give the results for the three sources. $ python availabilitychecker.py [ OK ] ` Hibob ` [ OK ] ` Jira ` [ OK ] ` Workable Api ` Check the Gist for the final code. Hope it helps :)", "tags": "posts", "url": "run-methods-dynamically-by-name.html", "loc": "run-methods-dynamically-by-name.html" }, { "title": "Using Python with Jinja and PDFkit to generate a resume", "text": "This project contains a simple example on how to build a resume with Python using Jinja, HTML, Bootstrap and a data file. In the past I have always created my resume with Latex, but to make life a little easier I chose to switch to a combination of Python and HTML. Maintaining a Latex document is cumbersome and it is difficult to divide the data from the style. By using Jinja it is straightforward to separate the resume data from the actual layout. And the most important part, I can stick to Python! Library overview flask - Application to render the Jinja templates with. jinja - Library to create templates and populate fields with Python variables. pdfkit - Tool to write HTML to PDF with Python. pyyaml - Library to read and write Yaml files with Python. Getting started Clone this repository and navigate inside the folder. ~/code/ $ git clone https://github.com/jitsejan/jinja-resume-template.git ~/code/ $ cd jinja-resume-template Create the virtual environment with pipenv to run the project in. ~/code/jinja-resume-template $ pipenv shell Creating a virtualenv for this project… Pipfile: /Users/jitsejan/code/jinja-resume-template/Pipfile Using /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 ( 3 .7.4 ) to create virtualenv… ⠇ Creating virtual environment...Already using interpreter /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 Using base prefix '/Library/Frameworks/Python.framework/Versions/3.7' New python executable in /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt/bin/python3 Also creating executable in /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt/bin/python Installing setuptools, pip, wheel... done . Running virtualenv with interpreter /Library/Frameworks/Python.framework/Versions/3.7/bin/python3 ✔ Successfully created virtual environment! Virtualenv location: /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt Launching subshell in virtual environment… . /Users/jitsejan/.local/share/virtualenvs/jinja-resume-template-97zV94Wt/bin/activate Install the dependencies: jinja-resume-template-97zV94Wt ~/code/jinja-resume-template $ pipenv install Pipfile.lock not found, creating… Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… ✔ Success! Updated Pipfile.lock ( 546278 ) ! Installing dependencies from Pipfile.lock ( 546278 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 8 /8 — 00 :00:10 Creating the PDF Execute the script run.py to generate the PDF: jinja-resume-template-97zV94Wt ~/code/jinja-resume-template $ pipenv run python run.py Loading pages ( 1 /6 ) Counting pages ( 2 /6 ) Resolving links ( 4 /6 ) Loading headers and footers ( 5 /6 ) Printing pages ( 6 /6 ) Done Information Jinja helps to create structures like this: < header > < h1 > {{ personal.get('name').get('first') }} {{ personal.get('name').get('last') }} </ h1 > < h2 > {{ occupation }} </ h2 > </ header > Everything between {{ and }} is interpreted as Python. The input of the template is a dictionary with the keys personal and occupation , which are all defined in data.yml . All the variables defined in the YAML file can be used in the template. personal : name : first : Pete last : Peterson occupation : Resume builder The following Python script will load the YAML and the HTML and save the populated template to output_text . def _get_data (): \"\"\" Load the data from the YAML file \"\"\" with open ( DATA_FILE , 'r' ) as stream : try : return yaml . safe_load ( stream ) except yaml . YAMLError as exc : print ( exc ) def _get_template (): \"\"\" Retrieve the template \"\"\" template_loader = jinja2 . FileSystemLoader ( searchpath = \"templates\" ) template_env = jinja2 . Environment ( loader = template_loader ) return template_env . get_template ( TEMPLATE_FILE ) # Loads YAML file data = _get_data () # Loads HTML file template = _get_template () # Fills in the variables in the HTML file output_text = template . render ( ** data ) Jinja for-loop With Jinja it is also easy to loop through lists and dictionaries. Below I have defined the languages variable with two elements where each element has a description . languages : English : description : Full professional proficiency Italian : description : Elementary proficiency In the HTML we use {% and %} to indicate Python code is executed. < ul > {% for dict_item in languages %} < li > {{dict_item}}: {{ languages[dict_item]['description'] }} </ li > {% endfor %} </ ul > In the above case I use it for a for -loop, but the same syntax is used for conditionals too. For example, you could write a condition like the following: {% if social.get('github') not None %} < div class = \"social\" > {{ social.get('github') }} </ div > {% endif %} For further Jinja tricks, take a look at their documentation . Take a look at my Github repo for the code, clone it and play with the template.", "tags": "posts", "url": "using-python-with-jinja-and-pdfkit-to-generate-resume.html", "loc": "using-python-with-jinja-and-pdfkit-to-generate-resume.html" }, { "title": "Using Faker to generate events", "text": "Faker A quick introduction to Faker. Installation In [1]: %%sh pip install Faker Collecting Faker Using cached https://files.pythonhosted.org/packages/d4/ed/2fd5337ed405c4258dde1254e60f4e8ef9f1787576c0a2cd0d750b1716a6/Faker-2.0.3-py2.py3-none-any.whl Requirement already satisfied: six>=1.10 in /Users/j.waterschoot/.local/share/virtualenvs/pdf-project-FE8EO07q/lib/python3.7/site-packages (from Faker) (1.12.0) Requirement already satisfied: python-dateutil>=2.4 in /Users/j.waterschoot/.local/share/virtualenvs/pdf-project-FE8EO07q/lib/python3.7/site-packages (from Faker) (2.8.0) Requirement already satisfied: text-unidecode==1.3 in /Users/j.waterschoot/.local/share/virtualenvs/pdf-project-FE8EO07q/lib/python3.7/site-packages (from Faker) (1.3) Installing collected packages: Faker Successfully installed Faker-2.0.3 Initialization In [2]: from faker import Faker faker = Faker () Implementation Create a random integer: In [3]: faker . random_int ( min = 1 , max = 8 , step = 1 ) Out[3]: 2 Run it a second time and it might give another integer: In [4]: faker . random_int ( min = 1 , max = 8 , step = 1 ) Out[4]: 2 Or we can define a list with some elements and create random element with Faker: In [5]: characters = [ \"Mario\" , \"Luigi\" , \"Peach\" , \"Toad\" ] faker . random_element ( characters ) Out[5]: 'Luigi' In [6]: faker . random_element ( characters ) Out[6]: 'Toad' Additionally, we can create a date between a start and end date, i.e. the last month: In [7]: import datetime from dateutil.relativedelta import relativedelta date_end = datetime . datetime . now () date_start = date_end + relativedelta ( months =- 1 ) In [8]: faker . date_between_dates ( date_start = date_start , date_end = date_end ) Out[8]: datetime.date(2019, 10, 27) In [9]: faker . date_between_dates ( date_start = date_start , date_end = date_end ) Out[9]: datetime.date(2019, 10, 12) Event generator Let's put this together to make an event generator that can be used to create fake data for any other project. In [10]: import json CHARACTERS = [ \"Mario\" , \"Luigi\" , \"Peach\" , \"Toad\" ] DATE_END = datetime . datetime . now () DATE_START = DATE_END + relativedelta ( months =- 1 ) MAX_LIVES = 100 MIN_LIVES = 1 NUM_EVENTS = 2 def _generate_events (): \"\"\" Generate the metric data \"\"\" for _ in range ( NUM_EVENTS ): yield { \"character\" : faker . random_element ( CHARACTERS ), \"lives\" : faker . random_int ( min = MIN_LIVES , max = MAX_LIVES , step = 1 ), \"time\" : str ( faker . date_between_dates ( DATE_START , DATE_END )), } print ( json . dumps ( list ( _generate_events ()), indent = 2 )) [ { \"character\": \"Mario\", \"lives\": 12, \"time\": \"2019-10-01\" }, { \"character\": \"Peach\", \"lives\": 78, \"time\": \"2019-10-14\" } ] C'est tout! Please use this wiseley.", "tags": "posts", "url": "using-faker-to-generate-events.html", "loc": "using-faker-to-generate-events.html" }, { "title": "Moving from `unittest` to `pytest`", "text": "In my two previous articles Unittesting in a Jupyter notebook and Mocking in unittests in Python I have discussed the use of unittest and mock to run tests for a simple Castle and Character class. For the code behind this article please check Github . The classes Let's recap the classes first. Castle class The Castle class has a name, boss and world property and a simple method to determine if a character has access bases on his powerup. Note that the classes have been cleaned up since the last article. \"\"\" jj_classes/castle.py \"\"\" class Castle ( object ): \"\"\" Defines the Castle class \"\"\" def __init__ ( self , name ): \"\"\" Initialize the class \"\"\" self . _name = name self . _boss = \"Bowser\" self . _world = \"Grass Land\" def has_access ( self , character ): \"\"\" Check if character has access \"\"\" return character . powerup == \"Super Mushroom\" def get_boss ( self ): \"\"\" Returns the boss \"\"\" return self . boss def get_world ( self ): \"\"\" Returns the world \"\"\" return self . world @property def name ( self ): \"\"\" Name of the castle \"\"\" return self . _name @property def boss ( self ): \"\"\" Boss of the castle \"\"\" return self . _boss @property def world ( self ): \"\"\" World of the castle \"\"\" return self . _world Character class \"\"\" jj_classes/character.py \"\"\" class Character ( object ): \"\"\" Defines the character class \"\"\" def __init__ ( self , name ): \"\"\" Initialize the class \"\"\" self . _name = name self . _powerup = \"\" def get_powerup ( self ): \"\"\" Returns the powerup \"\"\" return self . powerup @property def name ( self ): \"\"\" Name of the character \"\"\" return self . _name @property def powerup ( self ): \"\"\" Powerup of the character \"\"\" return self . _powerup @powerup . setter def powerup ( self , powerup ): \"\"\" Sets the powerup \"\"\" self . _powerup = powerup Unittests In the previous articles, I've written two tests sets, one for Character and one for Character and Castle. Looking back at the tests now, I noticed that the Character/Castle testset is not very tidy so at the end I will simply have a set to test the Character class and one to test the Castle class. \"\"\" tests/charactertestclass.py \"\"\" import unittest import unittest.mock as mock try : from jj_classes.castle import Castle except ModuleNotFoundError : import sys , os sys . path . insert ( 0 , f \" { os . path . dirname ( os . path . abspath ( __file__ )) } /../\" ) from jj_classes.castle import Castle from jj_classes.character import Character class CharacterTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character class \"\"\" def setUp ( self ): \"\"\" Set the castle for the test cases \"\"\" self . castle = Castle ( \"Bowsers Castle\" ) def test_mock_access_denied ( self ): \"\"\" Access denied for star powerup \"\"\" mock_character = mock . Mock ( powerup = \"Starman\" ) self . assertFalse ( self . castle . has_access ( mock_character )) def test_mock_access_granted ( self ): \"\"\" Access granted for mushroom powerup \"\"\" mock_character = mock . Mock ( powerup = \"Super Mushroom\" ) self . assertTrue ( self . castle . has_access ( mock_character )) def test_default_castle_boss ( self ): \"\"\" Verifty the default boss is Bowser \"\"\" self . assertEqual ( self . castle . get_boss (), \"Bowser\" ) def test_default_castle_world ( self ): \"\"\" Verify the default world is Grass Land \"\"\" self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock a class method @mock . patch . object ( Castle , \"get_boss\" ) def test_mock_castle_boss ( self , mock_get_boss ): mock_get_boss . return_value = \"Hammer Bro\" self . assertEqual ( self . castle . get_boss (), \"Hammer Bro\" ) self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock an instance @mock . patch ( __name__ + \".Castle\" ) def test_mock_castle ( self , MockCastle ): instance = MockCastle instance . get_boss . return_value = \"Toad\" instance . get_world . return_value = \"Desert Land\" self . castle = Castle self . assertEqual ( self . castle . get_boss (), \"Toad\" ) self . assertEqual ( self . castle . get_world (), \"Desert Land\" ) # Mock an instance method def test_mock_castle_instance_method ( self ): # Boss is still Bowser self . assertNotEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) # Set a return_value for the get_boss method self . castle . get_boss = mock . Mock ( return_value = \"Koopa Troopa\" ) # Boss is Koopa Troopa now self . assertEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) def test_castle_with_more_bosses ( self ): multi_boss_castle = mock . Mock () # Set a list as side_effect for the get_boss method multi_boss_castle . get_boss . side_effect = [ \"Goomba\" , \"Boo\" ] # First value is Goomba self . assertEqual ( multi_boss_castle . get_boss (), \"Goomba\" ) # Second value is Boo self . assertEqual ( multi_boss_castle . get_boss (), \"Boo\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_boss_castle . get_boss ) def test_calls_to_castle ( self ): self . castle . has_access = mock . Mock () self . castle . has_access . return_value = \"No access\" # We should retrieve no access for everybody self . assertEqual ( self . castle . has_access ( \"Let me in\" ), \"No access\" ) self . assertEqual ( self . castle . has_access ( \"Let me in, please\" ), \"No access\" ) self . assertEqual ( self . castle . has_access ( \"Let me in, please sir!\" ), \"No access\" ) # Verify the length of the arguments list self . assertEqual ( len ( self . castle . has_access . call_args_list ), 3 ) if __name__ == \"__main__\" : unittest . main () \"\"\" tests/charactercastletestclass.py \"\"\" import unittest import unittest.mock as mock try : from jj_classes.castle import Castle except ModuleNotFoundError : import sys , os sys . path . insert ( 0 , f \" { os . path . dirname ( os . path . abspath ( __file__ )) } /../\" ) from jj_classes.castle import Castle from jj_classes.character import Character class CharacterCastleTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character and Castle class together \"\"\" @mock . patch ( __name__ + \".Castle\" ) @mock . patch ( __name__ + \".Character\" ) def test_mock_castle_and_character ( self , MockCharacter , MockCastle ): # Note the order of the arguments of this test MockCastle . name = \"Mocked Castle\" MockCharacter . name = \"Mocked Character\" self . assertEqual ( Castle . name , \"Mocked Castle\" ) self . assertEqual ( Character . name , \"Mocked Character\" ) def test_fake_powerup ( self ): character = Character ( \"Sentinel Character\" ) character . powerup = mock . Mock () character . powerup . return_value = mock . sentinel . fake_superpower self . assertEqual ( character . powerup (), mock . sentinel . fake_superpower ) def test_castle_with_more_powerups ( self ): self . castle = Castle ( \"Beautiful Castle\" ) multi_characters = mock . Mock () # Set a list as side_effect for the get_boss method multi_characters . get_powerup . side_effect = [ \"mushroom\" , \"star\" ] # First value is mushroom self . assertEqual ( multi_characters . get_powerup (), \"mushroom\" ) # Second value is star self . assertEqual ( multi_characters . get_powerup (), \"star\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_characters . get_powerup ) if __name__ == \"__main__\" : unittest . main () Rewriting the tests to use pytest In order to increase readability and reduce repetition, I favor pytest over unittest . PyTest offers some nice features to make writing tests faster and cleaner. Main differences Assert With unittest we always use self.assertEqual and the other variations. With pytest only assert is used. # unittest self . assertEqual ( 5 , \"five\" ) # pytest assert 5 == five Capturing errors is easier with PyTest, you can even assert the raised message in the same go. # unittest self . assertRaises ( StopIteration , multi_boss_castle . get_boss ) # pytest with pytest . raises ( StopIteration ): multi_boss_castle . get_boss () expected_error = r \"__init__\\(\\) missing 1 required positional argument: \\'name\\'\" with pytest . raises ( TypeError , match = expected_error ): castle = Castle () Mock # unittest # Mock a class method @mock . patch . object ( Castle , \"get_boss\" ) def test_mock_castle_boss ( self , mock_get_boss ): mock_get_boss . return_value = \"Hammer Bro\" self . assertEqual ( self . castle . get_boss (), \"Hammer Bro\" ) Make sure that for the mock functionality in PyTest the package pytest-mock is installed. # pytest # Mock a class method def test_mock_castle_boss ( self , mocker , castle ): mock_get_boss = mocker . patch . object ( Castle , \"get_boss\" ) mock_get_boss . return_value = \"Hammer Bro\" assert castle . get_boss (), \"Hammer Bro\" Fixtures PyTest has the functionality to add fixtures to your tests. They are normally placed in conftest.py in your tests folder where it will be automatically be picked up. For the sake of example, I have added the fixture to the same file as the test itself. In case of defining castle in each test like for unittest , we create a fixture for castle once and add it as an argument to the tests. # unittest def test_get_boss_returns_bowser ( self ): \"\"\" Test that the get_boss returns Bowser \"\"\" castle = Castle ( \"My Fixture Castle\" ) assert castle . get_boss () == 'Bowser' def test_get_world_returns_grass_land ( self ): \"\"\" Test that the get_boss returns Grass Land \"\"\" castle = Castle ( \"My Fixture Castle\" ) assert castle . get_world () == 'Grass Land' # pytest @pytest . fixture ( scope = 'session' ) def castle (): returns Castle ( \"My Fixture Castle\" ) def test_get_boss_returns_bowser ( self , castle ): \"\"\" Test that the get_boss returns Bowser \"\"\" assert castle . get_boss () == 'Bowser' def test_get_world_returns_grass_land ( self , castle ): \"\"\" Test that the get_boss returns Grass Land \"\"\" assert castle . get_world () == 'Grass Land' Conclusion In the end I have cleaned up my tests to only use pytest and I have introduced the fixture file conftest.py to reduce the complexity of the test files. \"\"\" tests/conftest.py \"\"\" import pytest CASTLE_NAME = \"Castle Name\" CHARACTER_NAME = \"Character Name\" from jj_classes.castle import Castle from jj_classes.character import Character @pytest . fixture ( scope = \"class\" ) def castle (): return Castle ( CASTLE_NAME ) @pytest . fixture ( scope = \"class\" ) def character (): return Character ( CHARACTER_NAME ) And the tests look like the following: \"\"\" tests/test_castle_class.py \"\"\" import pytest from jj_classes.castle import Castle class TestCastleClass : \"\"\" Defines the tests for the Castle class \"\"\" def test_init_sets_name ( self ): \"\"\" Test that init sets the name \"\"\" castle = Castle ( 'Test name' ) assert castle . name == \"Test name\" def test_init_error_when_no_name ( self ): \"\"\" Test that init fails without the name \"\"\" expected_error = r \"__init__\\(\\) missing 1 required positional argument: \\'name\\'\" with pytest . raises ( TypeError , match = expected_error ): castle = Castle () def test_has_access_true_with_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns True for Super Mushroom \"\"\" character . powerup = 'Super Mushroom' assert castle . has_access ( character ) def test_has_access_false_without_super_mushroom ( self , castle , character ): \"\"\" Test that has_access returns False for other powerups \"\"\" character . powerup = 'Not a mushroom' assert castle . has_access ( character ) is False def test_get_boss_returns_bowser ( self , castle ): \"\"\" Test that the get_boss returns Bowser \"\"\" assert castle . get_boss () == 'Bowser' def test_get_world_returns_grass_land ( self , castle ): \"\"\" Test that the get_boss returns Grass Land \"\"\" assert castle . get_world () == 'Grass Land' # Mock a class method def test_mock_castle_boss ( self , mocker , castle ): \"\"\" Test that the mocked get_boss returns overwritten value \"\"\" mock_get_boss = mocker . patch . object ( Castle , \"get_boss\" ) mock_get_boss . return_value = \"Hammer Bro\" assert castle . get_boss (), \"Hammer Bro\" # Mock an instance def test_mock_castle ( self , mocker ): \"\"\" Test that the mocked instance returns overwritten values \"\"\" instance = mocker . patch ( __name__ + \".Castle\" ) instance . get_boss . return_value = \"Toad\" instance . get_world . return_value = \"Desert Land\" castle = Castle assert castle . get_boss () == \"Toad\" assert castle . get_world () == \"Desert Land\" # Mock an instance method def test_mock_castle_instance_method ( self , mocker , castle ): \"\"\" Test that overwriting the instance method worked \"\"\" assert castle . get_boss () != \"Koopa Troopa\" castle . get_boss = mocker . Mock ( return_value = \"Koopa Troopa\" ) assert castle . get_boss () == \"Koopa Troopa\" def test_castle_with_more_bosses ( self , mocker ): \"\"\" Test that get_boss gets overwritten several times \"\"\" multi_boss_castle = mocker . Mock () multi_boss_castle . get_boss . side_effect = [ \"Goomba\" , \"Boo\" ] assert multi_boss_castle . get_boss () == \"Goomba\" assert multi_boss_castle . get_boss () == \"Boo\" with pytest . raises ( StopIteration ): multi_boss_castle . get_boss () def test_calls_to_castle ( self , mocker , castle ): \"\"\" Test that has_access gets called 3 times \"\"\" castle . has_access = mocker . Mock () castle . has_access . return_value = \"No access\" assert castle . has_access ( \"Let me in\" ) == \"No access\" assert castle . has_access ( \"Let me in, please\" ) == \"No access\" assert castle . has_access ( \"Let me in, please sir!\" ) == \"No access\" assert len ( castle . has_access . call_args_list ) == 3 \"\"\" tests/test_character_class.py \"\"\" import pytest from jj_classes.character import Character class TestCharacterClass : \"\"\" Defines the tests for the Character class \"\"\" def test_init_sets_name ( self ): \"\"\" Test that init sets the name \"\"\" character = Character ( 'Test name' ) assert character . name == \"Test name\" def test_init_error_when_no_name ( self ): \"\"\" Test that init fails without the name \"\"\" expected_error = r \"__init__\\(\\) missing 1 required positional argument: \\'name\\'\" with pytest . raises ( TypeError , match = expected_error ): character = Character () def test_get_powerup_returns_correct_value_when_not_set ( self , character ): \"\"\" Test that the get_powerup returns the right value when not set \"\"\" assert character . get_powerup () == \"\" def test_get_powerup_returns_correct_value_when_set ( self , character ): \"\"\" Test that the get_powerup returns the right value when set \"\"\" character . powerup = \"Fire Flower\" assert character . get_powerup () == \"Fire Flower\" def test_fake_powerup ( self , mocker , character ): \"\"\" Test that the powerup can be mocked \"\"\" character . powerup = mocker . Mock () character . powerup . return_value = mocker . sentinel . fake_superpower assert character . powerup () == mocker . sentinel . fake_superpower def test_characters_with_more_powerups ( self , mocker , castle ): \"\"\" Test that get_powerup gets overwritten several times \"\"\" multi_characters = mocker . Mock () multi_characters . get_powerup . side_effect = [ \"mushroom\" , \"star\" ] assert multi_characters . get_powerup () == \"mushroom\" assert multi_characters . get_powerup () == \"star\" with pytest . raises ( StopIteration ): multi_characters . get_powerup () $ pytest -v ================================================================================================= test session starts ================================================================================================== platform darwin -- Python 3 .7.3, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 -- /Users/jitsejan/.local/share/virtualenvs/blog-testing-KMgUXSdn/bin/python3.7m cachedir: .pytest_cache rootdir: /Users/jitsejan/code/blog-testing plugins: mock-1.11.1 collected 17 items tests/test_castle_class.py::TestCastleClass::test_init_sets_name PASSED [ 5 % ] tests/test_castle_class.py::TestCastleClass::test_init_error_when_no_name PASSED [ 11 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_true_with_super_mushroom PASSED [ 17 % ] tests/test_castle_class.py::TestCastleClass::test_has_access_false_without_super_mushroom PASSED [ 23 % ] tests/test_castle_class.py::TestCastleClass::test_get_boss_returns_bowser PASSED [ 29 % ] tests/test_castle_class.py::TestCastleClass::test_get_world_returns_grass_land PASSED [ 35 % ] tests/test_castle_class.py::TestCastleClass::test_mock_castle_boss PASSED [ 41 % ] tests/test_castle_class.py::TestCastleClass::test_mock_castle PASSED [ 47 % ] tests/test_castle_class.py::TestCastleClass::test_mock_castle_instance_method PASSED [ 52 % ] tests/test_castle_class.py::TestCastleClass::test_castle_with_more_bosses PASSED [ 58 % ] tests/test_castle_class.py::TestCastleClass::test_calls_to_castle PASSED [ 64 % ] tests/test_character_class.py::TestCharacterClass::test_init_sets_name PASSED [ 70 % ] tests/test_character_class.py::TestCharacterClass::test_init_error_when_no_name PASSED [ 76 % ] tests/test_character_class.py::TestCharacterClass::test_get_powerup_returns_correct_value_when_not_set PASSED [ 82 % ] tests/test_character_class.py::TestCharacterClass::test_get_powerup_returns_correct_value_when_set PASSED [ 88 % ] tests/test_character_class.py::TestCharacterClass::test_fake_powerup PASSED [ 94 % ] tests/test_character_class.py::TestCharacterClass::test_characters_with_more_powerups PASSED [ 100 % ] ================================================================================================== 17 passed in 0 .10s ==================================================================================================", "tags": "posts", "url": "moving-from-unittest-to-pytest.html", "loc": "moving-from-unittest-to-pytest.html" }, { "title": "Using SFTP with Spark", "text": "Pre-req Setup a simple SFTP server on your ( Ubuntu ) machine. Add a file with the following content: $ cat /home/ftpuser/data/sample.psv title | name | age mr | john doe | 34 mrs | jane doe | 30 ZIP the PSV file. $ zip /home/ftpuser/data/testpsv.zip /home/ftpuser/data/sample.psv Have Spark installed Have the JAR from https://github.com/springml/spark-sftp available in your Spark JAR directory. Setup context In [2]: # !wget https://repo1.maven.org/maven2/com/springml/sftp.client/1.0.3/sftp.client-1.0.3.jar In [3]: # !wget https://repo1.maven.org/maven2/com/springml/spark-sftp_2.10/1.0.2/spark-sftp_2.10-1.0.2.jar In [4]: from pyspark import SparkContext , SparkConf , SQLContext import os os . environ [ 'HADOOP_HOME' ] = '/opt/hadoop/' os . environ [ 'JAVA_HOME' ] = '/usr/lib/jvm/java-8-openjdk-amd64' os . environ [ 'PYSPARK_DRIVER_PYTHON' ] = 'python3' os . environ [ 'PYSPARK_PYTHON' ] = 'python3' os . environ [ 'LD_LIBRARY_PATH' ] = '/opt/hadoop/lib/native' os . environ [ 'SPARK_DIST_CLASSPATH' ] = \"/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*\" os . environ [ 'SPARK_HOME' ] = '/opt/spark/' conf = ( SparkConf () . setAppName ( \"Spark SFTP Test\" ) . set ( \"spark.hadoop.fs.sftp.impl\" , \"org.apache.hadoop.fs.sftp.SFTPFileSystem\" ) ) sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) Read a file directly into a dataframe In [5]: df = sqlContext \\ . read \\ . format ( \"com.springml.spark.sftp\" ) \\ . option ( \"host\" , os . environ . get ( 'FTP_HOST' )) \\ . option ( \"username\" , os . environ . get ( 'FTP_USER' )) \\ . option ( \"password\" , os . environ . get ( 'FTP_PASS' )) \\ . option ( \"fileType\" , \"csv\" ) \\ . option ( \"delimiter\" , \"|\" ) \\ . option ( \"quote\" , \" \\\" \" ) \\ . option ( \"escape\" , \" \\\\ \" ) \\ . option ( \"multiLine\" , \"true\" ) \\ . option ( \"inferSchema\" , \"false\" ) \\ . option ( \"header\" , \"false\" ) \\ . load ( \"/data/sample.psv\" ) \\ . show () +----+---+---+ | _c0|_c1|_c2| +----+---+---+ |john|doe| 35| |jane|doe| 32| +----+---+---+ Read a file as binary file and transform into a dataframe In [6]: from pyspark.sql.types import StructType , StructField , StringType schema = StructType ([ StructField ( \"first_name\" , StringType ()), StructField ( \"last_name\" , StringType ()), StructField ( \"age\" , StringType ()) ]) file_path = 'data/sample.psv' psv_df = sc \\ . binaryFiles ( f \"sftp:// { os . environ . get ( 'FTP_USER' ) } : { os . environ . get ( 'FTP_PASS' ) } @ { os . environ . get ( 'FTP_HOST' ) } / { file_path } \" ) \\ . map ( lambda row : row [ 1 ] . decode ( 'utf-8' ) . strip ()) \\ . flatMap ( lambda row : str ( row ) . split ( ' \\n ' )) \\ . map ( lambda row : str ( row ) . split ( '|' )) \\ . toDF ( schema = schema ) \\ . show () +----------+---------+---+ |first_name|last_name|age| +----------+---------+---+ | john| doe| 35| | jane| doe| 32| +----------+---------+---+ Extract a ZIP archive and parse the content into a dataframe The following snippet extracts a ZIP file in memory and returns the content of the first file. In [7]: import io import zipfile def zip_extract ( row ): file_path , content = row zfile = zipfile . ZipFile ( io . BytesIO ( content ), \"r\" ) files = [ i for i in zfile . namelist ()] return zfile . open ( files [ 0 ]) . read () . decode ( \"utf-8\" , errors = 'ignore' ) file_path = 'data/testpsv.zip' psv_df = sc \\ . binaryFiles ( f \"sftp:// { os . environ . get ( 'FTP_USER' ) } : { os . environ . get ( 'FTP_PASS' ) } @ { os . environ . get ( 'FTP_HOST' ) } / { file_path } \" ) \\ . map ( zip_extract ) \\ . map ( lambda row : row . strip ()) \\ . flatMap ( lambda row : str ( row ) . split ( ' \\n ' )) \\ . map ( lambda row : str ( row ) . split ( '|' )) \\ . toDF ( schema = schema ) \\ . show () +----------+---------+---+ |first_name|last_name|age| +----------+---------+---+ | john| doe| 35| | jane| doe| 32| +----------+---------+---+", "tags": "posts", "url": "sftp-with-spark.html", "loc": "sftp-with-spark.html" }, { "title": "Creating abstract classes with Lambda and Terraform", "text": "Objective Understand Python's abstract classes Implement a few concrete classes Understand basic software principles Create foundation for scalable serverless deployment with Lambda functions Introduction In this article I want to touch on different things. I have been working with AWS Lambda and Terraform for a while now and I am constantly trying to improve my knowledge. By making a small project I hope to give a good idea of a minimal Terraform deployment for people that are new to Terraform. In previous years I have been working as a Software Engineer where code was written in a more consistent way and software practices were followed according to the different Software Design Patterns ( Gang of Four Patterns ). I have noticed that in my current team everybody understands Python and PySpark, but often code is simply written as a sequential script rather than a proper structure with appropriate classes. This was not a problem when my team was still small, but since we are scaling here at MarketInvoice, more people are working on the code base. For example, we have a variety of AWS Lambda functions that crawl data from third parties (i.e. Jira, PagerDuty) which are all written as standalone functions with (a lot) of overlapping code. The idea is to create a common class with shared functionality which can then be used by the different crawlers and avoid repeating writing the same code over and over again ( Don't repeat yourself - Wikipedia ). By utilizing these software principles, I am sure my team will become more productive and can spend more time on adding more interesting data to our data platform and reduce fixing bugs. Initialization First I create the Git repository for this tutorial on Github. After creating the repo, I clone it to my code directory. ~/code $ git clone https://github.com/jitsejan/abstract-lambdas-terraform.git I have the alias that will open PyCharm from my command line so I can simply do the following: ~/code $ which charm /usr/local/bin/charm ~/code $ cd abstract-lambdas-terraform ~/code/abstract-lambdas-terraform $ charm . No IDE instance has been found. New one will be started. After opening PyCharm, make sure to add the .idea folder to the .gitignore . ~/code/abstract-lambdas-terraform $ echo .idea >> .gitignore Additionally, add the API key from IGDB (see below) to your shell, either add the export IGDB_KEY=\"<KEY>\" to ~/.bashrc or ~/.zshrc and run source on the file you've just changed to make variable available in your terminal and Python. ~/code/abstract-lambdas-terraform $ echo $IGDB_KEY <KEY> Testing I will use the public gaming database from IGDB: Free Video Game Database API as a source for this project. Check GitHub - public-apis/public-apis for other APIs. Make sure to sign-up and get your personal API key to interact with the IGDB API. Test the API with Postman to ensure the credentials are working and you understand the endpoints. Add your API key as user-key to the Headers field. At the query to the (raw) Body field. First I hit the platform endpoint to retrieve the ID for the N64 platform. Second I hit the games endpoint to filter for N64 games that contain Mario in the name. Implementation Single file Let's first start by recreating the Postman test in Python with the following code to get the platform ID for the N64. \"\"\" __main__.py \"\"\" import os import requests BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def main (): \"\"\" Main function \"\"\" response = requests . get ( BASE_URL . format ( endpoint = 'platforms' ), data = 'fields id; where abbreviation = \"N64\";' , headers = HEADERS ) print ( response . json ()) if __name__ == \"__main__\" : main () Running this in the terminal will print the response with a list containing ID 4. ~/code/abstract-lambdas-terraform $ python . [{ u 'id' : 4 }] After cleaning up the code by introducing a private function the code looks like this: \"\"\" __main__.py \"\"\" import os import requests BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_platform_id ( platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' response = requests . get ( BASE_URL . format ( endpoint = endpoint ), data = query . format ( abbr = platform_abbreviation ), headers = HEADERS ) return response . json ()[ 0 ][ \"id\" ] def main (): \"\"\" Main function \"\"\" print ( _get_platform_id ( \"N64\" )) if __name__ == \"__main__\" : main () Let's add the second endpoint function to retrieve the games for the platform like we did in the second Postman call. \"\"\" __main__.py \"\"\" import json import os import requests BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_games ( platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' response = requests . get ( BASE_URL . format ( endpoint = endpoint ), data = query . format ( name = name , platform_id = platform_id ), headers = HEADERS ) return response . json () def _get_platform_id ( platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' response = requests . get ( BASE_URL . format ( endpoint = endpoint ), data = query . format ( abbr = platform_abbreviation ), headers = HEADERS ) return response . json ()[ 0 ][ \"id\" ] def main (): \"\"\" Main function \"\"\" platform_id = _get_platform_id ( \"N64\" ) games = _get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () Running this will result in the list with games. Note that I use the jq tool to pretty print the JSON response from the API. ~/code/abstract-lambdas-terraform $ python . | jq [ { \"id\" : 47731 , \"name\" : \"Mario No Photopie\" } , { \"id\" : 3541 , \"name\" : \"Mario no Photopi\" } , { \"id\" : 2327 , \"name\" : \"Mario Party\" } , { \"id\" : 3475 , \"name\" : \"Dr. Mario 64\" } , { \"id\" : 44059 , \"name\" : \"Mario Artist: Talent Studio\" } , { \"id\" : 2329 , \"name\" : \"Mario Party 3\" } ] Introduce classes We now introduce the IGDBApiResolver class that contains the code for the two endpoints. The initial version looks like this: \"\"\" __main__.py \"\"\" import json import os import requests class IGDBApiResolver : \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' response = requests . get ( self . BASE_URL . format ( endpoint = endpoint ), data = query . format ( name = name , platform_id = platform_id ), headers = self . HEADERS ) return response . json () def _get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' response = requests . get ( self . BASE_URL . format ( endpoint = endpoint ), data = query . format ( abbr = platform_abbreviation ), headers = self . HEADERS ) return response . json ()[ 0 ][ \"id\" ] def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . _get_platform_id ( \"N64\" ) games = game_api . _get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () Because we use the requests call several times, it's good practice to create a function for the call. Additionally, a function that can be called from a class externally should not have the leading underscore. Let's separate private and public functions properly. \"\"\" __main__.py \"\"\" import json import os import requests class IGDBApiResolver : \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_api_json_response ( self , endpoint , data ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_platform_id ( \"N64\" ) games = game_api . get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () We split the class and the main function into two files. __main__.py contains \"\"\" __main__.py \"\"\" import json from igdbapiresolver import IGDBApiResolver def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_platform_id ( \"N64\" ) games = game_api . get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) if __name__ == \"__main__\" : main () and igdbapiresolver.py contains the class: \"\"\" igdbapiresolver.py \"\"\" import os import requests class IGDBApiResolver : \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def _get_api_json_response ( self , endpoint , data ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] Running this will result in the exact same list. ~/code/abstract-lambdas-terraform $ python . | jq [ { \"id\" : 47731 , \"name\" : \"Mario No Photopie\" } , { \"id\" : 3541 , \"name\" : \"Mario no Photopi\" } , { \"id\" : 2327 , \"name\" : \"Mario Party\" } , { \"id\" : 3475 , \"name\" : \"Dr. Mario 64\" } , { \"id\" : 44059 , \"name\" : \"Mario Artist: Talent Studio\" } , { \"id\" : 2329 , \"name\" : \"Mario Party 3\" } ] Another API.. I will use the GitHub - 15Dkatz/official_joke_api to add another API resolver to this project. Add the following class to jokeapiresolver.py : \"\"\" jokeapiresolver.py \"\"\" import os import requests class JokeApiResolver : \"\"\" Class definition for the JokeApiResolver \"\"\" BASE_URL = 'https://official-joke-api.appspot.com/ {endpoint} ' HEADERS = None def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () def get_programming_joke ( self ): endpoint = 'jokes/programming/random' return self . _get_api_json_response ( endpoint ) def get_random_joke ( self ): endpoint = 'random_joke' return self . _get_api_json_response ( endpoint ) and extend the __main__.py with the new API: \"\"\" __main__.py \"\"\" import json from igdbapiresolver import IGDBApiResolver from jokeapiresolver import JokeApiResolver def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_platform_id ( \"N64\" ) games = game_api . get_games ( platform_id , \"Mario\" ) print ( json . dumps ( games )) joke_api = JokeApiResolver () print ( json . dumps ( joke_api . get_random_joke ())) print ( json . dumps ( joke_api . get_programming_joke ())) if __name__ == \"__main__\" : main () Running the main function will give back the games as before, plus two jokes from the new API. ~/code/abstract-lambdas-terraform $ python . | jq [ { \"id\" : 47731 , \"name\" : \"Mario No Photopie\" } , { \"id\" : 3541 , \"name\" : \"Mario no Photopi\" } , { \"id\" : 2327 , \"name\" : \"Mario Party\" } , { \"id\" : 3475 , \"name\" : \"Dr. Mario 64\" } , { \"id\" : 44059 , \"name\" : \"Mario Artist: Talent Studio\" } , { \"id\" : 2329 , \"name\" : \"Mario Party 3\" } ] { \"setup\" : \"What does a female snake use for support?\" , \"type\" : \"general\" , \"id\" : 247 , \"punchline\" : \"A co-Bra!\" } [ { \"setup\" : \"Where do programmers like to hangout?\" , \"type\" : \"programming\" , \"id\" : 17 , \"punchline\" : \"The Foo Bar.\" } ] Introduce an Abstract Base Class (ABC) It's time to combine some logic in one central class, since we have the _get_api_json_response in both the classes we have introduced. It is good practice to combine common methods in a base class and inherit from that base class with subclasses for specific functionality for those classes. In this case we will make an ApiResolver base class and both the Game and Joke subclasses will inherit from that class. We will use the abc module in Python to create an Abstract Base Class which will help us defining common functions shared between classes common properties shared between classes necessary functions to be implemented by the subclass (concrete class) necessary properties to be implemented by the subclass (concrete class) (Note that abstract properties in Python > 3.5 are defined with the two decorators @property and @abstractmethod . Add the following to abstractapiresolver.py : \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . BASE_URL . format ( endpoint = endpoint ), data = data , headers = self . HEADERS ) return response . json () @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass We can now inherit from this ABC in the Joke API resolver as such: \"\"\" jokeapiresolver.py \"\"\" from abstractapiresolver import AbstractApiResolver class JokeApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the JokeApiResolver \"\"\" BASE_URL = 'https://official-joke-api.appspot.com/ {endpoint} ' HEADERS = None def get_programming_joke ( self ): endpoint = 'jokes/programming/random' return self . _get_api_json_response ( endpoint ) def get_random_joke ( self ): endpoint = 'random_joke' return self . _get_api_json_response ( endpoint ) and the API resolver for the games as: \"\"\" igdbapiresolver.py \"\"\" import os from abstractapiresolver import AbstractApiResolver class IGDBApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the IGDBApiResolver \"\"\" BASE_URL = 'https://api-v3.igdb.com/ {endpoint} ' HEADERS = { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] Note that I did not clean this file up yet and this will not work yet. Instead, when running the code now will result in the following error: ~/code/abstract-lambdas-terraform $ python3 . Traceback ( most recent call last ) : File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 193 , in _run_module_as_main \"__main__\" , mod_spec ) File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 85 , in _run_code exec ( code, run_globals ) File \"./__main__.py\" , line 19 , in <module> main () File \"./__main__.py\" , line 8 , in main game_api = IGDBApiResolver () TypeError: Can ' t instantiate abstract class IGDBApiResolver with abstract methods base_url, headers Because in abstractapiresolver.py we defined base_url and headers as abstract properties, each inherited class should define these properties, otherwise the class can not be instantiated. I have moved the properties BASE_URL and HEADERS to be a property of the ABC, so let's rewrite them as below. \"\"\" jokeapiresolver.py \"\"\" from abstractapiresolver import AbstractApiResolver class JokeApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the JokeApiResolver \"\"\" def get_programming_joke ( self ): endpoint = 'jokes/programming/random' return self . _get_api_json_response ( endpoint ) def get_random_joke ( self ): endpoint = 'random_joke' return self . _get_api_json_response ( endpoint ) @property def headers ( self ): return None @property def base_url ( self ): return 'https://official-joke-api.appspot.com/ {endpoint} ' and \"\"\" igdbapiresolver.py \"\"\" import os from abstractapiresolver import AbstractApiResolver class IGDBApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the IGDBApiResolver \"\"\" def get_games ( self , platform_id , name = None ): \"\"\" Get the games for a given platform and an optional name filter \"\"\" endpoint = 'games' if name : query = 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' else : query = 'fields name; where platforms = {platform_id} ; limit 50;' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( name = name , platform_id = platform_id )) def get_platform_id ( self , platform_abbreviation ): \"\"\" Get the platform ID for a given platform abbreviation \"\"\" endpoint = 'platforms' query = 'fields id; where abbreviation = \" {abbr} \";' return self . _get_api_json_response ( endpoint = endpoint , data = query . format ( abbr = platform_abbreviation ))[ 0 ][ 'id' ] @property def headers ( self ): return { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } @property def base_url ( self ): return 'https://api-v3.igdb.com/ {endpoint} ' In order to use base_url and headers we rewrite the ABC as: \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . base_url . format ( endpoint = endpoint ), data = data , headers = self . headers ) return response . json () @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass Now the code will run fine, since we implemented the correct properties for the concrete classes. Additionally, we can add a @abstractmethod , which is a method that we define in the ABC to enforce it to be implemented by each concrete class. Add the following to the ABC: @abstractmethod def get_data ( self ): pass so it becomes \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . base_url . format ( endpoint = endpoint ), data = data , headers = self . headers ) return response . json () @abstractmethod def get_data ( self ): pass @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass ~/code/abstract-lambdas-terraform $ python3 . | jq Traceback ( most recent call last ) : File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 193 , in _run_module_as_main \"__main__\" , mod_spec ) File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py\" , line 85 , in _run_code exec ( code, run_globals ) File \"./__main__.py\" , line 19 , in <module> main () File \"./__main__.py\" , line 8 , in main game_api = IGDBApiResolver () TypeError: Can ' t instantiate abstract class IGDBApiResolver with abstract methods get_data Rewrite the classes The abstract class contains the shared method to get the API response, the get_data method that each class should implement, as well as the base_url , headers and endpoints abstract properties. \"\"\" abstractapiresolver.py \"\"\" from abc import ABCMeta , abstractmethod import requests class AbstractApiResolver ( metaclass = ABCMeta ): \"\"\" Class definition of the AbstractApiResolver \"\"\" def _get_api_json_response ( self , endpoint , data = None ): \"\"\" Return the API JSON response \"\"\" response = requests . get ( url = self . base_url . format ( endpoint = endpoint ), data = data , headers = self . headers ) return response . json () @abstractmethod def get_data ( self ): pass @property @abstractmethod def headers ( self ): pass @property @abstractmethod def base_url ( self ): pass @property @abstractmethod def endpoints ( self ): pass Note that I wrote the get_data function differently in the Joke API resolver compared to the Game API resolver, just for the sake of example. This get_data function is generic enough to put in the ABC instead of defining it for each subclass since the endpoints are defined the same way, but that's an easy fix! \"\"\" jokeapiresolver.py \"\"\" from abstractapiresolver import AbstractApiResolver class JokeApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the JokeApiResolver \"\"\" def get_data ( self , endpoint , ** params ): \"\"\" Get data from the API \"\"\" url = self . endpoints [ endpoint ][ 'url' ] return self . _get_api_json_response ( url ) @property def base_url ( self ): return 'https://official-joke-api.appspot.com/ {endpoint} ' @property def endpoints ( self ): return { 'get_programming_joke' : { 'data' : None , 'url' : 'jokes/programming/random' , }, 'get_random_joke' : { 'data' : None , 'url' : 'random_joke' , } } @property def headers ( self ): return None \"\"\" igdbapiresolver.py \"\"\" import os from abstractapiresolver import AbstractApiResolver class IGDBApiResolver ( AbstractApiResolver ): \"\"\" Class definition for the IGDBApiResolver \"\"\" def get_data ( self , endpoint , ** params ): \"\"\" Get data from the API \"\"\" url = self . endpoints [ endpoint ][ 'url' ] data = self . endpoints [ endpoint ][ 'data' ] . format ( ** params ) return self . _get_api_json_response ( url , data ) @property def base_url ( self ): return 'https://api-v3.igdb.com/ {endpoint} ' @property def endpoints ( self ): return { 'get_platform_id' : { 'data' : 'fields id; where abbreviation = \" {abbr} \";' , 'url' : 'platforms' , }, 'get_games_for_platform' : { 'data' : 'fields name; where platforms = {platform_id} ; limit 50;' , 'url' : 'games' , }, 'get_games_for_platform_with_name' : { 'data' : 'fields name; where name ~ *\" {name} \"* & platforms = {platform_id} ; limit 50;' , 'url' : 'games' , }, } @property def headers ( self ): return { 'user-key' : os . environ . get ( 'IGDB_KEY' , '' ) } Finally, we need to update our __main__.py to call the two APIs with the right methods. \"\"\" __main__.py \"\"\" import json from igdbapiresolver import IGDBApiResolver from jokeapiresolver import JokeApiResolver def main (): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_data ( endpoint = \"get_platform_id\" , abbr = \"N64\" )[ 0 ][ 'id' ] games = game_api . get_data ( endpoint = \"get_games_for_platform_with_name\" , platform_id = platform_id , name = \"Mario\" ) print ( json . dumps ( games )) joke_api = JokeApiResolver () print ( json . dumps ( joke_api . get_data ( endpoint = 'get_random_joke' ))) print ( json . dumps ( joke_api . get_data ( endpoint = 'get_programming_joke' ))) if __name__ == \"__main__\" : main () Lambda functions This article is not about the use or the definition of a Lambda function, but simply an article to show how to simplify Lambda functions and layers with user defined classes. Get a programming joke Let's define a function that calls the Joke API and get a programming joke. lambda_handler is the default handler for a Lambda function, which we will call with an empty event and context . \"\"\" get_programming_joke.py \"\"\" import json from jokeapiresolver import JokeApiResolver def lambda_handler ( event , context ): \"\"\" Main function \"\"\" joke_api = JokeApiResolver () print ( json . dumps ( joke_api . get_data ( endpoint = \"get_programming_joke\" ))) if __name__ == \"__main__\" : lambda_handler ({}, {}) Get the Mario games for N64 In this case we add the platform_abbr and name to the event. This means we can keep the Lambda function the same and simply update these two parameters to get different sets of games. \"\"\" get_games_for_platform.py \"\"\" import json from igdbapiresolver import IGDBApiResolver def lambda_handler ( event , context ): \"\"\" Main function \"\"\" game_api = IGDBApiResolver () platform_id = game_api . get_data ( endpoint = \"get_platform_id\" , abbr = event . get ( \"platform_abbr\" , \"\" ) )[ 0 ][ \"id\" ] games = game_api . get_data ( endpoint = \"get_games_for_platform_with_name\" , platform_id = platform_id , name = event . get ( \"name\" , \"\" ), ) print ( json . dumps ( games )) if __name__ == \"__main__\" : event = { \"platform_abbr\" : \"N64\" , \"name\" : \"Mario\" } lambda_handler ( event , {}) Terraform See my article on Creating a Lambda function with Terraform to upload a Looker view | JJ's World for a simple Terraform introduction. In this article I will simply show the steps to deploy the following: get_games_for_platform Lambda function get_programming_joke Lambda function Lambda layer with Abstract class AbstractApiResolver Concrete class IGDBApiResolver Concrete class JokeApiResolver Initialization ~/code/abstract-lambdas-terraform $ terraform init ... ~/code/abstract-lambdas-terraform $ terraform workspace new dev Created and switched to workspace \"dev\" ! ~/code/abstract-lambdas-terraform $ touch main.tf ~/code/abstract-lambdas-terraform $ touch variables.tf Structure ~/code/abstract-lambdas-terraform $ tree . . ├── README.md ├── images │ ├── postman_get_games.png │ └── postman_get_platform.png ├── main.tf ├── sources │ ├── lambda-functions │ │ ├── get-games-for-platform │ │ │ └── get_games_for_platform.py │ │ └── get-programming-joke │ │ └── get_programming_joke.py │ └── lambda-layers │ └── abstract-layer │ └── python │ └── abstractlayer │ ├── __init__.py │ ├── __main__.py │ ├── abstractapiresolver.py │ ├── igdbapiresolver.py │ └── jokeapiresolver.py ├── terraform.tfstate.d │ └── dev └── variables.tf Build Lambda layer In order to create the right lambda.zip for the Lambda layer, we create a dist folder and copy the content of the python folder inside. Additionally we need to install all the requirements (in this case only requests ) for AWS with Docker. We add everything to the ZIP file inside the dist folder. ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ mkdir -p dist/python ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ docker run --rm -v $( PWD ) :/foo -w /foo lambci/lambda:build-python3.7 \\ pip install -r requirements.txt -t ./dist/python ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ cp -r ./python/* ./dist ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer $ cd dist ~/code/abstract-lambdas-terraform/sources/lambda-layers/abstract-layer/dist $ zip -rD lambda.zip . adding: python/abstractlayer/jokeapiresolver.py ( deflated 55 % ) adding: python/abstractlayer/abstractapiresolver.py ( deflated 61 % ) adding: python/abstractlayer/__init__.py ( stored 0 % ) adding: python/abstractlayer/igdbapiresolver.py ( deflated 62 % ) adding: python/abstractlayer/__main__.py ( deflated 56 % ) Lambda functions For the Lambda functions we make sure we first rename the files to lambda.py since that's the default filename AWS expects as default module name. Then we create again a dist folder, add the lambda.py and compress the file. ~/code/abstract-lambdas-terraform/sources/lambda-functions/get-games-for-platform $ mkdir dist ~/code/abstract-lambdas-terraform/sources/lambda-functions/get-games-for-platform $ cp lambda.py dist/ ~/code/abstract-lambdas-terraform/sources/lambda-functions/get-games-for-platform $ cd dist && zip -rD lambda.zip . adding: lambda.py ( deflated 50 % ) Validate Run terraform validate to ensure all files are there and there is no configuration issue. ~/code/abstract-lambdas-terraform $ terraform validate Success! The configuration is valid. Plan & apply ~/code/abstract-lambdas-terraform $ terraform plan ... ~/code/abstract-lambdas-terraform $ terraform apply ... Apply complete! Resources: 5 added, 0 changed, 0 destroyed. Results Overview of the two Lambda functions: Running the Lambda function: Conclusion Using abstract classes makes it easier to create complex software, even when running serverless code on AWS. By using these type of classes you can enforce the developer that creates a new concrete class to implement all the necessary methods and properties to ensure code consistency. The approach in my team would be to create all classes as part of one Lambda layer that contains the Python module with the different abstract classes (APIConnector, FTPConnector, etc) and all concrete classes (CompaniesHouseApiConnector, ExperianApiConnector, etc). The Lambda functions to execute the actual data crawl can remain short and simple making it more scalable and flexible to add new endpoints and therefore new datasets to the data lake. Reference GitHub - jitsejan/abstract-lambdas-terraform", "tags": "posts", "url": "create-abstract-classes-with-lambda-and-terraform.html", "loc": "create-abstract-classes-with-lambda-and-terraform.html" }, { "title": "Find and delete empty columns in Pandas dataframe", "text": "# Find the columns where each value is null empty_cols = [ col for col in df . columns if df [ col ] . isnull () . all ()] # Drop these columns from the dataframe df . drop ( empty_cols , axis = 1 , inplace = True )", "tags": "posts", "url": "find-and-delete-empty-columns-pandas-dataframe.html", "loc": "find-and-delete-empty-columns-pandas-dataframe.html" }, { "title": "Setting up Spark with minIO as object storage", "text": "Objective Install Spark Install Hadoop Install minIO server Extend Ansible configuration Introduction At work we use AWS S3 for our datalake. Since I am working on some data projects, I would like to have a similar experience, but without AWS and simply on my own server. This is the reason why I chose minIO as object storage, it's free, runs on Ubuntu and is compatible with the AWS S3 API. Installation The Ansible configuration from my previous blog post already installed an older version of Spark. During my several attempts to get minIO working with Spark, I had to try different Hadoop versions, Spark and AWS libraries to make the installation work. I used the latest version from the Spark download page , which at the time of writing is 2.4.3 . Since I have to use the latest Hadoop version ( 3.1.2 ), I have to get the Spark download without Hadoop. The current Spark only support Hadoop version 2.7 or lower. For all the AWS libraries that are needed, I could only get the integration to work with version 1.11.534 . The following Java libraries are needed to get minIO working with Spark: hadoop-aws-3.1.2.jar aws-java-sdk-1.11.534.jar aws-java-sdk-core-1.11.534.jar aws-java-sdk-dynamodb-1.11.534.jar aws-java-sdk-kms-1.11.534.jar aws-java-sdk-s3-1.11.534.jar httpclient-4.5.3.jar joda-time-2.9.9.jar To run the minIO server, I first create a minIO user and minIO group. Additionally I create the data folder that minIO will store the data. After preparing the environment I install minIO and add it as a service /etc/systemd/system/minIO.service . [ Unit ] Description = minIO Documentation = https://docs.minIO.io Wants = network-online.target After = network-online.target AssertFileIsExecutable = /usr/local/bin/minIO [ Service ] WorkingDirectory = /usr/local/ User = minIO Group = minIO PermissionsStartOnly = true EnvironmentFile = /etc/default/minIO ExecStartPre = /bin/bash -c \"[ -n \\\" ${ minIO_VOLUMES } \\\" ] || echo \\\"Variable minIO_VOLUMES not set in /etc/default/minIO\\\"\" ExecStart = /usr/local/bin/minIO server $minIO_OPTS $minIO_VOLUMES # Let systemd restart this service only if it has ended with the clean exit code or signal. Restart = on-success StandardOutput = journal StandardError = inherit # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE = 65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec = 0 # SIGTERM signal is used to stop minIO KillSignal = SIGTERM SendSIGKILL = no SuccessExitStatus = 0 [ Install ] WantedBy = multi-user.target The minIO environment file located at /etc/default/minIO contains the configuration for the volume, the port and the credentials. # minIO local/remote volumes. minIO_VOLUMES=\"/minIO-data/\" # minIO cli options. minIO_OPTS=\"--address :9091 \" minIO_ACCESS_KEY=\"mykey\" minIO_SECRET_KEY=\"mysecret\" $ minIO version Version: 2019 -06-27T21:13:50Z Release-Tag: RELEASE.2019-06-27T21-13-50Z Commit-ID: 36c19f1d653adf3ef70128eb3be1a35b6b032731 For the complete configuration, check the role in Github. Code The important bit is setting the right environment variables. Make sure the following variables are set: export HADOOP_HOME = /opt/hadoop export JAVA_HOME = /usr/lib/jvm/java-8-openjdk-amd64 export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin export PATH = $PATH : $HADOOP_HOME /bin export LD_LIBRARY_PATH = $HADOOP_HOME /lib/native export SPARK_DIST_CLASSPATH = $( hadoop classpath ) After setting the environment variables, we need to make sure we connect to the minIO endpoint and set the credentials. Make sure the path.style.access is set to True . from pyspark import SparkContext , SparkConf , SQLContext conf = ( SparkConf () . setAppName ( \"Spark minIO Test\" ) . set ( \"spark.hadoop.fs.s3a.endpoint\" , \"http://localhost:9091\" ) . set ( \"spark.hadoop.fs.s3a.access.key\" , os . environ . get ( 'minIO_ACCESS_KEY' )) . set ( \"spark.hadoop.fs.s3a.secret.key\" , os . environ . get ( 'minIO_SECRET_KEY' )) . set ( \"spark.hadoop.fs.s3a.path.style.access\" , True ) . set ( \"spark.hadoop.fs.s3a.impl\" , \"org.apache.hadoop.fs.s3a.S3AFileSystem\" ) ) sc = SparkContext ( conf = conf ) . getOrCreate () sqlContext = SQLContext ( sc ) Once this is done, we can simply access the bucket and read a text file (given that this bucket and text file exists), and we are able to write a dataframe to minIO. print ( sc . wholeTextFiles ( 's3a://datalake/test.txt' ) . collect ()) # Returns: [('s3a://datalake/test.txt', 'Some text\\nfor testing\\n')] path = \"s3a://user-jitsejan/mario-colors-two/\" rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . write . csv ( path ) Todo Currently there seems to be an issue with reading small files, it will give a Parquet error that the files are not big enough to read. It seems more like a library issue, so I should just make sure I only work on big data. Credits Thanks to atosatto for the Ansible role and minIO for the great example.", "tags": "posts", "url": "setting-up-spark-with-minio-as-object-storage.html", "loc": "setting-up-spark-with-minio-as-object-storage.html" }, { "title": "Creating an Ansible playbook to provision my Ubuntu VPS", "text": "Objective Clean installation on Ubuntu 18.04 with the following requirements: Create user account for myself from the root account Install basic applications ( curl , java , etc) Install Jupyter notebooks Install Spark Install nginx Introduction The goal is to provision my VPS at SSDNodes with all the tools I need to develop Spark and Python code in Jupyter notebooks. While in previous installations I have been using both Docker and Kubernetes to make it easy to spin up the Spark notebooks, it would still require me to install Docker, Kubernetes and all relevant software after manually creating the user account and doing all the boring work. Plus, installation of Kubernetes is always a challenge and a lengthy process. Since I don't need any containerization now, I would expect that a clean installation without Docker and Kubernetes should be more than enough to get my development environment up and running. At work I have introduced Terraform to deploy our data platform on AWS (and a few resources on Azure ). While Terraform works great for setting up the services for all our data pipelines, APIs and machine learning models, I wanted to understand more about Ansible to see if it would be useful too. In short, Terraform should be used for setting up resources and services and Ansible should be used to provision single (or multiple) instances. For example, you would create an EC2 instance automatically with Terraform and install all relevant applications on the EC2 machine with Ansible. WHY ANSIBLE? Working in IT, you're likely doing the same tasks over and over. What if you could solve problems once and then automate your solutions going forward? Ansible is here to help. Preparation Installation of Ansible on Mac (and Linux) is easy with Brew . $ brew install ansible After installation we can verify if Ansible is available by checking the version. In my case I have installed version 2.8.1 . $ ansible --version ansible 2 .8.1 config file = /Users/jitsejan/.ansible.cfg configured module search path = [ '/Users/jitsejan/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/Cellar/ansible/2.8.1_1/libexec/lib/python3.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 3 .7.3 ( default, Jun 19 2019 , 07 :38:49 ) [ Clang 10 .0.1 ( clang-1001.0.46.4 )] Finally, you need to make sure the machine you want to provision is defined in the Ansible configuration /etc/ansible/hosts . [ssdnodes] development.jitsejan.com To make it easier to connect to the host, it is smart to create a key pair on the local computer and copy it to the host. This way you do not need to provide login details when you run the Ansible tool. Create the keypair with ssh-keygen : $ ssh-keygen -t rsa -b 4096 Copy the key to the host to be deployed. It will ask you to provide the password for the root, but it will be the only time we need the password. $ ssh-copy-id root@development.jitsejan.com And that is all there is to it to get started with Ansible. Structure The directory of the repository at the time of writing looks like the following. Please note that this might not be the best structure, but it gave me a good head-start to work with the file structure, local and global variables and templating. ├── README.md ├── provision_vps.yaml └── roles ├── base │ ├── tasks │ │ └── main.yml │ └── vars │ └── main.yml ├── common │ └── tasks │ └── main.yml ├── jupyter │ ├── defaults │ │ └── main.yml │ ├── files │ │ ├── custom.css │ │ └── requirements.txt │ ├── tasks │ │ └── main.yml │ ├── templates │ │ ├── etc │ │ │ └── systemd │ │ │ └── system │ │ │ └── jupyter.service │ │ └── jupyterhub_config.py.j2 │ └── vars │ └── main.yml ├── nginx │ ├── tasks │ │ └── main.yml │ └── templates │ └── nginx.conf.j2 └── spark ├── tasks │ └── main.yml └── vars └── main.yml Breakdown Playbook The core of an Ansible deployment is a playbook (or multiple). The playbook contains all the different items that should be deployed. In Ansible these items are divided by roles , each role can relate to a tool, a service, a user or anything else that makes sense to group together. In my repository I have called the playbook `provision_vps.yaml'. The file is small, since I have used multiple roles to be loaded when running the playbook. The first part of the file contains the host and the user you want to run the deployment with. I have only specified one host, since I only have one VPS to deploy. - hosts : ssdnodes remote_user : root For each host, we can define global variables by defining them in the playbook under vars . This makes the variables available in all the different roles. If the variable is only to be used within a role, it is smarter to define it at the role level. In my case I want to deploy a Jupyter notebook and therefore I should define the parameters on playbook level, so the variables become available for both the Jupyter and nginx installation. I define the domain, the name and port to host the notebook from, and I supply the location of the SSL certificate and key. vars : jupyter_domain : dev.jitsejan.com jupyter_name : 'dev' jupyter_port : 8181 key_out : /home/jupyter/.jupyter/jupyter.key cert_out : /home/jupyter/.jupyter/jupyter.cert If you want to prompt the user for input for certain variables you can use vars_prompt . In my playbook I want to ask the user (me) for the username and password to create an account on the Ubuntu box for me. It will ask to confirm both the user and password before it continues to run the playbook. vars_prompt : - name : \"user_name\" prompt : \"Enter a name for the new user\" private : no confirm : no - name : \"user_password\" prompt : \"Enter a password for the new user\" private : yes encrypt : \"sha512_crypt\" confirm : yes salt_size : 7 Finally, I define the roles to be executed as part of the playbook. This is the part where you can easily add new roles when you want to add more services to your machine. roles : - base - common - { role : 'spark' , spark_version : '2.2.1' , hadoop_version : '2.7' } - jupyter - { role : 'nginx' , app_ip : localhost , } As you can see, you can call the role as simple parameter, or call the role by specifying the role and any additional variable. For the Spark role we supply the Spark and Hadoop version. For the Nginx role we indicate that our app will only run on localhost. (We use Nginx as proxy to make it accessible on a public address). Roles I have divided the roles along the way and I do not guarantee this is the most logical way of doing things. It makes sense for me now, so I am just going to go with it. The goal is to have building blocks to make the final application run smoothly with all dependencies in the right place. Base role The first role I created is the base role which will take care of the following: Check if all necessary packages are installed. Create the personal user account based on the prompt of the playbook. Add the user to sudo and create a password less login on the local computer. The tasks to be executed are defined inside the role in the tasks folder. Ansible expect the main.yml inside this folder to have all the steps for the role. Variables can be defined in two folders, either in vars/main.yml or in defaults/main.yml . The defaults, as the name says, contains the default value for certain variables. These can be overwritten by variables defines in the vars folder. Note that some of the variables are passed through from the playbook and are not defined on this level. To install a package the apt module can be used. Variables are passed in using the double curly brackets ( {{ }} ). The following is part of roles/base/tasks/main.yml . I will leave the rest out to keep this article short. - name : Ensure necessary packages are installed apt : name : \"{{ base_pkgs }}\" state : present update_cache : yes ... To keep things simple, I have only defined one package to be installed in roles/base/vars/main.yml . I prefer mosh over ssh since my internet is often buggy and I don't like to keep on reconnecting with ssh. base_pkgs : - mosh Common role The common role currently only installs openjdk-8-jdk , openjdk-8-jre-headless and exports the JAVA_HOME variable. Java 8 is needed to make Spark work, since it is not compatible (yet) with Java 11. Jupyter role At this point it gets more interesting. To deploy Jupyter on the VPS some more advanced steps are needed. Just to recap, the structure of the Jupyter role looks like this: ├── defaults │ └── main.yml ├── files │ ├── custom.css │ └── requirements.txt ├── tasks │ └── main.yml ├── templates │ ├── etc │ │ └── systemd │ │ └── system │ │ └── jupyter.service │ └── jupyterhub_config.py.j2 └── vars └── main.yml The defaults contain the default values for the Jupyter deployment. The main.yml has the following content, which basically tells the system that for the notebook server I want to use Python 3 as default and only run it from localhost. --- jupyter_python_executable : python3 jupyter_package_manager : pip3 jupyter_package_manager_become : no jupyter_package_state : latest jupyter_password : 'sha1:b3af1b4adee9:9e86cb52435cc24db0b487451c10f6d348734645' jupyter_open_browser : false jupyter_ip : 'localhost' Under the files folder you can put the files you wish to copy to the VPS. In my case I want to copy the custom.css and requirements.txt to the VPS. The CSS file contains the custom layout I like for my notebooks, the requirements file obviously contains the Python packages I wish to use. The tasks/main.yml is pretty lengthy, so I will summarize what it does: Ensure important Jupyter dependencies are installed Create a Jupyter user and group Create the folder for the data and the virtual environment Copy the requirements.txt and install the libraries Create the Jupyter configuration Create the SSL certificates Copy the custom CSS file to the server Make and run the jupyter.service One of the nifty things of Ansible is conditional execution, only run certain tasks if a condition is met. For example, I will not recreate the virtual environment if it already exists, as shown in the code snippet below. - name : Check to see if the Jupyter environment exists stat : path : /data/jupyter register : environment_exists become_user : jupyter When the folder exists already, it will register environment_exists that can be used under the when condition with environment_exists.stat.exists == False . - block : - name : Install virtual environment pip : name : virtualenv executable : pip3 state : latest - name : Create virtualenv root directory file : path : /data/jupyter state : directory owner : jupyter group : jupyter mode : 0755 - name : Set up virtual environment shell : virtualenv -p python3 /data/jupyter become_user : jupyter changed_when : no when : environment_exists.stat.exists == False In the templates folder I have used two different approaches. The first one is to replicate the location of the folder on the server by creating a similar directory structure. The jupyter.service will be placed under /etc/systemd/system/ . The second approach is the preferred method and doesn't create the annoying directory structure. It uses Jinja templates to easily configure files dynamically with variables. I have defined the structure of a jupyterhub_config.py with variables that will be filled in automatically by Ansible. As you might remember, some of these variables are part of the defaults of this role, while some other where defined on playbook level. # jupyterhub_config.py c . NotebookApp . open_browser = {{ jupyter_open_browser }} c . NotebookApp . ip = '{{ jupyter_ip }}' c . NotebookApp . port = {{ jupyter_port }} c . NotebookApp . password = '{{ jupyter_password }}' c . NotebookApp . allow_origin = '*' c . NotebookApp . allow_remote_access = True c . NotebookApp . certfile = '{{ cert_out }}' c . NotebookApp . keyfile = '{{ key_out }}' c . NotebookApp . notebook_dir = '/data/notebooks/' c . InteractiveShell . ast_node_interactivity = \"all\" In the vars/main.yml I have only defined the Jupyter packages that I wish to install, the location of the Jupyter configuration and the variables to create the SSL certificate for the server. Nginx role The role for Nginx is relatively simple and only contains a couple of steps. Install Nginx Add reverse proxy for the Jupyter server Create the symlink to add the proxy to available websites Restart Nginx To easily create the Nginx configuration file, I have created a template that will set the domain name, the port and the certificates. Since I am not aiming to use Nginx to host anything else, I haven't created any additional templates yet. Spark role Currently the Spark role is the final step of my playbook. This role will install Spark and download all necessary JAR files I need to work on my Data Engineering tasks. Todo There are a few things that I might still modify in my approach, since I have noticed that I could parameterize more steps in the playbook. An annoying issue is that the Jupyter server does not pick the right environment variables, so I need to think of a good way to set them before running the notebooks. Right now my workaround is to hardcode the paths to SPARK_HOME and JAVA_HOME inside the notebook itself. Secondly, the certificate that I create in this playbook is not trusted by the browser, because it is self-signed. I have used openssl , but I should look into letsencrypt or use the acme tool that Ansible provides. Conclusion Ansible is a fantastic tool to automate the provisioning of a VPS (or any other fresh Ubuntu installation). It took me often over several hours, and with Kubernetes days, to setup the Ubuntu box, but using Ansible I simply enter the username and password and after 10 minutes the system is ready. Another great benefit is the use of Jinja templates, which should be very familiar for a Python developer. My goal is to add more roles and keep everything up to date in my GitHub repo .", "tags": "posts", "url": "creating-ansible-deployment-for-ubuntu-vps.html", "loc": "creating-ansible-deployment-for-ubuntu-vps.html" }, { "title": "Creating a Lambda function with Terraform to upload a Looker view", "text": "Objective Using the Terraform tool, I will create a simple example where I upload the output from a look from our BI tool Looker to AWS S3 in CSV format. By automating the export of a Looker query to S3, we could make certain data publicly available with a regular update to make sure the data contains the latest changes. Introduction Last week I've introduced Terraform to the company. I have worked with Azure Resource Manager before, both in my previous job and one of the first tasks I had here at MI and I have some exposure to AWS CloudFormation . Since the data platform I have created is cross-platform (a hybrid solution with both Azure and AWS), I thought it would be wise to not use ARM nor CloudFormation, but go a level higher by using Terraform. Terraform supports many cloud providers and can help to define the infrastructure of both Azure and AWS in a few configuration files (#InfrastructureAsCode). By using Terraform we make sure all services are added to the configuration and checked in to version control. In case something breaks, or in the worst case we have a disaster, we can easily recreate the platform with Terraform. Structure The structure of this project will look like this: ├── deploy.sh ├── initial_run.py ├── main.tf ├── sources │ └── lambda-functions │ └── looker-upload │ ├── lambda.py │ └── requirements.txt ├── terraform.tfvars └── variables.tf Requirements Terraform Currently I have installed Terraform on my MacBooks and my VPS. For Mac the installation is straightforward using brew : brew install terraform For Ubuntu we need to download the ZIP-file, extract it and add it manually: sudo apt-get install unzip wget https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip unzip terraform_0.11.13_linux_amd64.zip sudo mv terraform /usr/local/bin ╭─ ~/code/data-engineer-solutions ╰─ terraform version Terraform v0.11.13 Other You should have an AWS account and have the credentials available in the default ~/.aws/credentials . You should have [Download Python | Python.org](https://www.python.org/downloads/. I am using version 3.7.3 at the moment of writing. You should have Docker Desktop for Mac and Windows | Docker installed. Implementation The core of the infrastructure is defined in main.tf . The first element is the provider that is being used. # The AWS provider provider \"aws\" { region = \"${var.region}\" } Secondly, we need to create the policy and the role for executing the Looker Lambda function. The role should be able to start a Lambda function, creating CloudWatch logs, get parameters from SSM and interact with S3. The resources are not limited in this example, but of course it is good practice to keep the permissions of a role limited and reduce the blast radius. # Define the policy for the role resource \"aws_iam_role_policy\" \"policy-lambda\" { name = \"dev-jwat-policy-lambda\" role = \" ${ aws_iam_role . role - lambda . id } \" policy = < <EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"lambda:InvokeFunction\", \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"ssm:GetParameter\", \"s3:PutObject\", \"s3:ListBucket\", \"s3:GetObject\" ], \"Resource\": [ \"*\" ] } ] } EOF } # Define the role resource \"aws_iam_role\" \"role-lambda\" { name = \"dev-jwat-role-lambda\" description = \"Role to execute all Lambda related tasks.\" assume_role_policy = <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"sts:AssumeRole\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Effect\": \"Allow\", \"Sid\": \"\" } ] } EOF } The Lambda function needs the files for executing the function in a ZIP archive. This should contain both the function itself and it's dependencies. The filename links to the ZIP file that is created by my deployment script. The deployment script makes sure that the Python dependencies are being build for the Amazon image the Lambda function is running on instead of complying it for my Mac. When a Python library (such as pandas ) is compiled for Mac it is likely not to work for the Amazon image because pandas depends on many C libraries which are OS specific. # Define the Lambda function resource \"aws_lambda_function\" \"lambda-function\" { function_name = \"dev-lambda-looker-upload\" description = \"Lambda function for uploading a Looker view\" handler = \"${var.handler}\" runtime = \"${var.runtime}\" filename = \"sources/lambda-functions/looker-upload/lambda.zip\" role = \"${aws_iam_role.role-lambda.arn}\" tags = { Environment = \"Development\" Owner = \"Jitse-Jan\" } } The deployment script deploy.sh contains the following steps: Create packages for the Lambda functions Deploy the infrastructure Run the initial script It will first iterate through the Lambda function directory to find the different functions. For each function it will pick up the requirements.txt to determine the libraries to be installed. They will be installed inside the Docker container for the Lambda image and added to lambda.zip . The __pycache__ files are removed to reduce the size of the Lambda package, since its size is limited to 350 MB uncompressed. After adding the library artifacts to the ZIP file, the main function lambda.py is added to the ZIP file. We run the terraform apply to apply the changes to the infrastructure and finally run a Python script. The Python script is limited to running the Lambda functions. #!/usr/bin/env bash # deploy.sh export PKG_DIR = \"python\" export PY_VERSION = \"python3.7\" SCRIPT_DIR = \" $( cd \" $( dirname \" ${ BASH_SOURCE [0] } \" ) \" && pwd ) \" LAMBDA_DIR = \"sources/lambda-functions\" FULL_DIR = ${ SCRIPT_DIR } / ${ LAMBDA_DIR } for fldr in ${ FULL_DIR } /* do printf \"\\033[1;35m>> Zipping ${ fldr } \\033[0m\\n\" cd ${ fldr } && rm -rf ${ PKG_DIR } && mkdir -p ${ PKG_DIR } docker run --rm -v $( pwd ) :/foo -w /foo lambci/lambda:build- ${ PY_VERSION } \\ pip install -r requirements.txt -t ${ PKG_DIR } --no-deps cd ${ fldr } / ${ PKG_DIR } find . -type d -name '__pycache__' -print0 | xargs -0 rm -rf rm ${ fldr } /lambda.zip && zip --quiet -r ${ fldr } /lambda.zip . cd ${ fldr } && zip --quiet -r ${ fldr } /lambda.zip lambda.py rm -rf ${ fldr } / ${ PKG_DIR } done cd ${ SCRIPT_DIR } terraform apply ${ PY_VERSION } initial_run.py Note that for running the Python script boto3 should be installed. \"\"\" initial_run.py \"\"\" import boto3 PREFIX = 'dev-' lambda_client = boto3 . client ( \"lambda\" , \"eu-west-2\" ) # Trigger the Lambda functions for function in [ fun [ \"FunctionName\" ] for fun in lambda_client . list_functions ()[ \"Functions\" ] if fun [ \"FunctionName\" ] . startswith ( PREFIX ) ]: print ( \"> Running function ` %s `.\" % function ) response = lambda_client . invoke ( FunctionName = function ) print ( \"< Response: %s \" % response ) In order to run the Lambda function at a given interval, we use a cron expression in a CloudWatch event rule. We will run the schedule at midnight every day. # Define the CloudWatch schedule resource \"aws_cloudwatch_event_rule\" \"cloudwatch-event-rule-midnight-run\" { name = \"dev-cloudwatch-event-rule-midnight-run-looker-upload\" description = \"Cloudwatch event rule to run every day at midnight for the Looker upload.\" schedule_expression = \"${var.schedule_midnight}\" } The Lambda function and CloudWatch event rule should be attached to get things working. # Define the CloudWatch target resource \"aws_cloudwatch_event_target\" \"cloudwatch-event-target\" { rule = \"dev-cloudwatch-event-rule-midnight-run-looker-upload\" arn = \"arn:aws:lambda:${var.region}:848373817713:function:dev-lambda-looker-upload\" } Finally we need to make sure that CloudWatch is allowed to run the Lambda function. # Define the Lambda permission to run Lambda from CloudWatch resource \"aws_lambda_permission\" \"lambda-permission-cloudwatch\" { statement_id = \"AllowExecutionFromCloudWatch\" action = \"lambda:InvokeFunction\" function_name = \"dev-lambda-looker-upload\" principal = \"events.amazonaws.com\" source_arn = \"arn:aws:events:${var.region}:848373817713:rule/dev-cloudwatch-event-rule-midnight-run-looker-upload\" } The result of the Lambda function will be written to a S3 bucket. In the definition we make sure the bucket is public and we enable versioning. # Define the public bucket resource \"aws_s3_bucket\" \"bucket-lambda-deployments\" { bucket = \"dev-jwat\" region = \"${var.region}\" acl = \"public-read\" versioning = { enabled = true } tags = { Environment = \"Development\" Owner = \"Jitse-Jan\" } } Variables for a Terraform deployment can be stored in different ways. Terraform will automatically pick up all *.tf and .tfvars and add it to the deployment. Most commonly the two files terraform.tfvars and variables.tf are used. The first one defines the specific values of the parameters, while the latter often contains the type and the default. In this example it I have defined the following. The value of region is inside the first file, while the type {} (string) is defined in the second file. terraform.tfvars region = \"eu-west-2\" variables.tf variable \"handler\" { default = \"lambda.handler\" } variable \"region\" {} variable \"runtime\" { default = \"python3.7\" } variable \"schedule_midnight\" { default = \"cron(0 0 * * ? *)\" } Execution Terraform initialization ╭─ ~/code/terraform-aws-lambda-looker $ ╰─ terraform init Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"aws\" ( 2 .10.0 ) ... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.aws: version = \"~> 2.10\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Terraform plan ╭─ ~/code/terraform-aws-lambda-looker $ ╰─ terraform plan Refreshing Terraform state in -memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: + aws_cloudwatch_event_rule.cloudwatch-event-rule-midnight-run id: <computed> arn: <computed> description: \"Cloudwatch event rule to run every day at midnight for the Looker upload.\" is_enabled: \"true\" name: \"dev-cloudwatch-event-rule-midnight-run-looker-upload\" schedule_expression: \"cron(0 0 * * ? *)\" + aws_cloudwatch_event_target.cloudwatch-event-target id: <computed> arn: \"arn:aws:lambda:eu-west-1:848373817713:function:dev-lambda-looker-upload\" rule: \"dev-cloudwatch-event-rule-midnight-run\" target_id: <computed> + aws_iam_role.role-lambda id: <computed> arn: <computed> assume_role_policy: \"{\\n \\\"Version\\\": \\\"2012-10-17\\\",\\n \\\"Statement\\\": [\\n {\\n \\\"Action\\\": \\\"sts:AssumeRole\\\",\\n \\\"Principal\\\": {\\n \\\"Service\\\": \\\"lambda.amazonaws.com\\\"\\n },\\n \\\"Effect\\\": \\\"Allow\\\",\\n \\\"Sid\\\": \\\"\\\"\\n }\\n ]\\n}\\n\" create_date: <computed> description: \"Role to execute all Lambda related tasks.\" force_detach_policies: \"false\" max_session_duration: \"3600\" name: \"dev-jwat-role-lambda\" path: \"/\" unique_id: <computed> + aws_iam_role_policy.policy-lambda id: <computed> name: \"dev-jwat-policy-lambda\" policy: \"{\\n \\\"Version\\\": \\\"2012-10-17\\\",\\n \\\"Statement\\\": [\\n {\\n \\\"Effect\\\": \\\"Allow\\\",\\n \\\"Action\\\": [\\n \\\"lambda:InvokeFunction\\\",\\n \\\"logs:CreateLogGroup\\\",\\n \\\"logs:CreateLogStream\\\",\\n \\\"logs:PutLogEvents\\\",\\n \\\"ssm:GetParameter\\\",\\n \\\"s3:PutObject\\\",\\n \\\"s3:ListBucket\\\",\\n \\\"s3:GetObject\\\"\\n ],\\n \\\"Resource\\\": [\\n \\\"*\\\"\\n ]\\n }\\n ]\\n}\\n\" role: \" ${ aws_iam_role .role-lambda.id } \" + aws_lambda_function.lambda-function id: <computed> arn: <computed> description: \"Lambda function for uploading a Looker view\" filename: \"sources/lambda-functions/looker-upload/lambda.zip\" function_name: \"dev-lambda-looker-upload\" handler: \"lambda.handler\" invoke_arn: <computed> last_modified: <computed> memory_size: \"128\" publish: \"false\" qualified_arn: <computed> reserved_concurrent_executions: \"-1\" role: \" ${ aws_iam_role .role-lambda.arn } \" runtime: \"python3.7\" source_code_hash: <computed> source_code_size: <computed> tags.%: \"2\" tags.Environment: \"Development\" tags.Owner: \"Jitse-Jan\" timeout: \"3\" tracing_config.#: <computed> version: <computed> + aws_lambda_permission.lambda-permission-cloudwatch id: <computed> action: \"lambda:InvokeFunction\" function_name: \"dev-lambda-looker-upload\" principal: \"events.amazonaws.com\" source_arn: \"arn:aws:events:eu-west-1:848373817713:rule/dev-cloudwatch-event-rule-midnight-run-looker-upload\" statement_id: \"AllowExecutionFromCloudWatch\" + aws_s3_bucket.bucket-lambda-deployments id: <computed> acceleration_status: <computed> acl: \"public-read\" arn: <computed> bucket: \"dev-jwat\" bucket_domain_name: <computed> bucket_regional_domain_name: <computed> force_destroy: \"false\" hosted_zone_id: <computed> region: \"eu-west-2\" request_payer: <computed> tags.%: \"2\" tags.Environment: \"Development\" tags.Owner: \"Jitse-Jan\" versioning.#: \"1\" versioning.0.enabled: \"true\" versioning.0.mfa_delete: \"false\" website_domain: <computed> website_endpoint: <computed> Plan: 7 to add, 0 to change, 0 to destroy. ------------------------------------------------------------------------ Note: You didn 't specify an \"-out\" parameter to save this plan, so Terraform can' t guarantee that exactly these actions will be performed if \"terraform apply\" is subsequently run. Terraform deploy ╭─ ~/code/terraform-aws-lambda-looker $ ╰─ ./deploy.sh [ 1 /3 ] Creating packages for Lambda > Checking for Lambda functions in /Users/j.waterschoot/code/terraform-aws-lambda-looker/sources/lambda-functions >> Zipping /Users/j.waterschoot/code/terraform-aws-lambda-looker/sources/lambda-functions/looker-upload Collecting certifi == 2019 .3.9 ( from -r requirements.txt ( line 1 )) Downloading https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl ( 158kB ) Collecting lookerapi == 3 .0.0 ( from -r requirements.txt ( line 2 )) Downloading https://files.pythonhosted.org/packages/5e/b5/49ecd3c4c86803e62e24ee206681e64820e24ab289b3d8496db98a073c60/lookerapi-3.0.0-py3-none-any.whl ( 687kB ) Collecting python-dateutil == 2 .8.0 ( from -r requirements.txt ( line 3 )) Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl ( 226kB ) Collecting six == 1 .12.0 ( from -r requirements.txt ( line 4 )) Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl Collecting urllib3 == 1 .24.2 ( from -r requirements.txt ( line 5 )) Downloading https://files.pythonhosted.org/packages/df/1c/59cca3abf96f991f2ec3131a4ffe72ae3d9ea1f5894abe8a9c5e3c77cfee/urllib3-1.24.2-py2.py3-none-any.whl ( 131kB ) Installing collected packages: certifi, lookerapi, python-dateutil, six, urllib3 Successfully installed certifi-2019.3.9 lookerapi-3.0.0 python-dateutil-2.8.0 six-1.12.0 urllib3-1.24.2 You are using pip version 19 .0.3, however version 19 .1.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. [ 2 /3 ] Deploying on AWS aws_s3_bucket.bucket-lambda-deployments: Refreshing state... ( ID: dev-jwat ) aws_cloudwatch_event_target.cloudwatch-event-target: Refreshing state... ( ID: dev-cloudwatch-event-rule-midnight-run-...d-terraform-20190513083217509800000001 ) aws_iam_role.role-lambda: Refreshing state... ( ID: dev-jwat-role-lambda ) aws_lambda_permission.lambda-permission-cloudwatch: Refreshing state... ( ID: AllowExecutionFromCloudWatch ) aws_cloudwatch_event_rule.cloudwatch-event-rule-midnight-run: Refreshing state... ( ID: dev-cloudwatch-event-rule-midnight-run-looker-upload ) aws_lambda_function.lambda-function: Refreshing state... ( ID: dev-lambda-looker-upload ) aws_iam_role_policy.policy-lambda: Refreshing state... ( ID: dev-jwat-role-lambda:dev-jwat-policy-lambda ) Apply complete! Resources: 0 added, 0 changed, 0 destroyed. [ 3 /3 ] Executing the initial run script > Running function ` dev-lambda-looker-upload ` . < Response: { 'ResponseMetadata' : { 'RequestId' : '1d492f55-78c4-47c2-921a-a27cf784d4b0' , 'HTTPStatusCode' : 200 , 'HTTPHeaders' : { 'date' : 'Mon, 13 May 2019 08:40:36 GMT' , 'content-type' : 'application/json' , 'content-length' : '123' , 'connection' : 'keep-alive' , 'x-amzn-requestid' : '1d492f55-78c4-47c2-921a-a27cf784d4b0' , 'x-amz-function-error' : 'Unhandled' , 'x-amzn-remapped-content-length' : '0' , 'x-amz-executed-version' : '$LATEST' , 'x-amzn-trace-id' : 'root=1-5cd92d84-d306a2215d6228179c96cb04;sampled=0' } , 'RetryAttempts' : 0 } , 'StatusCode' : 200 , 'FunctionError' : 'Unhandled' , 'ExecutedVersion' : '$LATEST' , 'Payload' : <botocore.response.StreamingBody object at 0x10ea5ceb8> } Validation Lambda functions Lambda function detail Cloudwatch event Cloudwatch rule Conclusion It is straightforward to deploy a Lambda function using Terraform and keep the infrastructure in version control. See GitHub for the final result.", "tags": "posts", "url": "creating-terraform-deployment-aws-lambda-looker.html", "loc": "creating-terraform-deployment-aws-lambda-looker.html" }, { "title": "Interacting with AWS Glue", "text": "In [1]: import boto3 In [2]: boto3 . __version__ Out[2]: '1.9.103' In [3]: def get_databases (): \"\"\" Returns the databases available in the Glue data catalog :return: list of databases \"\"\" return [ dat [ \"Name\" ] for dat in glue_client . get_databases ()[ \"DatabaseList\" ]] In [4]: def get_tables_for_database ( database ): \"\"\" Returns a list of tables in a Glue database catalog :param database: Glue database :return: list of tables \"\"\" starting_token = None next_page = True tables = [] while next_page : paginator = glue_client . get_paginator ( operation_name = \"get_tables\" ) response_iterator = paginator . paginate ( DatabaseName = database , PaginationConfig = { \"PageSize\" : 100 , \"StartingToken\" : starting_token }, ) for elem in response_iterator : tables += [ { \"name\" : table [ \"Name\" ], } for table in elem [ \"TableList\" ] ] try : starting_token = elem [ \"NextToken\" ] except : next_page = False return tables Setup the Glue client with boto3 : In [5]: glue_client = boto3 . client ( 'glue' , 'eu-west-1' ) Create two tables in the default database: In [6]: params = { 'DatabaseName' : 'default' , 'TableInput' : { 'Name' : 'table_one' , } } glue_client . create_table ( ** params ) params [ 'TableInput' ] . update ({ 'Name' : 'table_two' }) glue_client . create_table ( ** params ) Out[6]: {'ResponseMetadata': {'RequestId': 'acd584c0-5536-11e9-9615-03d279e216a7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 02 Apr 2019 11:01:32 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': 'acd584c0-5536-11e9-9615-03d279e216a7'}, 'RetryAttempts': 0}} List the tables from the databases that contain the string default : In [7]: for database in [ dat for dat in get_databases () if 'default' in dat ]: print ( f \"Database: { database } \" ) for table in get_tables_for_database ( database ): print ( f \"Table: { table [ 'name' ] } \" ) Database: default Table: table_one Table: table_two Clean-up: In [8]: params = { 'DatabaseName' : 'default' , 'Name' : 'table_one' , } glue_client . delete_table ( ** params ) params . update ({ 'Name' : 'table_two' }) glue_client . delete_table ( ** params ) Out[8]: {'ResponseMetadata': {'RequestId': 'ad5c689b-5536-11e9-b79a-7706853c390d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 02 Apr 2019 11:01:33 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': 'ad5c689b-5536-11e9-b79a-7706853c390d'}, 'RetryAttempts': 0}} Verification: In [9]: for database in [ dat for dat in get_databases () if 'default' in dat ]: print ( f \"Database: { database } \" ) for table in get_tables_for_database ( database ): print ( f \"Table: { table [ 'name' ] } \" ) Database: default", "tags": "posts", "url": "interacting-with-aws-glue.html", "loc": "interacting-with-aws-glue.html" }, { "title": "Creating a data range with Python", "text": "In [1]: import datetime In [2]: def get_daterange_in_days ( start_date , end_date ): \"\"\" Return days between start and end date \"\"\" start_date = datetime . datetime . strptime ( start_date , \"%Y-%m- %d \" ) . date () end_date = datetime . datetime . strptime ( end_date , \"%Y-%m- %d \" ) . date () for n in range ( int (( end_date - start_date ) . days )): yield start_date + datetime . timedelta ( n ) In [3]: for day in get_daterange_in_days ( '2019-03-01' , '2019-03-07' ): print ( f \"Current day: { day } \" ) Current day: 2019-03-01 Current day: 2019-03-02 Current day: 2019-03-03 Current day: 2019-03-04 Current day: 2019-03-05 Current day: 2019-03-06 In [4]: def get_daterange_in_months ( start_month , end_month ): \"\"\" Returns the months between start and end month \"\"\" start_month = datetime . datetime . strptime ( start_month , \"%Y-%m\" ) . date () end_month = datetime . datetime . strptime ( end_month , \"%Y-%m\" ) . date () while start_month <= end_month : if start_month . month >= 12 : next = datetime . date ( start_month . year + 1 , 1 , 1 ) else : next = datetime . date ( start_month . year , start_month . month + 1 , 1 ) last = min ( next - datetime . timedelta ( 1 ), end_month ) yield ( start_month . strftime ( '%Y-%m' ), start_month , last ) start_month = next In [5]: for month in get_daterange_in_months ( '2019-01' , '2019-03' ): current_month , startdate , enddate = month print ( f 'Current month: { current_month } ' ) Current month: 2019-01 Current month: 2019-02 Current month: 2019-03", "tags": "posts", "url": "date-range-with-python.html", "loc": "date-range-with-python.html" }, { "title": "Using Azure Blob Storage and Parquet", "text": "Objective This notebook shows how to interact with Parquet on Azure Blob Storage. Please note that it is not possible to write Parquet to Blob Storage using PySpark. I have tried with version 2.2, 2.3 and 2.4 but none of them work (yet). It connects and creates the folder, but no data is written. Azure support was not able to help me, except for advising me to use HDinsights. Imports In [1]: from azure.storage.blob import BlockBlobService import pandas as pd import pyarrow.parquet as pq from io import BytesIO from configparser import RawConfigParser from pyspark import SparkConf , SparkContext , SQLContext Definitions In [2]: BLOB_NAME = \"characters.parquet\" Setup Blob In [3]: # Read the configuration config = RawConfigParser () config . read ( \"blobconfig.ini\" ) # Create blob_service blob_service = BlockBlobService ( account_name = config [ \"blob-store\" ][ \"blob_account_name\" ], account_key = config [ \"blob-store\" ][ \"blob_account_key\" ], ) Setup Spark In order to connect to Azure Blob Storage with Spark, we need to download two JARS ( hadoop-azure-2.7.3.jar and azure-storage-6.1.0.jar ) and add them to the Spark configuration. I chose these specific versions since they were the only ones working with reading data using Spark 2.4.0. Additionally, the fs.azure needs to be set to the Azure FileSytem and fs.azure.account.key.<youraccountname>.blob.core.windows.net should contain the account key. In [4]: def setup_spark ( config ): \"\"\" Setup Spark to connect to Azure Blob Storage \"\"\" jars = [ \"spark-2.4.0-bin-hadoop2.7/jars/hadoop-azure-2.7.3.jar\" , \"spark-2.4.0-bin-hadoop2.7/jars/azure-storage-6.1.0.jar\" , ] conf = ( SparkConf () . setAppName ( \"Spark Blob Test\" ) . set ( \"spark.driver.extraClassPath\" , \":\" . join ( jars )) . set ( \"fs.azure\" , \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\" ) . set ( f \"fs.azure.account.key. { config [ 'blob-store' ][ 'blob_account_name' ] } .blob.core.windows.net\" , config [ \"blob-store\" ][ \"blob_account_key\" ], ) ) sc = SparkContext ( conf = conf ) . getOrCreate () return SQLContext ( sc ) In [5]: # Create Spark context sql_context = setup_spark ( config ) In [6]: # Create dataframe df = pd . DataFrame . from_dict ( [( \"Mario\" , \"Red\" ), ( \"Luigi\" , \"Green\" ), ( \"Princess\" , \"Pink\" )] ) . rename ( columns = { 0 : \"name\" , 1 : \"color\" }) print ( df . head ()) name color 0 Mario Red 1 Luigi Green 2 Princess Pink Using PyArrow with Pandas it is easy to write a dataframe to Blob Storage. Convert the Pandas dataframe into Parquet using a buffer and write the buffer to a blob. In [7]: def write_pandas_dataframe_to_blob ( blob_service , df , container_name , blob_name ): \"\"\" Write Pandas dataframe to blob storage \"\"\" buffer = BytesIO () df . to_parquet ( buffer ) blob_service . create_blob_from_bytes ( container_name = container_name , blob_name = blob_name , blob = buffer . getvalue () ) In [8]: # Write to blob using pyarrow write_pandas_dataframe_to_blob ( blob_service , df , config [ 'blob-store' ][ 'blob_container' ], BLOB_NAME ) Reading data back from blob using Pandas is identical. The data is read into a ByteStream from the blob storage. In [9]: def get_pandas_dataframe_from_parquet_on_blob ( blob_service , container_name , blob_name ): \"\"\" Get a dataframe from Parquet file on blob storage \"\"\" byte_stream = BytesIO () try : blob_service . get_blob_to_stream ( container_name = container_name , blob_name = blob_name , stream = byte_stream ) df = pq . read_table ( source = byte_stream ) . to_pandas () finally : byte_stream . close () return df In [10]: # Read from blob using pyarrow rdf = get_pandas_dataframe_from_parquet_on_blob ( blob_service , config [ 'blob-store' ][ 'blob_container' ], BLOB_NAME ) print ( rdf . head ()) name color 0 Mario Red 1 Luigi Green 2 Princess Pink Reading the data using Spark for a single file Parquet blob is done using the following function. Note the path that uses the wasbs protocol. In [11]: def get_pyspark_dataframe_from_parquet_on_blob ( config , sql_context , container_name , blob_name ): \"\"\" Get a dataframe from Parquet file on blob storage using PySpark \"\"\" path = f \"wasbs:// { container_name } @ { config [ 'blob-store' ][ 'blob_account_name' ] } .blob.core.windows.net/ { blob_name } \" return sql_context . read . parquet ( path ) In [12]: # Read from blob using PySpark sdf = get_pyspark_dataframe_from_parquet_on_blob ( config , sql_context , config [ 'blob-store' ][ 'blob_container' ], BLOB_NAME ) print ( sdf . show ()) +--------+-----+-----------------+ | name|color|__index_level_0__| +--------+-----+-----------------+ | Mario| Red| 0| | Luigi|Green| 1| |Princess| Pink| 2| +--------+-----+-----------------+ None", "tags": "posts", "url": "using-azure-blob-and-parquet.html", "loc": "using-azure-blob-and-parquet.html" }, { "title": "Calculate differences with sparse date/value dataframes", "text": "Goal Find the difference between two dataframes with a date and a value column Create a continuous time axis to eliminate gaps in the date column Fill the interpolated with the value of the previous date In [1]: import pandas as pd Create a dataframe with two entries: In [2]: df_old = pd . DataFrame ( [ { 'date' : '2019-01-01' , 'value' : 0.844 },{ 'date' : '2019-01-02' , 'value' : 0.842 } ] ) Create a second dataframe with additional values for three more days: In [3]: df_new = pd . DataFrame ( [ { 'date' : '2019-01-01' , 'value' : 0.844 },{ 'date' : '2019-01-02' , 'value' : 0.842 },{ 'date' : '2019-01-03' , 'value' : 0.84 },{ 'date' : '2019-01-04' , 'value' : 0.84 },{ 'date' : '2019-01-07' , 'value' : 0.838 } ] ) Create a time axis between the minimum and maximum date found in the bigger dataframe: In [4]: time_index = pd . date_range ( df_new [ 'date' ] . min (), df_new [ 'date' ] . max ()) Set the index to the date column: In [5]: df_new . set_index ( 'date' , inplace = True ) Convert the index to a DatetimeIndex : In [6]: df_new . index = pd . DatetimeIndex ( df_new . index ) Reindex the existing date index with the continuous time_index to interpolate missing dates: In [7]: df_new = df_new . reindex ( time_index , method = 'ffill' ) Convert the date index back to a column: In [8]: df_new = df_new . reset_index () . rename ( columns = { 'index' : 'date' }) Convert the date column to the proper datatype for the smaller dataframe: In [9]: df_old [ 'date' ] = pd . to_datetime ( df_old [ 'date' ]) Combine the two dataframes and remove all the duplicate couples to only keep the none duplicate rows: In [10]: df_diff = pd . concat ([ df_new , df_old ]) . drop_duplicates ( keep = False ) In [11]: df_diff Out[11]: date value 2 2019-01-03 0.840 3 2019-01-04 0.840 4 2019-01-05 0.840 5 2019-01-06 0.840 6 2019-01-07 0.838", "tags": "posts", "url": "difference-with-sparse-dataframes.html", "loc": "difference-with-sparse-dataframes.html" }, { "title": "Using Spark to read from S3", "text": "Goal We want to read data from S3 with Spark. Ideally we want to be able to read Parquet files from S3 into our Spark Dataframe. Preparation On my Kubernetes cluster I am using the Pyspark notebook . In the home folder on the container I downloaded and extracted Spark 2.4.0 . After extracting I set the SPARK_HOME environment variable. In [1]: import os import sys os . environ [ \"SPARK_HOME\" ] = \"/home/jovyan/spark-2.4.0-bin-hadoop2.7\" We need to download the libraries to be able to communicate with AWS and use S3 as a file system. Download the following two jars to the jars folder in the Spark installation. In [2]: ! wget http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar -P $SPARK_HOME /jars/ --2019-01-04 16:16:09-- http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar Resolving central.maven.org (central.maven.org)... 151.101.48.209 Connecting to central.maven.org (central.maven.org)|151.101.48.209|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 11948376 (11M) [application/java-archive] Saving to: ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar' aws-java-sdk-1.7.4. 100%[===================>] 11.39M --.-KB/s in 0.09s 2019-01-04 16:16:09 (129 MB/s) - ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar' saved [11948376/11948376] In [3]: ! wget http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar -P $SPARK_HOME /jars/ --2019-01-04 16:16:09-- http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar Resolving central.maven.org (central.maven.org)... 151.101.48.209 Connecting to central.maven.org (central.maven.org)|151.101.48.209|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 126287 (123K) [application/java-archive] Saving to: ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar' hadoop-aws-2.7.3.ja 100%[===================>] 123.33K --.-KB/s in 0.002s 2019-01-04 16:16:09 (59.2 MB/s) - ‘/home/jovyan/spark-2.4.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar' saved [126287/126287] In order to read from AWS S3, we need to set some parameters in the configuration file for spark. This is normally located at $SPARK_HOME/conf/spark-defaults.conf . Enter the following three key value pairs replacing the obvious values: # spark-defaults.conf spark.hadoop.fs.s3a.access.key=MY_ACCESS_KEY spark.hadoop.fs.s3a.secret.key=MY_SECRET_KEY spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem Script Set the Spark configuration and create the Spark context and the SQL context. In [4]: from pyspark import SparkConf , SparkContext , SQLContext conf = ( SparkConf () . setAppName ( \"S3 Configuration Test\" ) . set ( \"spark.executor.instances\" , \"1\" ) . set ( \"spark.executor.cores\" , 1 ) . set ( \"spark.executor.memory\" , \"2g\" )) sc = SparkContext ( conf = conf ) sqlContext = SQLContext ( sc ) Path where to write to and read from: In [5]: path = \"s3a://user-jwaterschoot/mario-colors/\" Let's create a simple RDD and save it as a dataframe in Parquet format: In [6]: rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . write . parquet ( path ) Read the data back from the S3 path: In [7]: df = sqlContext . read . parquet ( path ) In [8]: df . show ( 5 ) +--------+-----+ | name|color| +--------+-----+ |Princess| Pink| | Luigi|Green| | Mario| Red| +--------+-----+", "tags": "posts", "url": "using-spark-to-read-from-s3.html", "loc": "using-spark-to-read-from-s3.html" }, { "title": "Developing AWS Glue scripts on Mac OSX", "text": "Prerequisites Java Python 3.5 Spark 2.2.0 Zeppelin 0.7.3 Java installation Make sure a recent Java version is installed. $ java -version java version \"1.8.0_172\" Java ( TM ) SE Runtime Environment ( build 1 .8.0_172-b11 ) Java HotSpot ( TM ) 64 -Bit Server VM ( build 25 .172-b11, mixed mode ) Spark installation Assuming brew is installed, navigate to the Formula folder. $ cd /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula We need to create the formula to install the correct version of Spark. If there is already a apache-spark.rb rename it to apache-spark.rb.old . Create the apache-spark.rb formula with the following content. Note that we explicitly use Spark 2.2.0 . class ApacheSpark < Formula desc \"Engine for large-scale data processing\" homepage \"https://spark.apache.org/\" url \"http://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\" version \"2.2.0\" sha256 \"97fd2cc58e08975d9c4e4ffa8d7f8012c0ac2792bcd9945ce2a561cf937aebcc\" head \"https://github.com/apache/spark.git\" bottle :unneeded def install # Rename beeline to distinguish it from hive's beeline mv \"bin/beeline\" , \"bin/spark-beeline\" rm_f Dir [ \"bin/*.cmd\" ] libexec . install Dir [ \"*\" ] bin . write_exec_script Dir [ \" #{ libexec } /bin/*\" ] end test do assert_match \"Long = 1000\" , pipe_output ( bin / \"spark-shell\" , \"sc.parallelize(1 to 1000).count()\" ) end end Now install apache-spark and verify the correct version gets installed. $ brew install apache-spark $ brew list apache-spark --versions apache-spark 2 .2.0 Verify that we can use Spark now by starting pyspark in a terminal. Before we run pyspark make sure JAVA_HOME is set to the correct path and PYSPARK_PYTHON is not using Python 2. Use sudo find / -name javac to find the Java path. export JAVA_HOME = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/\" export PYSPARK_PYTHON = python3 $ pyspark Python 3 .5.0 ( v3.5.0:374f501f4567, Sep 12 2015 , 11 :00:19 ) [ GCC 4 .2.1 ( Apple Inc. build 5666 ) ( dot 3 )] on darwin Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. Using Spark 's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 18/11/21 16:45:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/11/21 16:45:16 WARN Utils: Service ' SparkUI ' could not bind on port 4040. Attempting port 4041. 18/11/21 16:45:16 WARN Utils: Service ' SparkUI ' could not bind on port 4041. Attempting port 4042. 18/11/21 16:45:21 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ ' _/ /__ / .__/ \\_ ,_/_/ /_/ \\_\\ version 2 .2.0 /_/ Using Python version 3 .5.0 ( v3.5.0:374f501f4567, Sep 12 2015 11 :00:19 ) SparkSession available as 'spark' . >>> spark.version '2.2.0' >>> sc <SparkContext master = local [ * ] appName = PySparkShell> >>> Apart from some warnings, we can see pyspark is working, connects to the local Spark, refers to the right Spark version and Python 3 ( 3.5.0 ) is used in the shell. Zeppelin installation We need to install Zeppelin 0.7.3 (and not 0.8.0!) to setup the connection with AWS in a later stage. Navigate to Zeppelin's Download page and scroll down for Zeppelin-0.7.3-bin-all . Unpack the TGZ-file and start Zeppelin with the command: $ bin/zeppelin-daemon.sh start By navigating to http://localhost:8080 the Zeppelin interface should show. Development with local Spark Create a new note and verify the master of the Spark context. Since we did not configure it yet, it will default to connect to the local Spark cluster. We simply test the notebook by creating a simple RDD and converting it to a dataframe. rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . show () +--------+-----+ | name|color| +--------+-----+ | Mario| Red | | Luigi|Green| |Princess| Pink| +--------+-----+ Development with AWS Spark Create a Glue development endpoint We need to connect the Spark interpreter to the AWS Glue endpoint. Navigate to the Glue page via the AWS console and click on Add endpoint . In the Properties pane set the name and assign the role for the development endpoint. Skip through the Networking pane. In the SSH public key pane, create a new key pair using ssh-keygen in the terminal. Save the two files to a safe place and upload the public key (i.e. id_rsa.pub ) to the development endpoint. Finally, review the endpoint and click on Finish . It will take a couple of minutes for the endpoint to go from Provisioning to the Ready state. Go the the details page for the endpoint and copy the SSH tunnel to remote interpeter line. Replace the with the private key that was just generated in the previous step. Run the command in a terminal and keep it running. Choose yes when it prompts to accept the connection. We now have an active SSH tunnel that will route all traffic to localhost port 9007 to the development endpoint on AWS. $ ssh -i id_rsa -vnNT -L :9007:123.456.789.255:9007 glue@ec2-52-16-115-181.eu-west-1.compute.amazonaws.com Connect Zeppelin to Glue endpoint Enable the Spark interpreter by clicking on anonymous -> Interpreter and scroll down to the Spark section. Click on edit to change the settings. Select Connect to existing process to connect to localhost on port 9007. In the Properties section select yarn-client in order to be able to use the Spark running on AWS. Scroll down and select Save . Restart the Spark interpreter when prompted. Run the script Create a new note to verify the master of the Spark context. If all things went well it should show yarn-client . This means the Spark context on AWS will be used instead of the local Spark (and the user is billed for running Spark jobs!). Deploying on AWS Glue The final step of running a Glue job is submitting and scheduling the script. After debugging and cleaning up the code in the Zeppelin notebook, the script has to be added via the Glue console . Click on Add job and fill in the name and role for the script. Select the folder to save the script and make sure the option to A new script to be authored by you is selected. Accept the defaults and continue to the next two pages to get to Save job and edit script . For this tutorial we keep things simple and only add the simple cost that we have used before. The script will have some default content and by adding the two lines for the RDD and dataframe it should look like the code below. Note that I did not clean up the imports since in a normal ETL jobs these imports are needed to manipulate the data properly. import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job ## @params: [JOB_NAME] args = getResolvedOptions ( sys . argv , [ 'JOB_NAME' ]) sc = SparkContext () glueContext = GlueContext ( sc ) spark = glueContext . spark_session job = Job ( glueContext ) job . init ( args [ 'JOB_NAME' ], args ) rdd = sc . parallelize ([( 'Mario' , 'Red' ), ( 'Luigi' , 'Green' ), ( 'Princess' , 'Pink' )]) rdd . toDF ([ 'name' , 'color' ]) . show () job . commit () After modifying the code, save the job and run it to verify the script is working. Conclusion To develop Glue scripts the proposed way of working would be: Develop locally with both (a subset of the) data and Spark on the local machine. Develop locally with AWS data and local Spark Develop using data from AWS and the Spark running on AWS Clean up the Zeppelin notebook to create the final script Submit the final script as a Glue job Resources Tutorial: Set Up a Local Apache Zeppelin Notebook to Test and Debug ETL Scripts - AWS Glue Programming ETL Scripts - AWS Glue", "tags": "posts", "url": "developing-glue-scripts-on-mac-osx.html", "loc": "developing-glue-scripts-on-mac-osx.html" }, { "title": "AWS Lambda development - Python & SAM", "text": "Requirements AWS CLI already configured with at least PowerUser permission Python 3 installed Pipenv installed pip install pipenv Docker installed SAM Local installed Preparation Make sure Python 3 is installed on the machine, either as default version or alongside Python 2. Check the available downloads on Python.org . ~/c/python-lambda-tutorial $ python --version Python 2 .7.15 ~/c/python-lambda-tutorial $ python3 --version Python 3 .6.6 By installing Python, pip should be available on the machine. In case Python 3 is not the default Python interpreter, pip should be called with pip3 . ~/c/python-lambda-tutorial $ pip --version pip 18 .1 from /usr/local/lib/python2.7/site-packages/pip ( python 2 .7 ) ~/c/python-lambda-tutorial $ pip3 --version pip 18 .1 from /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pip ( python 3 .6 ) Create the Python virtual environment using pipenv . pipenv is the recommended way to create virtual environments for Python. The same can be achieved using conda or virtualenv or other tools, but the preferred way for Python 3 is pipenv . ( Source ) Install pipenv : ~/c/python-lambda-tutorial $ pip3 install pipenv Collecting pipenv ... Installing collected packages: pipenv Successfully installed pipenv-2018.10.13 Create an environment for Python 3.6: ~/c/python-lambda-tutorial $ pipenv --python 3 .6 Creating a virtualenv for this project… Pipfile: /Users/jitsejan/code/python-lambda-tutorial/Pipfile Using /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6m ( 3 .6.6 ) to create virtualenv… ⠦Running virtualenv with interpreter /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6m Using base prefix '/Library/Frameworks/Python.framework/Versions/3.6' New python executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python3.6m Also creating executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python Installing setuptools, pip, wheel...done. Virtualenv location: /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM Activate the environment: ~/c/python-lambda-tutorial $ pipenv shell Launching subshell in virtual environment… Install AWS CLI : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv install awscli Installing awscli… ... Installing collected packages: urllib3, docutils, six, python-dateutil, jmespath, botocore, s3transfer, PyYAML, pyasn1, rsa, colorama, awscli Successfully installed PyYAML-3.13 awscli-1.16.35 botocore-1.12.25 colorama-0.3.9 docutils-0.14 jmespath-0.9.3 pyasn1-0.4.4 python-dateutil-2.7.3 rsa-3.4.2 s3transfer-0.1.13 six-1.11.0 urllib3-1.23 Adding awscli to Pipfile ' s [ packages ] … Pipfile.lock not found, creating… Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( 94bc2a ) ! Installing dependencies from Pipfile.lock ( 94bc2a ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 12 /12 — 00 :00:03 python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws --version aws-cli/1.16.35 Python/3.6.6 Darwin/18.0.0 botocore/1.12.25 Install AWS SAM CLI : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv install aws-sam-cli Installing aws-sam-cli… ... Requirement already satisfied, skipping upgrade: docutils> = 0 .10 in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/lib/python3.6/site-packages ( from botocore< 1 .13.0,> = 1 .12.25->boto3~ = 1 .5->aws-sam-cli ) ( 0 .14 ) Collecting arrow ( from jinja2-time> = 0 .1.0->cookiecutter~ = 1 .6.0->aws-sam-cli ) Installing collected packages: enum34, click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, docker-pycreds, websocket-client, certifi, chardet, idna, requests, docker, jsonschema, boto3, aws-sam-translator, arrow, jinja2-time, binaryornot, poyo, future, whichcraft, cookiecutter, pytz, tzlocal, regex, dateparser, pystache, aws-sam-cli Successfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 arrow-0.12.1 aws-sam-cli-0.6.0 aws-sam-translator-1.6.0 binaryornot-0.4.4 boto3-1.9.25 certifi-2018.10.15 chardet-3.0.4 click-6.7 cookiecutter-1.6.0 dateparser-0.7.0 docker-3.5.0 docker-pycreds-0.3.0 enum34-1.1.6 future-0.16.0 idna-2.7 itsdangerous-0.24 jinja2-time-0.2.0 jsonschema-2.6.0 poyo-0.4.2 pystache-0.5.4 pytz-2018.5 regex-2018.8.29 requests-2.19.1 tzlocal-1.5.1 websocket-client-0.53.0 whichcraft-0.5.2 Adding aws-sam-cli to Pipfile ' s [ packages ] … Pipfile.lock ( a1782d ) out of date, updating to ( 94bc2a ) … Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( a1782d ) ! Installing dependencies from Pipfile.lock ( a1782d ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 42 /42 — 00 :00:10 python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam --version SAM CLI, version 0 .6.0 Install the template tool cookiecutter : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv install cookiecutter Installing cookiecutter… ... Adding cookiecutter to Pipfile ' s [ packages ] … Pipfile.lock ( 23abb4 ) out of date, updating to ( a1782d ) … Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( 23abb4 ) ! Installing dependencies from Pipfile.lock ( 23abb4 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 42 /42 — 00 :00:11 Verify the environment: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ tree . ├── Pipfile └── Pipfile.lock 0 directories, 2 files python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cat Pipfile [[ source ]] url = \"https://pypi.org/simple\" verify_ssl = true name = \"pypi\" [ dev-packages ] [ packages ] awscli = \"*\" aws-sam-cli = \"*\" cookiecutter = \"*\" [ requires ] python_version = \"3.6\" Important Make sure the AWS credentials are saved in ~/.aws/credentials with the following content and the ID and key replaced with the correct values. [default] aws_access_key_id = AAAAAAAAAAAAAAAAAAAA aws_secret_access_key = aAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaAaA Local development Install the template with the minimal option set: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cookiecutter gh:aws-samples/cookiecutter-aws-sam-python project_name [ Name of the project ] : python-lambda-tutorial-project project_short_description [ A short description of the project ] : include_apigw [ y ] : n include_xray [ y ] : n include_safe_deployment [ y ] : n include_experimental_make [ n ] : n [ INFO ] : Removing Makefile from project due to chosen options... [ SUCCESS ] : Project initialized successfully! You can now jump to python-lambda-tutorial-project folder [ INFO ] : python-lambda-tutorial-project/README.md contains instructions on how to proceed. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ tree . ├── Pipfile ├── Pipfile.lock └── python-lambda-tutorial-project ├── Pipfile ├── Pipfile.lock ├── README.md ├── first_function │ ├── __init__.py │ └── app.py ├── requirements.txt ├── template.yaml └── tests └── unit ├── __init__.py └── test_handler.py 4 directories, 11 files Navigate inside the python-lambda-tutorial-project folder and install the application and development dependencies. Note that this creates a different virtual environment, namely the one with the dependencies for the lambda function. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cd python-lambda-tutorial-project/ python-lambda-tutorial-mfatrPYM ~/c/p/python-lambda-tutorial-project $ pipenv install Creating a virtualenv for this project… Pipfile: /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/Pipfile Using /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python3.6m ( 3 .6.6 ) to create virtualenv… ⠹Running virtualenv with interpreter /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-mfatrPYM/bin/python3.6m Using real prefix '/Library/Frameworks/Python.framework/Versions/3.6' New python executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-project-scMbNPxZ/bin/python3.6m Also creating executable in /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-project-scMbNPxZ/bin/python Installing setuptools, pip, wheel...done. Virtualenv location: /Users/jitsejan/.local/share/virtualenvs/python-lambda-tutorial-project-scMbNPxZ Pipfile.lock ( 26f9f9 ) out of date, updating to ( 49fffa ) … Locking [ dev-packages ] dependencies… Locking [ packages ] dependencies… Updated Pipfile.lock ( 26f9f9 ) ! Installing dependencies from Pipfile.lock ( 26f9f9 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 8 /8 — 00 :00:03 python-lambda-tutorial-mfatrPYM ~/c/p/python-lambda-tutorial-project $ pipenv install -d Installing dependencies from Pipfile.lock ( 26f9f9 ) … 🐍 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 17 /17 — 00 :00:04 The cookiecutter template will create a first_function . The function code is located in first_function/app.py , while the function itself is defined in the template.yaml as FirstFunction . Before we can test the function, we need to prepare the function for deployment. If we run the test without creating the deployment package, the tests will fail. Note that the first time the function is invoked, the lambda:python3.6 image will be downloaded first. Start the local lambda server: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial sam local start-lambda 2018 -10-17 12 :54:38 Starting the Local Lambda Service. You can now invoke your Lambda Functions defined in your template through the endpoint. 2018 -10-17 12 :54:38 * Running on http://127.0.0.1:3001/ ( Press CTRL+C to quit ) Call the FirstFunction with a simple JSON payload: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ echo '{\"lambda\": \"payload\"}' | sam local invoke FirstFunction 2018 -10-17 12 :55:37 Reading invoke payload from stdin ( you can also pass it from file with --event ) 2018 -10-17 12 :55:37 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 12 :55:37 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image.............................................................................................. 2018 -10-17 12 :55:48 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: 6928ee1b-e1df-4b0c-bd36-c181102fd447 Version: $LATEST Unable to import module 'app' : No module named 'app' END RequestId: 6928ee1b-e1df-4b0c-bd36-c181102fd447 REPORT RequestId: 6928ee1b-e1df-4b0c-bd36-c181102fd447 Duration: 4 ms Billed Duration: 100 ms Memory Size: 128 MB Max Memory Used: 19 MB { \"errorMessage\" : \"Unable to import module 'app'\" } To create the deployment, we first create a hashed requirements.txt from the Pipfile : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pipenv lock -r > requirements.txt /usr/local/lib/python2.7/site-packages/pipenv/vendor/vistir/compat.py:109: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/var/folders/06/61h5ywpd0936tr9cvk_gc38r0rr60q/T/pipenv-tjo3nI-requirements' > warnings.warn ( warn_message, ResourceWarning ) python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cat requirements.txt -i https://pypi.python.org/simple boto3 == 1 .9.25 botocore == 1 .12.25 docutils == 0 .14 jmespath == 0 .9.3 python-dateutil == 2 .7.3 ; python_version > = '2.7' s3transfer == 0 .1.13 six == 1 .11.0 urllib3 == 1 .23 Install the dependencies directly to the build folder of the function: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ pip install -r requirements.txt -t first_function/build/ Looking in indexes: https://pypi.python.org/simple ... Installing collected packages: jmespath, docutils, urllib3, six, python-dateutil, botocore, s3transfer, boto3 Successfully installed boto3-1.9.25 botocore-1.12.25 docutils-0.14 jmespath-0.9.3 python-dateutil-2.7.3 s3transfer-0.1.13 six-1.11.0 urllib3-1.23 Finally, copy the app.py for the function to the build folder: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cp -R first_function/app.py first_function/build/ We can test the function again and see that in this case we get the expected result as defined in first_function/app.py . python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ echo '{\"lambda\": \"payload\"}' | sam local invoke FirstFunction 2018 -10-17 13 :04:10 Reading invoke payload from stdin ( you can also pass it from file with --event ) 2018 -10-17 13 :04:10 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 13 :04:10 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image...... 2018 -10-17 13 :04:12 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: 8b3f19d2-9fd9-41bd-9842-c6347e18259f Version: $LATEST END RequestId: 8b3f19d2-9fd9-41bd-9842-c6347e18259f REPORT RequestId: 8b3f19d2-9fd9-41bd-9842-c6347e18259f Duration: 1149 ms Billed Duration: 1200 ms Memory Size: 128 MB Max Memory Used: 25 MB { \"hello\" : \"world\" } To make testing simpler, we can write the JSON payload the event.json and call the function with the event file as an argument. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ echo '{\"lambda\": \"payload\"}' > event.json python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam local invoke -e event.json FirstFunction 2018 -10-17 13 :10:03 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 13 :10:03 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image...... 2018 -10-17 13 :10:04 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: ea8c34a0-c44f-4719-a20a-fdf6613d75d4 Version: $LATEST END RequestId: ea8c34a0-c44f-4719-a20a-fdf6613d75d4 REPORT RequestId: ea8c34a0-c44f-4719-a20a-fdf6613d75d4 Duration: 1161 ms Billed Duration: 1200 ms Memory Size: 128 MB Max Memory Used: 25 MB { \"hello\" : \"world\" } As we can see in the generated app.py , there is an event and context parameter. Simplify the app.py to the following to test the two parameters: import boto3 import json import os def runs_on_aws_lambda (): \"\"\" Returns True if this function is executed on AWS Lambda service. \"\"\" return 'AWS_SAM_LOCAL' not in os . environ and 'LAMBDA_TASK_ROOT' in os . environ session = boto3 . Session () def lambda_handler ( event , context ): \"\"\" AWS Lambda handler \"\"\" message = get_message ( event , context ) return message def get_message ( event , context ): return { \"event\" : event , \"function_name\" : context . function_name , } Copy the app.py in the build folder and invoke the function again. We can see the output has changed and shows us more content. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cp -R first_function/app.py first_function/build/ python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam local invoke -e event.json FirstFunction 2018 -10-17 13 :18:11 Invoking app.lambda_handler ( python3.6 ) 2018 -10-17 13 :18:11 Found credentials in shared credentials file: ~/.aws/credentials Fetching lambci/lambda:python3.6 Docker container image...... 2018 -10-17 13 :18:13 Mounting /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/first_function/build as /var/task:ro inside runtime container START RequestId: 7160a23c-f8db-4c22-bc95-34d8822b2427 Version: $LATEST END RequestId: 7160a23c-f8db-4c22-bc95-34d8822b2427 REPORT RequestId: 7160a23c-f8db-4c22-bc95-34d8822b2427 Duration: 1003 ms Billed Duration: 1100 ms Memory Size: 128 MB Max Memory Used: 25 MB { \"event\" : { \"lambda\" : \"payload\" } , \"function_name\" : \"test\" } Deployment First and foremost, we need a S3 bucket where we can upload our Lambda functions packaged as ZIP before we deploy anything - If you don't have a S3 bucket to store code artifacts then this is a good time to create one: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws s3 mb s3://lambda-artifacts make_bucket: lambda-artifacts python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws s3 ls | grep lambda-artifacts 2018 -10-17 13 :25:24 lambda-artifacts Run the following command to package our Lambda function to S3: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam package --template-file template.yaml --output-template-file packaged.yaml --s3-bucket lambda-artifacts Uploading to dbafe95d37cbdd0d76a83e2c289f2536 7395857 / 7395857 .0 ( 100 .00% ) Successfully packaged artifacts and wrote output template to file packaged.yaml. Execute the following command to deploy the packaged template aws cloudformation deploy --template-file /Users/jitsejan/code/python-lambda-tutorial/python-lambda-tutorial-project/packaged.yaml --stack-name <YOUR STACK NAME> Next, the following command will create a Cloudformation Stack and deploy your SAM resources. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ sam deploy --template-file packaged.yaml --stack-name python-lambda-tutorial --capabilities CAPABILITY_IAM Waiting for changeset to be created.. Waiting for stack create/update to complete Successfully created/updated stack - python-lambda-tutorial The deployment stack can be checked with CloudFormation too: python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws cloudformation describe-stacks --stack-name python-lambda-tutorial --query 'Stacks[].Outputs' [ [ { \"OutputKey\" : \"FirstFunction\" , \"OutputValue\" : \"arn:aws:lambda:eu-west-1:848373817713:function:python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" , \"Description\" : \"First Lambda Function ARN\" } ] ] We can list the available functions on AWS Lambda and search for FirstFunction : python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws lambda list-functions | grep FirstFunction \"FunctionName\" : \"python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" , \"FunctionArn\" : \"arn:aws:lambda:eu-west-1:848373817713:function:python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" , \"Role\" : \"arn:aws:iam::848373817713:role/python-lambda-tutorial-FirstFunctionRole-6V7HKJLXSE52\" , Running We can invoke the function on AWS Lambda with the following command, where the function name is copied from the output of the previous command. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ aws lambda invoke --invocation-type RequestResponse --function-name python-lambda-tutorial-FirstFunction-10Z13KCZEJ575 outputfile.txt { \"StatusCode\" : 200 , \"ExecutedVersion\" : \" $LATEST \" } The result is written to output.txt and should contain the event and the function name. python-lambda-tutorial-mfatrPYM ~/c/python-lambda-tutorial $ cat outputfile.txt { \"event\" : {} , \"function_name\" : \"python-lambda-tutorial-FirstFunction-10Z13KCZEJ575\" }", "tags": "posts", "url": "aws-lambda-development-with-sam.html", "loc": "aws-lambda-development-with-sam.html" }, { "title": "None shall PAAS", "text": "Unlocking the data warehouse > It is often the case with start-ups that the transitioning framework is built first around an enhanced functionality of the product to engage with the customers and, at a later stage, around a more consistent structure of the system as a whole. The story was no different in the early days of MarketInvoice. Features had to be added rapidly to accommodate the great customer adoption and there were no manuals illustrating a viable architecture or even predictable working timelines for the platform of a fast-growing company. Over time, with the increasing participation of new customers, it was evident that the existing structure needed to be more scalable so that reaching the prospected growth was sustainable. Our recent partnership with Barclays , for example, means that the expectations and demands on the platform are set to grow at full speed . According to our internal projections, by the end of 2019 the traffic on the platform could easily be incremented by 10X. While this is a remarkable opportunity, it would not have been possible if we hadn't been prepared to create new paradigms to promptly categorise the data by cleaning it and organising it for our business intelligence (BI) and machine learning models. The goal was to increase the automation process so that we can give our customers faster responses, reducing the waiting time that occurs while making an application and receiving the funds. Decreasing the number of manual steps in the application process also facilitates the work for our risk underwriters. Tracking additional data appears to be an astute evaluation since it will ensure the creation of new consistent decision-making models intended at automating some of the tasks that the underwriters are currently spending the majority of their time on. Figure 1. Diagram showing the structured environment of the data platform for the hardware manufacturer. 2500 machines write their data into a shared folder which will be transformed into business value using the compute platform. In my previous job, I was working as a data engineer in the tech field for a hardware manufacturer. They were building out their data platform and asked me to help design their data-flows via different pipelines. Their data source was relatively simple as they had five different types of machines, only generating logging data containing details from testing and producing. The main difficulty I faced, while working on this specific project, was the amount of data and the speed to calculate the business KPIs. During my assignment, I received data from 2500 machines every 2 hours containing roughly 50 MB per file, thus in total 1.5 TB per day. Calculating the statistics for this data took over 24 hours. This was reduced to 10 minutes through platform optimisations (and scaling up the computing power). The data arrived from various locations around the world in a shared folder where it was fed into an aggregation script. A parsing job reads this data and aggregates it into a data warehouse to be subsequently used by the BI teams. While I was dealing with large volumes of information, the complexity of the task was simple because the data was homogeneous. Legacy solution – time driven The old situation When I joined MarketInvoice, it was far from a clean and structured platform. Two cloud providers were being used for different segments of the company. The platform for our customers was built on Microsoft Azure Platform as a Service (PAAS) and contained different database servers with a large number of tables. Part of the data was pushed to Amazon AWS to make it accessible in the BI tool Looker . Additionally, data from third-party sources such as Google Analytics and Salesforce were pushed into AWS with the support of third-party synchronisation tools. Machine learning was performed by combining the data from AWS and Azure and was hosted on Azure. The data in AWS resided in one colossal database with tables from diverse sources plus aggregated tables based on the raw data. Figure 2. Diagram showing the old unstructured architecture of the data platform with data synchronised and stored using different tools. While that approach seemed effective and the important data was effortlessly available in the necessary locations, it didn't show an actual prospect of being scalable . There were challenges while performing tasks requiring heavy computing or while adding new datasets from external sources. Virtual machines were constantly running to extract data but were idle the majority of the time. Data was synced at fixed intervals, making it impossible for the platform to do live predictions based on the most recent data. My challenge when I started at MarketInvoice was to find an optimal solution for the scalability of the data platform. Fundamentally, to scale the system we needed a novel data architecture. The data platform can be characterised by the following requirements: Gather additional data in a centralised place to analyse leads, traffic, etc. and monitor the company's KPIs Create a platform where third-party sources can be effortlessly integrated Develop a scalable compute platform for number crunching and machine learning to create fast and efficient risk models Provide an environment where different sections of the company can easily add their own scripts to create more insights from other sources Design an architecture which is low in cost and leverages the power of serverless computing New solution — data driven Reshaping the data platform After the requirements of the new data platform were well-defined, it was necessary to investigate the effectiveness of as many approaches as possible, to really make sure that only the most suitable solution was taken into consideration. We had several conversations with various third-parties like Microsoft and AWS to discuss their tools and data warehouse common practises, more specifically the ones dealing with financial data. In the meanwhile, as a data team, we aimed at building different proof of concepts on Azure and AWS (and the potential mixture of the two) to test data ingestion, data migration and implementation of machine learning models. In reality, what we were truly trying to achieve was the creation of a so-called platform agnostic solution, which could run on any cloud provider with minimal changes, i.e. running a Python extraction script on AWS is easy using AWS Lambda, but we can simply copy that same script and run it on Azure. In the future prospect of possibly moving to another cloud provider, the entire migration process should be uncomplicated and just a matter of placing the correct scripts in the right place to get the same data pipelines working. Figure 3. Diagram showing the new architecture of the data platform. The platform consists of a data lake and data warehouse layer. The data lake contains raw data coming from the user platform and third-party source. The data warehouse contains processed data with aggregations and statistics to serve the business intelligence and data science teams. In the new data platform, we use both AWS S3 and Azure Blob file storage as a data lake to save all the raw data from the diverse sources. By using this redundancy, we can smoothly switch between the platforms and an additional backup is readily available in case one of the two providers develops issues. We store the history of the tables on both platforms and we can gather historical statistics. Data is kept in sync between the platform using a data-driven approach; when there is new data in the database it will trigger a function to push that data into file storage on Azure, which will, in turn, be copied to AWS using Azure Functions . By using triggers based on the data, we do not need to have machines running 24/7, but only pay for the execution time of the triggered functions. Data from the external sources is retrieved using AWS Lambda , consisting of minor running jobs that will pull data from the APIs and push the data into the data lake. These jobs do not necessitate running machines as they are simply scheduled to retrieve data at a set interval. Finally, the platform is set up to listen to events in the data lake, so when new data is added to a certain folder, it can trigger an ETL (Extract, Transform, Load) job to put the new data in the right projections for the data warehouse. Similarly, the data warehouse is using AWS S3, which means the projections are saved in a readable format, users of the platform can inspect the data easily and querying it in a BI tool is straightforward. We deemed it unnecessary to push the data into a database since that will limit the usability of the data and significantly increases the cost of the platform. We're using the data analytics tool Looker, which in fact integrates well with S3 using the AWS Glue Data Catalog to index the data in the data warehouse. Furthermore, the machine learning tools and parallel computing platform of AWS work even better when using data saved on S3 instead of a database. As the platform scales up, this approach will give us the ability to easily handle large volumes of data, without affecting the platform performance or generating huge bills at the end of the month. Future solution — event-driven Redesigning the user platform While the new data platform has contributed towards making us less dependent on any PAAS provider, our user platform is still the Goliath of the story. At the time of writing this, the Tech team is busy modularising the platform and changing the entire underlying structure. Figure 4. Simplification of the non-event-driven method. A script will check periodically if new data was written to the database and push the new data to the data warehouse. Ideally, the data that was originally written to a database from the platform will follow a different route. Instead of writing data directly to the database, it will send the data event to a message bus, making it possible to have different clients listening to that bus and triggering a job to respond to that event. In the data-driven approach, we respond to a row added in the table (Figure 4). Whereas for our event-driven approach, we do not want to check the table but write the event data straight to the data lake and thereby avoiding the necessity of checking databases (Figure 5). While it does not influence the architecture of the data platform, it will help to respond faster to events on the user platform, and therefore increase the reliability of the data platform ensuring that the last data is always present, instead of waiting for the daily extraction job to get data from the databases in the data lakes. Figure 5. Simplification of the event-driven method. All events will be written to a message bus. The data warehouse will listen for new events in the bus and copy the data. Conclusion We made a cost-effective solution by using the best of both platforms without depending on any specific vendor. Following the new approach, we created modular jobs to get data from the raw sources into the data warehouse using both Azure Functions and AWS Lambda. Data is stored in files on Azure Blob Storage and AWS S3 making it the perfect source for machine learning and business intelligence. All the tools that comprise the solution are scalable, consenting to follow the growth trends and projections for the company, as we embrace our new customers with originality in our solutions!", "tags": "posts", "url": "none-shall-paas.html", "loc": "none-shall-paas.html" }, { "title": "Write a Pandas dataframe to CSV on S3", "text": "Write a pandas dataframe to a single CSV file on S3. import boto3 from io import StringIO DESTINATION = 'my-bucket' def _write_dataframe_to_csv_on_s3 ( dataframe , filename ): \"\"\" Write a dataframe to a CSV on S3 \"\"\" print ( \"Writing {} records to {} \" . format ( len ( dataframe ), filename )) # Create buffer csv_buffer = StringIO () # Write dataframe to buffer dataframe . to_csv ( csv_buffer , sep = \"|\" , index = False ) # Create S3 object s3_resource = boto3 . resource ( \"s3\" ) # Write buffer to S3 object s3_resource . Object ( DESTINATION , filename ) . put ( Body = csv_buffer . getvalue ()) _write_dataframe_to_csv_on_s3 ( my_df , 'my-folder' ) Gist", "tags": "posts", "url": "write-dataframe-to-csv-on-s3.html", "loc": "write-dataframe-to-csv-on-s3.html" }, { "title": "Write a Pandas dataframe to Parquet on S3", "text": "Write a pandas dataframe to a single Parquet file on S3. # Note: make sure `s3fs` is installed in order to make Pandas use S3. # Credentials for AWS in the normal location ~/.aws/credentials DESTINATION = 'my-bucket' def _write_dataframe_to_parquet_on_s3 ( dataframe , filename ): \"\"\" Write a dataframe to a Parquet on S3 \"\"\" print ( \"Writing {} records to {} \" . format ( len ( dataframe ), filename )) output_file = f \"s3:// { DESTINATION } / { filename } /data.parquet\" dataframe . to_parquet ( output_file ) _write_dataframe_to_parquet_on_s3 ( my_df , 'my-folder' ) Gist", "tags": "posts", "url": "write-dataframe-to-parquet-on-s3.html", "loc": "write-dataframe-to-parquet-on-s3.html" }, { "title": "Interacting with Parquet on S3 with PyArrow and s3fs", "text": "Prerequisites Create the hidden folder to contain the AWS credentials: In [1]: ! mkdir ~/.aws Write the credentials to the credentials file: In [2]: %%file ~/.aws/credentials [ default ] aws_access_key_id = AKIAJAAAAAAAAAJ4ZMIQ aws_secret_access_key = fVAAAAAAAALuLBvYQZ / 5 G + zxSe7wwJy + AAA Writing /Users/j.waterschoot/.aws/credentials Alternatively we can use the key and secret from other locations, or environment variables that we provide to the S3 instance. Write to Parquet on S3 Create the inputdata: In [3]: %%file inputdata.csv name , description , color , occupation , picture Luigi , This is Luigi , green , plumber , https : // upload . wikimedia . org / wikipedia / en / f / f1 / LuigiNSMBW . png Mario , This is Mario , red , plumber , https : // upload . wikimedia . org / wikipedia / en / 9 / 99 / MarioSMBW . png Peach , My name is Peach , pink , princess , https : // s - media - cache - ak0 . pinimg . com / originals / d2 / 4 d / 77 / d24d77cfbba789256c9c1afa1f69b385 . png Toad , I like funghi , red ,, https : // upload . wikimedia . org / wikipedia / en / d / d1 / Toad_3D_Land . png Overwriting inputdata.csv Read the data into a dataframe with Pandas: In [4]: import pandas as pd dataframe = pd . read_csv ( 'inputdata.csv' ) dataframe Out[4]: name description color occupation picture 0 Luigi This is Luigi green plumber https://upload.wikimedia.org/wikipedia/en/f/f1... 1 Mario This is Mario red plumber https://upload.wikimedia.org/wikipedia/en/9/99... 2 Peach My name is Peach pink princess https://s-media-cache-ak0.pinimg.com/originals... 3 Toad I like funghi red NaN https://upload.wikimedia.org/wikipedia/en/d/d1... Convert to a PyArrow table: In [5]: import pyarrow as pa table = pa . Table . from_pandas ( dataframe ) table Out[5]: pyarrow.Table name: string description: string color: string occupation: string picture: string __index_level_0__: int64 metadata -------- {b'pandas': b'{\"index_columns\": [\"__index_level_0__\"], \"column_indexes\": [{\"na' b'me\": null, \"field_name\": null, \"pandas_type\": \"unicode\", \"numpy_' b'type\": \"object\", \"metadata\": {\"encoding\": \"UTF-8\"}}], \"columns\":' b' [{\"name\": \"name\", \"field_name\": \"name\", \"pandas_type\": \"unicode' b'\", \"numpy_type\": \"object\", \"metadata\": null}, {\"name\": \"descript' b'ion\", \"field_name\": \"description\", \"pandas_type\": \"unicode\", \"nu' b'mpy_type\": \"object\", \"metadata\": null}, {\"name\": \"color\", \"field' b'_name\": \"color\", \"pandas_type\": \"unicode\", \"numpy_type\": \"object' b'\", \"metadata\": null}, {\"name\": \"occupation\", \"field_name\": \"occu' b'pation\", \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"meta' b'data\": null}, {\"name\": \"picture\", \"field_name\": \"picture\", \"pand' b'as_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null}, ' b'{\"name\": null, \"field_name\": \"__index_level_0__\", \"pandas_type\":' b' \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}], \"pandas_ver' b'sion\": \"0.23.3\"}'} Create the output path for S3: In [6]: BUCKET_NAME = 'my-game-bucket-for-demo' CONTAINER_NAME = 'nintendo-container' TABLE_NAME = 'character-table' output_file = f \"s3:// { BUCKET_NAME } / { CONTAINER_NAME } / { TABLE_NAME } .parquet\" output_file Out[6]: 's3://my-game-bucket-for-demo/nintendo-container/character-table.parquet' Setup connection with S3: In [7]: from s3fs import S3FileSystem s3 = S3FileSystem () # or s3fs.S3FileSystem(key=ACCESS_KEY_ID, secret=SECRET_ACCESS_KEY) s3 Out[7]: <s3fs.core.S3FileSystem at 0x1030f6eb8> Create the bucket if it does not exist yet: In [8]: BUCKET_EXISTS = False try : s3 . ls ( BUCKET_NAME ) BUCKET_EXISTS = True except : print ( \"Create bucket first!\" ) Create bucket first! In [9]: if not BUCKET_EXISTS : s3 . mkdir ( BUCKET_NAME ) Write the table to the S3 output: In [10]: import pyarrow.parquet as pq pq . write_to_dataset ( table = table , root_path = output_file , filesystem = s3 ) Check the files: In [11]: s3 . ls ( BUCKET_NAME ) Out[11]: ['my-game-bucket-for-demo/nintendo-container'] In [12]: s3 . ls ( f \" { BUCKET_NAME } / { CONTAINER_NAME } \" ) Out[12]: ['my-game-bucket-for-demo/nintendo-container/character-table.parquet'] Read the data from the Parquet file In [13]: import pyarrow.parquet as pq dataset = pq . ParquetDataset ( output_file , filesystem = s3 ) df = dataset . read_pandas () . to_pandas () df Out[13]: name description color occupation picture 0 Luigi This is Luigi green plumber https://upload.wikimedia.org/wikipedia/en/f/f1... 1 Mario This is Mario red plumber https://upload.wikimedia.org/wikipedia/en/9/99... 2 Peach My name is Peach pink princess https://s-media-cache-ak0.pinimg.com/originals... 3 Toad I like funghi red None https://upload.wikimedia.org/wikipedia/en/d/d1...", "tags": "posts", "url": "interacting-with-parquet-on-s3.html", "loc": "interacting-with-parquet-on-s3.html" }, { "title": "Notes on Kubernetes", "text": "TLDR: Final solution: docker kubeadm kubectl kubelet helm weave heapster traefik Sources Some of the sources I have used to get my Kubernetes cluster up and running: Kubernetes in 10 minutes Your instant Kubernetes cluster Setting up a single node Kubernetes Cluster Kubernetes On Bare Metal Kubernetes : Ingress Controller with Træfɪk and Let's Encrypt Walkthrough Before we begin, update the system: jitsejan @dev16 : ~ $ sudo apt - get update Install Docker jitsejan@dev16:~$ sudo apt-get install -qy docker.io jitsejan@dev16:~$ docker --version Docker version 17 .03.2-ce, build f5ec1e2 Install Kubernetes apt repository jitsejan@dev16:~$ sudo apt-get install -y apt-transport-https && curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - jitsejan@dev16:~$ echo \"deb http://apt.kubernetes.io/ kubernetes-xenial main\" \\ | sudo tee -a /etc/apt/sources.list.d/kubernetes.list \\ && sudo apt-get update Install kubeadm , kubectl and kubelet jitsejan@dev16:~$ sudo apt-get install -y kubelet kubeadm kubectl jitsejan@dev16:~$ kubelet --version Kubernetes v1.11.1 jitsejan@dev16:~$ kubeadm version kubeadm version: & version.Info { Major: \"1\" , Minor: \"11\" , GitVersion: \"v1.11.1\" , GitCommit: \"b1b29978270dc22fecc592ac55d903350454310a\" , GitTreeState: \"clean\" , BuildDate: \"2018-07-17T18:50:16Z\" , GoVersion: \"go1.10.3\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } jitsejan@dev16:~$ kubectl version Client Version: version.Info { Major: \"1\" , Minor: \"10\" , GitVersion: \"v1.10.0\" , GitCommit: \"fc32d2f3698e36b93322a3465f63a14e9f0eaead\" , GitTreeState: \"clean\" , BuildDate: \"2018-03-26T16:55:54Z\" , GoVersion: \"go1.9.3\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } The connection to the server localhost:8080 was refused - did you specify the right host or port? Initialize kubernetes jitsejan@dev16:~$ sudo kubeadm init [ init ] using Kubernetes version: v1.11.1 [ preflight ] running pre-flight checks I0807 05 :46:52.371257 3480 kernel_validator.go:81 ] Validating kernel version I0807 05 :46:52.371519 3480 kernel_validator.go:96 ] Validating kernel config [ preflight/images ] Pulling images required for setting up a Kubernetes cluster [ preflight/images ] This might take a minute or two, depending on the speed of your internet connection [ preflight/images ] You can also perform this action in beforehand using 'kubeadm config images pull' [ kubelet ] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [ kubelet ] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [ preflight ] Activating the kubelet service [ certificates ] Generated ca certificate and key. [ certificates ] Generated apiserver certificate and key. [ certificates ] apiserver serving cert is signed for DNS names [ dev16.jitsejan.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local ] and IPs [ 10 .96.0.1 142 .44.184.35 ] [ certificates ] Generated apiserver-kubelet-client certificate and key. [ certificates ] Generated sa key and public key. [ certificates ] Generated front-proxy-ca certificate and key. [ certificates ] Generated front-proxy-client certificate and key. [ certificates ] Generated etcd/ca certificate and key. [ certificates ] Generated etcd/server certificate and key. [ certificates ] etcd/server serving cert is signed for DNS names [ dev16.jitsejan.com localhost ] and IPs [ 127 .0.0.1 ::1 ] [ certificates ] Generated etcd/peer certificate and key. [ certificates ] etcd/peer serving cert is signed for DNS names [ dev16.jitsejan.com localhost ] and IPs [ 142 .44.184.35 127 .0.0.1 ::1 ] [ certificates ] Generated etcd/healthcheck-client certificate and key. [ certificates ] Generated apiserver-etcd-client certificate and key. [ certificates ] valid certificates and keys now exist in \"/etc/kubernetes/pki\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\" [ kubeconfig ] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\" [ controlplane ] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" [ controlplane ] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" [ controlplane ] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" [ etcd ] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\" [ init ] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\" [ init ] this might take a minute or longer if the control plane images have to be pulled [ apiclient ] All control plane components are healthy after 46 .003711 seconds [ uploadconfig ] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [ kubelet ] Creating a ConfigMap \"kubelet-config-1.11\" in namespace kube-system with the configuration for the kubelets in the cluster [ markmaster ] Marking the node dev16.jitsejan.com as master by adding the label \"node-role.kubernetes.io/master=''\" [ markmaster ] Marking the node dev16.jitsejan.com as master by adding the taints [ node-role.kubernetes.io/master:NoSchedule ] [ patchnode ] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"dev16.jitsejan.com\" as an annotation [ bootstraptoken ] using token: j8gf09.pe16y63l0hxygc77 [ bootstraptoken ] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [ bootstraptoken ] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [ bootstraptoken ] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [ bootstraptoken ] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [ addons ] Applied essential addon: CoreDNS [ addons ] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 142 .44.184.35:6443 --token j8gf09.pe16y63l0hxygc77 --discovery-token-ca-cert-hash sha256:d8d47cbd352035d5a7746d0ed7e54ae7c0c6bcbb5e89ef410039f7b457be853f Run the steps mentioned in the previous output: jitsejan @dev16 : ~ $ mkdir - p $ HOME / . kube jitsejan @dev16 : ~ $ sudo cp - i / etc / kubernetes / admin . conf $ HOME / . kube / config jitsejan @dev16 : ~ $ sudo chown $ ( id - u ) :$ ( id - g ) $ HOME / . kube / config Weave net jitsejan @ dev16 : ~$ export kubever =$ ( kubectl version | base64 | tr - d ' \\n ' ) jitsejan @ dev16 : ~$ kubectl apply - f \"https://cloud.weave.works/k8s/net?k8s-version=$kubever\" serviceaccount / weave - net created clusterrole . rbac . authorization . k8s . io / weave - net created clusterrolebinding . rbac . authorization . k8s . io / weave - net created role . rbac . authorization . k8s . io / weave - net created rolebinding . rbac . authorization . k8s . io / weave - net created daemonset . extensions / weave - net created Untaint the master node Make sure we can install pods on the master node: jitsejan @dev16 : ~ $ kubectl taint nodes -- all node - role . kubernetes . io / master - node / dev16 . jitsejan . com untainted Check status of the pods: jitsejan@dev16:~$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1 /1 Running 0 4m kube-system coredns-78fcdf6894-flmnd 1 /1 Running 0 4m kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 3m kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 3m kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 4m kube-system kube-proxy-59l87 1 /1 Running 0 4m kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 4m kube-system weave-net-rkmrb 2 /2 Running 0 1m Install helm Install the package manager for Kubernetes: jitsejan@dev16:~$ sudo snap install helm 2018 -08-07T06:04:43-07:00 INFO Waiting for restart... helm 2 .9.1 from 'snapcrafters' installed jitsejan@dev16:~$ sudo kubectl create -f ./helm-rbac.yml serviceaccount/tiller created clusterrolebinding.rbac.authorization.k8s.io/tiller created jitsejan@dev16:~$ sudo cp ~/.kube/config /root/snap/helm/common/kube/config jitsejan@dev16:~$ sudo helm init --service-account tiller Creating /root/snap/helm/common/repository Creating /root/snap/helm/common/repository/cache Creating /root/snap/helm/common/repository/local Creating /root/snap/helm/common/plugins Creating /root/snap/helm/common/starters Creating /root/snap/helm/common/cache/archive Creating /root/snap/helm/common/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/snap/helm/common. Tiller ( the Helm server-side component ) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! jitsejan@dev16:~/kubernetes-dev$ sudo helm version --short Client: v2.9.1+g20adb27 Server: v2.9.1+g20adb27 Check status of the pods: jitsejan @dev16 : ~/ kubernetes - dev $ kubectl get pods -- all - namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube - system coredns - 78 fcdf6894 - 2 khbk 1 / 1 Running 0 45 m kube - system coredns - 78 fcdf6894 - flmnd 1 / 1 Running 0 45 m kube - system etcd - dev16 . jitsejan . com 1 / 1 Running 0 44 m kube - system kube - apiserver - dev16 . jitsejan . com 1 / 1 Running 0 44 m kube - system kube - controller - manager - dev16 . jitsejan . com 1 / 1 Running 0 44 m kube - system kube - proxy - 59 l87 1 / 1 Running 0 45 m kube - system kube - scheduler - dev16 . jitsejan . com 1 / 1 Running 0 44 m kube - system tiller - deploy - 759 cb9df9 - ddv2h 1 / 1 Running 0 1 m kube - system weave - net - rkmrb 2 / 2 Running 0 41 m Heapster jitsejan @ dev16 : ~/ kubernetes - dev $ sudo helm install stable / heapster -- name heapster -- set rbac . create = true NAME : heapster LAST DEPLOYED : Tue Aug 7 10 : 04 : 28 2018 NAMESPACE : default STATUS : DEPLOYED RESOURCES : ==> v1 / Service NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE heapster ClusterIP 10.108 . 195.4 < none > 8082 / TCP 1 s ==> v1beta1 / Deployment NAME DESIRED CURRENT UP - TO - DATE AVAILABLE AGE heapster - heapster 1 1 1 0 1 s ==> v1 / Pod ( related ) NAME READY STATUS RESTARTS AGE heapster - heapster - 7 bb6d67b9d - h77tj 0 / 2 ContainerCreating 0 0 s ==> v1 / ServiceAccount NAME SECRETS AGE heapster - heapster 1 1 s ==> v1beta1 / ClusterRoleBinding NAME AGE heapster - heapster 1 s ==> v1beta1 / Role NAME AGE heapster - heapster - pod - nanny 1 s ==> v1beta1 / RoleBinding NAME AGE heapster - heapster - pod - nanny 1 s NOTES : 1. Get the application URL by running these commands : export POD_NAME =$ ( kubectl get pods -- namespace default - l \"app=heapster-heapster\" - o jsonpath = \"{.items[0].metadata.name}\" ) kubectl -- namespace default port - forward $ POD_NAME 8082 Traefik Træfik is a modern HTTP reverse proxy and load balancer that makes deploying microservices easy. Træfik integrates with your existing infrastructure components (Docker, Swarm mode, Kubernetes, Marathon, Consul, Etcd, Rancher, Amazon ECS, ...) and configures itself automatically and dynamically. Pointing Træfik at your orchestrator should be the only configuration step you need. Docs --- kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : traefik-ingress-controller rules : - apiGroups : - \"\" resources : - services - endpoints - secrets verbs : - get - list - watch - apiGroups : - extensions resources : - ingresses verbs : - get - list - watch --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1beta1 metadata : name : traefik-ingress-controller roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : traefik-ingress-controller subjects : - kind : ServiceAccount name : traefik-ingress-controller namespace : kube-system jitsejan@dev16:~/kubernetes-dev$ kubectl apply -f traefik-rbac.yml clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created jitsejan@dev16:~/kubernetes-dev$ kubectl apply -f traefik-deployment.yml serviceaccount/traefik-ingress-controller created deployment.extensions/traefik-ingress-controller created service/traefik-ingress-service created jitsejan@dev16:~/kubernetes-dev$ kubectl --namespace = kube-system get pods NAME READY STATUS RESTARTS AGE coredns-78fcdf6894-2khbk 1 /1 Running 0 1h coredns-78fcdf6894-flmnd 1 /1 Running 0 1h etcd-dev16.jitsejan.com 1 /1 Running 0 1h kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 1h kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 1h kube-proxy-59l87 1 /1 Running 0 1h kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 1h tiller-deploy-759cb9df9-ddv2h 1 /1 Running 0 17m traefik-ingress-controller-6f6d87769d-5nrvq 1 /1 Running 0 20s weave-net-rkmrb 2 /2 Running 0 58m jitsejan@dev16:~/kubernetes-dev$ kubectl get services --namespace = kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kube-dns ClusterIP 10 .96.0.10 <none> 53 /UDP,53/TCP 1h tiller-deploy ClusterIP 10 .106.232.150 <none> 44134 /TCP 18m traefik-ingress-service NodePort 10 .106.201.31 <none> 80 :30321/TCP,8080:32262/TCP 1m jitsejan@dev16:~/kubernetes-dev$ curl -X GET 10 .106.201.31 404 page not found Check the pods: jitsejan@dev16:~/kubernetes-dev$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1 /1 Running 0 1h kube-system coredns-78fcdf6894-flmnd 1 /1 Running 0 1h kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 1h kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 1h kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 1h kube-system kube-proxy-59l87 1 /1 Running 0 1h kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 1h kube-system tiller-deploy-759cb9df9-tgnkq 1 /1 Running 0 5m kube-system traefik-ingress-controller-66f46799fd-54pkj 1 /1 Running 0 2m kube-system traefik-ingress-controller-66f46799fd-f42wk 1 /1 Running 0 2m kube-system traefik-ingress-controller-external-0 1 /1 Running 0 30s kube-system traefik-ingress-controller-external-1 1 /1 Running 0 28s kube-system weave-net-rkmrb 2 /2 Running 0 1h metallb-system controller-67cb74b4b5-4ll62 1 /1 Running 0 1h metallb-system speaker-bbqnd 1 /1 Running 0 1h Alternatives Minikube Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day. Use Minikube to install a single node Kubernetes cluster. I didn't go for this solution, because it requires too much additional configuration to get it properly running on a VPS. I have tried with different virtualization techniques, but KVM, KVM2 and VirtualBox all had their issues. I ended up using vm-driver=none , which means you run Kubernetes directly on Linux, but requires you to be root.. Locally this is the preferred way of testing Kubernetes if you are not using Docker for Desktop, which has native support for Kubernetes. Minikube is under heavy development and the community strives to make this the default way of testing Kubernetes for developers. Install jitsejan@dev16:~$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo cp minikube /usr/local/bin/ && rm minikube jitsejan@dev16:~$ minikube version minikube version: v0.28.2 Start the Kubernetes cluster jitsejan @ dev16 : ~$ sudo minikube start -- vm - driver = none Starting local Kubernetes v1 . 10.0 cluster ... Starting VM ... Getting VM IP address ... Moving files into cluster ... Downloading kubeadm v1 . 10.0 Downloading kubelet v1 . 10.0 Finished Downloading kubelet v1 . 10.0 Finished Downloading kubeadm v1 . 10.0 Setting up certs ... Connecting to cluster ... Setting up kubeconfig ... Starting cluster components ... Kubectl is now configured to use the cluster . =================== WARNING : IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacks When using the none driver , the kubectl config and credentials generated will be root owned and will appear in the root home directory . You will need to move the files to the appropriate location and then set the correct permissions . An example of this is below : sudo mv / root /. kube $ HOME /. kube # this will write over any previous configuration sudo chown - R $ USER $ HOME /. kube sudo chgrp - R $ USER $ HOME /. kube sudo mv / root /. minikube $ HOME /. minikube # this will write over any previous configuration sudo chown - R $ USER $ HOME /. minikube sudo chgrp - R $ USER $ HOME /. minikube This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER = true Loading cached images from config file . jitsejan @dev16 : ~ $ sudo kubectl config current - context minikube Flannel (instead of Weave) Another network layer for the containers that can be used is flannel . flannel is a virtual network that gives a subnet to each host for use with container runtimes. Platforms like Google's Kubernetes assume that each container (pod) has a unique, routable IP inside the cluster. The advantage of this model is that it reduces the complexity of doing port mapping. jitsejan@dev16:~$ kubeadm init --pod-network-cidr = 10 .244.0.0/16 --apiserver-advertise-address 142 .44.184.35 jitsejan@dev16:~$ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml jitsejan@dev16:~$ kubectl create -f kube-flannel.yml --namespace = kube-system clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.extensions/kube-flannel-ds-amd64 created daemonset.extensions/kube-flannel-ds-arm64 created daemonset.extensions/kube-flannel-ds-arm created daemonset.extensions/kube-flannel-ds-ppc64le created daemonset.extensions/kube-flannel-ds-s390x created jitsejan@dev16:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION dev16.jitsejan.com Ready master 7m v1.11.1 jitsejan@dev16:~$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-78fcdf6894-lfrf9 1 /1 Running 0 8m coredns-78fcdf6894-tblp9 1 /1 Running 0 8m etcd-dev16.jitsejan.com 1 /1 Running 0 7m kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 7m kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 7m kube-flannel-ds-amd64-bgdhb 1 /1 Running 0 1m kube-proxy-mz7jt 1 /1 Running 0 8m kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 7m Calico (instead of Weave) Instead of using Weave or Flannel, we can use Calico to create a virtual network in our Kubernetes cluster. jitsejan@dev16:~$ sudo kubectl apply -f https://docs.projectcalico.org/master/getting-started/kubernetes/installation/hosted/etcd.yaml daemonset.extensions \"calico-etcd\" created service \"calico-etcd\" created jitsejan@dev16:~$ sudo kubectl apply -f https://docs.projectcalico.org/master/getting-started/kubernetes/installation/rbac.yaml clusterrole.rbac.authorization.k8s.io \"calico-kube-controllers\" created clusterrolebinding.rbac.authorization.k8s.io \"calico-kube-controllers\" created clusterrole.rbac.authorization.k8s.io \"calico-node\" created clusterrolebinding.rbac.authorization.k8s.io \"calico-node\" created jitsejan@dev16:~$ sudo kubectl apply -f \\ https://docs.projectcalico.org/master/getting-started/kubernetes/installation/hosted/calico.yaml configmap \"calico-config\" created secret \"calico-etcd-secrets\" created daemonset.extensions \"calico-node\" created serviceaccount \"calico-node\" created deployment.extensions \"calico-kube-controllers\" created serviceaccount \"calico-kube-controllers\" created jitsejan@dev16:~$ watch kubectl get pods --all-namespaces Every 2 .0s: kubectl get pods --all-namespaces Fri Aug 3 04 :57:04 2018 NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-etcd-926qj 1 /1 Running 0 5m kube-system calico-kube-controllers-bcb7959cd-5njws 1 /1 Running 0 2m kube-system calico-node-p45mz 2 /2 Running 0 2m kube-system coredns-78fcdf6894-kxsff 1 /1 Running 0 7m kube-system coredns-78fcdf6894-r2brs 1 /1 Running 0 7m kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 6m kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 7m kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 7m kube-system kube-proxy-kb559 1 /1 Running 0 7m kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 7m metallb (instead of Traefik) Because we are not using EKS, AKS or GCP, we need to deal with the load balancer ourselves. Before I showed how to use Traefik as the load balancer to take care of all incoming traffic to the VPS, but [ metallb [(https://metallb.universe.tf/)] could be a good alternative. My initial approach involved both Traefik and metallb, but this seemed to create overhead and Traefik has enough functionality to handle the ingress and egress by itself. Install the loadbalancer metallb jitsejan@dev16:~$ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.2/manifests/metallb.yaml namespace/metallb-system created serviceaccount/controller created serviceaccount/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created role.rbac.authorization.k8s.io/config-watcher created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created rolebinding.rbac.authorization.k8s.io/config-watcher created daemonset.apps/speaker created deployment.apps/controller created jitsejan@dev16:~$ kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGE controller-67cb74b4b5-4ll62 1 /1 Running 0 31s speaker-bbqnd 1 /1 Running 0 31s Configure the loadbalancer metallb-conf.yml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 192.168.1.16/28 jitsejan@dev16:~$ kubectl apply -f metallb-conf.yml configmap/config created jitsejan@dev16:~$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-78fcdf6894-2khbk 1 /1 Running 0 52m kube-system coredns-78fcdf6894-flmnd 1 /1 Running 0 52m kube-system etcd-dev16.jitsejan.com 1 /1 Running 0 51m kube-system kube-apiserver-dev16.jitsejan.com 1 /1 Running 0 51m kube-system kube-controller-manager-dev16.jitsejan.com 1 /1 Running 0 51m kube-system kube-proxy-59l87 1 /1 Running 0 52m kube-system kube-scheduler-dev16.jitsejan.com 1 /1 Running 0 51m kube-system tiller-deploy-759cb9df9-ddv2h 1 /1 Running 0 8m kube-system weave-net-rkmrb 2 /2 Running 0 48m metallb-system controller-67cb74b4b5-4ll62 1 /1 Running 0 46m metallb-system speaker-bbqnd 1 /1 Running 0 46m", "tags": "posts", "url": "notes-on-kubernetes.html", "loc": "notes-on-kubernetes.html" }, { "title": "Using executemany to increase PyODBC connection", "text": "I recently had to insert data from a Pandas dataframe into a Azure SQL database using pandas.to_sql() . This was performing very poorly and seemed to take ages, but since PyODBC introduced executemany it is easy to improve the performance: simply add an event listener that activates the executemany for the cursor. For 2300 records I did a small comparison 8.67s and 7.22s versus 5min 57s and 5min 26s, so roughly 50 times faster for this small example dataset. import pandas as pd import pyodbc from sqlalchemy import create_engine , event from sqlalchemy.pool import StaticPool wh_conn = pyodbc . connect ( f \"DRIVER= { config [ name ][ 'driver' ] } ;SERVER= { config [ name ][ 'server' ] } , { config [ name ][ 'port' ] } ;DATABASE= { config [ name ][ 'database' ] } ;UID= { config [ name ][ 'username' ] } ;PWD= { config [ name ][ 'password' ] } \" ) engine = create_engine ( \"mssql+pyodbc://\" , poolclass = StaticPool , creator = lambda : wh_conn ) @event . listens_for ( engine , 'before_cursor_execute' ) def receive_before_cursor_execute ( conn , cursor , statement , params , context , executemany ): if executemany : cursor . fast_executemany = True df . to_sql ( name = 'Table' , con = engine , schema = 'Schema' , index = False , if_exists = 'replace' )", "tags": "posts", "url": "using-executemany-to-increase-pyodbc-connection.html", "loc": "using-executemany-to-increase-pyodbc-connection.html" }, { "title": "Using ButterCMS in a Jupyter notebook with VueJS", "text": "ButterCMS Goal The goal of this notebook is to get myself familiar with ButterCMS, a headless CMS that can be easily integrated with any platform in any language. In the past I've experienced with Joomla, Drupal and WordPress (and Blogger long time ago), where I have lost invested a lot of time in simply getting the CMS in place, extending the CMS with more functionality and migrating data between servers. Since I am investigating different ways of using microservices, becoming less platform dependent and overall learning new tools, I use this notebook to show a simple example of ButterCMS in Jupyter. About Add a blog or CMS to your site in minutes Drop-in our Headless CMS and get back to more interesting problems. Links Homepage Python client VueJS client Prerequisites In [1]: ! pip install buttercms-python Requirement already satisfied: buttercms-python in /home/jitsejan/miniconda3/lib/python3.6/site-packages (1.1) Requirement already satisfied: requests in /home/jitsejan/miniconda3/lib/python3.6/site-packages (from buttercms-python) (2.18.4) Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/jitsejan/miniconda3/lib/python3.6/site-packages (from requests->buttercms-python) (3.0.4) Requirement already satisfied: idna<2.7,>=2.5 in /home/jitsejan/miniconda3/lib/python3.6/site-packages (from requests->buttercms-python) (2.6) Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/jitsejan/miniconda3/lib/python3.6/site-packages (from requests->buttercms-python) (1.22) Requirement already satisfied: certifi>=2017.4.17 in /home/jitsejan/miniconda3/lib/python3.6/site-packages (from requests->buttercms-python) (2018.4.16) You are using pip version 10.0.0, however version 10.0.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. In [2]: import os os . environ [ 'buttercmskey' ] = '1b13611e144e11bbce5b484f4064e39638b223a1' Python implementation In [3]: from butter_cms import ButterCMS client = ButterCMS ( os . getenv ( 'buttercmskey' )) In [4]: client Out[4]: <butter_cms.ButterCMS at 0x7f3edc72f630> In [5]: import json print ( json . dumps ( client . posts . all ({ 'page_size' : 10 , 'page' : 1 }), indent = 4 )) { \"data\": [ { \"url\": \"jupyter-notebook\", \"created\": \"2018-06-14T11:53:54.533383Z\", \"published\": \"2018-06-14T11:50:00Z\", \"author\": { \"first_name\": \"Jitse-Jan\", \"last_name\": \"van Waterschoot\", \"email\": \"code@jitsejan.com\", \"slug\": \"jitse-jan-van-waterschoot\", \"bio\": \"\", \"title\": \"\", \"linkedin_url\": \"\", \"facebook_url\": \"\", \"instagram_url\": \"\", \"pinterest_url\": \"\", \"twitter_handle\": \"\", \"profile_image\": \"\" }, \"categories\": [ { \"name\": \"development\", \"slug\": \"development\" } ], \"tags\": [ { \"name\": \"\", \"slug\": \"\" }, { \"name\": \"buttercms\", \"slug\": \"buttercms\" }, { \"name\": \"python\", \"slug\": \"python\" }, { \"name\": \"vuejs\", \"slug\": \"vuejs\" } ], \"featured_image\": \"https://cdn.buttercms.com/eBXUxLoCR4CKQMUnhjur\", \"slug\": \"jupyter-notebook\", \"title\": \"Jupyter notebook\", \"body\": \"<p>I am working on a Jupyter notebook to show how to work with ButterCMS, VuejS and Python and how it all smoothly works together.&nbsp;</p>\", \"summary\": \"I am working on a Jupyter notebook to show how to work with ButterCMS, VuejS and Python and how it all smoothly works together.\", \"seo_title\": \"Jupyter notebook\", \"meta_description\": \"I am working on a Jupyter notebook to show how to work with ButterCMS, VuejS and Python and how it all smoothly works together.\", \"status\": \"published\" }, { \"url\": \"example-post\", \"created\": \"2018-06-14T11:47:08.121702Z\", \"published\": \"2018-06-14T11:47:08.121391Z\", \"author\": { \"first_name\": \"Jitse-Jan\", \"last_name\": \"van Waterschoot\", \"email\": \"code@jitsejan.com\", \"slug\": \"jitse-jan-van-waterschoot\", \"bio\": \"\", \"title\": \"\", \"linkedin_url\": \"\", \"facebook_url\": \"\", \"instagram_url\": \"\", \"pinterest_url\": \"\", \"twitter_handle\": \"\", \"profile_image\": \"\" }, \"categories\": [ { \"name\": \"Example Category\", \"slug\": \"example-category\" } ], \"tags\": [ { \"name\": \"Example Tag\", \"slug\": \"example-tag\" } ], \"featured_image\": \"https://d2devwt40at1e2.cloudfront.net/api/file/tdt3s1OHRO6wfQOpmAHw\", \"slug\": \"example-post\", \"title\": \"Example Post\", \"body\": \"<p>Welcome to ButterCMS! This an example blog post written using Butter.</p>\\n<h3>What's happening here?</h3>\\n<p>If you're viewing this post from your website or command line, you've successfully made a request to&nbsp;the <a href=\\\"https://buttercms.com/docs/api\\\">Butter API</a>. If you haven't already, make sure you have our <a href=\\\"https://buttercms.com/docs/\\\">development guides</a> pulled up for step-by-step instructions on setting up Butter.</p>\\n<h3>How does&nbsp;editing work?</h3>\\n<p>Butter's WYSIWYG editor supports standard text formatting including headings, links, quotes, code, text alignment, and more. You can upload, crop, and resize images which are automatically hosted and delivered through a CDN (see below). You can also edit HTML directly when needed.</p>\\n<figure class=\\\"image\\\"><img src=\\\"https://d2wzhk7xhrnk1x.cloudfront.net/rgPM9aHoSSKnjk44TQlD_butter-blog-post.jpg\\\" alt=\\\"Delivered to you via CDN\\\" />\\n<figcaption>Delivered to you via CDN</figcaption>\\n</figure>\\n<h3>Can I use Butter as a full CMS for&nbsp;things other than a&nbsp;blog?</h3>\\n<p>Yes. Butter can be used as a full CMS for managing dynamic content and creating pages across your entire website or app. Check out our <a href=\\\"https://buttercms.com/docs/\\\">development guides</a> for step-by-step tutorials on setting this up.</p>\\n\", \"summary\": \"This is an example blog post. Pretty neat huh?\", \"seo_title\": \"Example Post SEO Optimized Title\", \"meta_description\": \"This is our example blog posts SEO optimized meta description.\", \"status\": \"published\" } ], \"meta\": { \"next_page\": null, \"count\": 2, \"previous_page\": null } } In [6]: posts = client . posts . all ({ 'page_size' : 10 , 'page' : 1 }) In [7]: posts [ 'data' ][ 0 ] Out[7]: {'url': 'jupyter-notebook', 'created': '2018-06-14T11:53:54.533383Z', 'published': '2018-06-14T11:50:00Z', 'author': {'first_name': 'Jitse-Jan', 'last_name': 'van Waterschoot', 'email': 'code@jitsejan.com', 'slug': 'jitse-jan-van-waterschoot', 'bio': '', 'title': '', 'linkedin_url': '', 'facebook_url': '', 'instagram_url': '', 'pinterest_url': '', 'twitter_handle': '', 'profile_image': ''}, 'categories': [{'name': 'development', 'slug': 'development'}], 'tags': [{'name': '', 'slug': ''}, {'name': 'buttercms', 'slug': 'buttercms'}, {'name': 'python', 'slug': 'python'}, {'name': 'vuejs', 'slug': 'vuejs'}], 'featured_image': 'https://cdn.buttercms.com/eBXUxLoCR4CKQMUnhjur', 'slug': 'jupyter-notebook', 'title': 'Jupyter notebook', 'body': '<p>I am working on a Jupyter notebook to show how to work with ButterCMS, VuejS and Python and how it all smoothly works together.&nbsp;</p>', 'summary': 'I am working on a Jupyter notebook to show how to work with ButterCMS, VuejS and Python and how it all smoothly works together.', 'seo_title': 'Jupyter notebook', 'meta_description': 'I am working on a Jupyter notebook to show how to work with ButterCMS, VuejS and Python and how it all smoothly works together.', 'status': 'published'} In [8]: title = posts [ 'data' ][ 0 ][ 'title' ] title Out[8]: 'Jupyter notebook' Javascript implementation In [9]: %% javascript require . config ({ paths : { buttercms : \"https://cdnjs.buttercms.com/buttercms-1.1.1.min\" , }, shim : { buttercms : { exports : \"ButterCMS\" }, } }); In [10]: %% javascript require ([ 'buttercms' ], function ( ButterCMS ) { var butter = Butter ( '1b13611e144e11bbce5b484f4064e39638b223a1' ); butter . post . list ({ page : 1 , page_size : 10 }). then ( function ( response ) { console . log ( response [ 'data' ][ 'data' ][ 0 ][ 'title' ]) console . log ( response [ 'data' ][ 'data' ][ 0 ][ 'body' ]) }) }); VueJS implementation Finally an attempt to integrate VueJS with Jupyter using data from the ButterCMS. Following this guide we are able to add the blog posts to the body of this notebook. In [11]: %% javascript require . config ({ paths : { buttercms : \"https://cdnjs.buttercms.com/buttercms-1.1.1.min\" , vue : \"https://cdn.jsdelivr.net/npm/vue/dist/vue\" }, shim : { buttercms : { exports : \"ButterCMS\" }, vue : { exports : \"Vue\" } } }); In [12]: %%html < script type = \"text/x-template\" id = \"blog-home-template\" > < div id = \"blog-home\" > < h1 > {{ page_title }} < /h1> <!-- Create `v-for` and apply a `key` for Vue . Here we are using a combination of the slug and index . --> < div v - for = \"(post,index) in posts\" : key = \"post.slug + '_' + index\" > < article class = \"media\" > < figure > <!-- Bind results using a `:` --> <!-- Use a `v-if` / `else` if their is a `featured_image` --> < img v - if = \"post.featured_image\" : src = \"post.featured_image\" alt = \"\" > < img v - else src = \"http://via.placeholder.com/250x250\" alt = \"\" > < /figure> < h2 > {{ post . title }} < /h2> < p > {{ post . summary }} < /p> < /article> < /div> < /div> </ script > In [13]: %%html < div id = \"vue-app\" > < blog-home :posts = \"blogPosts\" /> </ div > In [14]: %% javascript require ([ 'buttercms' , 'vue' ], function ( ButterCMS , Vue ) { console . log ( Vue . version ); var butter = Butter ( '1b13611e144e11bbce5b484f4064e39638b223a1' ); var BlogHome = Vue . component ( 'blog-home' , { template : '#blog-home-template' , props : { data : Array , }, data : function () { return { page_title : 'Blog' , posts : [] } }, methods : { getPosts () { butter . post . list ({ page : 1 , page_size : 10 }). then (( res ) => { this . posts = res . data . data }) } }, created () { this . getPosts () } }) var vueApp = new Vue ({ el : '#vue-app' , components : { BlogHome : BlogHome }, data : { blogPosts : [] } }) }); In [15]: %%html < style > # blog-home article { width : 50 % ; float : left ; } # blog-home article figure img { height : 50 px ; } </ style > Result To put everything together I have created a JSfiddle and embedded the result below. In [16]: %%html < iframe width = \"100%\" height = \"500\" src = \"//jsfiddle.net/z4hapsLt/13/embedded/\" allowfullscreen = \"allowfullscreen\" allowpaymentrequest frameborder = \"0\" ></ iframe >", "tags": "posts", "url": "using-buttermcs-in-jupyter.html", "loc": "using-buttermcs-in-jupyter.html" }, { "title": "Creating Pandas dataframe from Azure Table Storage", "text": "import pandas as pd from azure.cosmosdb.table.tableservice import TableService CONNECTION_STRING = \"DUMMYSTRING\" SOURCE_TABLE = \"DUMMYTABLE\" def set_table_service (): \"\"\" Set the Azure Table Storage service \"\"\" return TableService ( connection_string = CONNECTION_STRING ) def get_dataframe_from_table_storage_table ( table_service , filter_query ): \"\"\" Create a dataframe from table storage data \"\"\" return pd . DataFrame ( get_data_from_table_storage_table ( table_service , filter_query )) def get_data_from_table_storage_table ( table_service , filter_query ): \"\"\" Retrieve data from Table Storage \"\"\" for record in table_service . query_entities ( SOURCE_TABLE , filter = filter_query ): yield record fq = \"PartitionKey eq '12345'\" ts = set_table_service () df = get_dataframe_from_table_storage_table ( table_service = ts , filter_query = fq )", "tags": "posts", "url": "creating-dataframe-from-table-storage.html", "loc": "creating-dataframe-from-table-storage.html" }, { "title": "Creating prediction model as a service using Flask and Docker", "text": "Introduction The aim of this notebook is to create a machine learning model and transform it into an API which, when given some novel input parameters, returns the model's prediction. The model The model that is going to be used is a Random Forest model, built using a data set of Titanic passengers ( data/train.csv ). The model looks to predict the probability of whether a passenger would have survived. The goal Input is an API call such as /predict?class=2&sex=male&age=22&sibsp=2&parch=0&title=mr with a response in the form of { \"probabilityOfSurvival\": 0.95 } Creating the model In [1]: import pandas as pd Import the data from the CSV file train.csv. In [2]: train = pd . read_csv ( 'data/train.csv' ) Explore the data. In [3]: train . head () Out[3]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S In [4]: train . dtypes Out[4]: PassengerId int64 Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype: object Get the mapping for the gender by creating a function that retrieves the categories of a column of type category . This will return a number and its corresponding value. In [5]: def _get_category_mapping ( column ): \"\"\" Return the mapping of a category \"\"\" return dict ([( cat , code ) for code , cat in enumerate ( column . cat . categories )]) The unique values before converting to a category: In [6]: train [ 'Sex' ] . unique () Out[6]: array(['male', 'female'], dtype=object) In [7]: train [ 'Sex' ] = train [ 'Sex' ] . astype ( 'category' ) sex_mapping = _get_category_mapping ( train [ 'Sex' ]) train [ 'Sex' ] = train [ 'Sex' ] . cat . codes and the values after converting: In [8]: sex_mapping Out[8]: {'female': 0, 'male': 1} We will keep the mapping so we can use it later once we deploy the Model as a Service. In [9]: train [ 'Sex' ] . unique () Out[9]: array([1, 0]) Create categories for the titles by extracting them from the names. In [10]: train [ 'Name' ] . head ( 10 ) Out[10]: 0 Braund, Mr. Owen Harris 1 Cumings, Mrs. John Bradley (Florence Briggs Th... 2 Heikkinen, Miss. Laina 3 Futrelle, Mrs. Jacques Heath (Lily May Peel) 4 Allen, Mr. William Henry 5 Moran, Mr. James 6 McCarthy, Mr. Timothy J 7 Palsson, Master. Gosta Leonard 8 Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg) 9 Nasser, Mrs. Nicholas (Adele Achem) Name: Name, dtype: object In [11]: FRENCH_MAPPING = { 'Mme' : 'Mrs' , # Madame 'Mlle' : 'Miss' , # Mademoiselle 'M.' : 'Mr' , # Monsieur } In [12]: FINAL_TITLES = [ 'master' , 'miss' , 'mr' , 'mrs' ] In [13]: import re def _extract_title ( column ): \"\"\" Extract the title \"\"\" # Remove dots title_column = column . apply ( lambda x : re . sub ( r '(.*, )|(\\..*)' , '' , x ) . lower ()) . astype ( str ) # Map the French to English titles title_column = title_column . replace ( FRENCH_MAPPING ) # Create the categories based on the final titles and the rare title title_column = title_column . apply ( lambda x : 'rare title' if x not in FINAL_TITLES else x ) return title_column In [14]: train [ 'Title' ] = _extract_title ( train [ 'Name' ]) In [15]: train [[ 'Name' , 'Title' ]] . head () Out[15]: Name Title 0 Braund, Mr. Owen Harris mr 1 Cumings, Mrs. John Bradley (Florence Briggs Th... mrs 2 Heikkinen, Miss. Laina miss 3 Futrelle, Mrs. Jacques Heath (Lily May Peel) mrs 4 Allen, Mr. William Henry mr In [16]: import matplotlib.pyplot as plt % matplotlib inline train . groupby ( 'Title' )[ 'Name' ] . count () . plot . pie ( title = \"Distribution of titles\" ) Out[16]: <matplotlib.axes._subplots.AxesSubplot at 0x7f45edca0cf8> Let's now convert the titles to a category as we did for the gender: In [26]: train [ 'Title' ] = train [ 'Title' ] . astype ( \"category\" ) title_mapping = _get_category_mapping ( train [ 'Title' ]) train [ 'Title' ] = train [ 'Title' ] . cat . codes In [29]: title_mapping Out[29]: {'master': 0, 'miss': 1, 'mr': 2, 'mrs': 3, 'rare title': 4} Let us now investigate the ages of the people aboard the Titanic. In [17]: train . groupby ( 'Age' )[ 'Name' ] . count () . nlargest ( 20 ) . plot . bar ( title = \"Top 20 ages\" ) Out[17]: <matplotlib.axes._subplots.AxesSubplot at 0x7f45eda0cac8> In [18]: train [ train [ 'Age' ] . isnull ()] Out[18]: PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title 5 6 0 3 Moran, Mr. James 1 NaN 0 0 330877 8.4583 NaN Q mr 17 18 1 2 Williams, Mr. Charles Eugene 1 NaN 0 0 244373 13.0000 NaN S mr 19 20 1 3 Masselmani, Mrs. Fatima 0 NaN 0 0 2649 7.2250 NaN C mrs 26 27 0 3 Emir, Mr. Farred Chehab 1 NaN 0 0 2631 7.2250 NaN C mr 28 29 1 3 O'Dwyer, Miss. Ellen \"Nellie\" 0 NaN 0 0 330959 7.8792 NaN Q miss 29 30 0 3 Todoroff, Mr. Lalio 1 NaN 0 0 349216 7.8958 NaN S mr 31 32 1 1 Spencer, Mrs. William Augustus (Marie Eugenie) 0 NaN 1 0 PC 17569 146.5208 B78 C mrs 32 33 1 3 Glynn, Miss. Mary Agatha 0 NaN 0 0 335677 7.7500 NaN Q miss 36 37 1 3 Mamee, Mr. Hanna 1 NaN 0 0 2677 7.2292 NaN C mr 42 43 0 3 Kraeff, Mr. Theodor 1 NaN 0 0 349253 7.8958 NaN C mr 45 46 0 3 Rogers, Mr. William John 1 NaN 0 0 S.C./A.4. 23567 8.0500 NaN S mr 46 47 0 3 Lennon, Mr. Denis 1 NaN 1 0 370371 15.5000 NaN Q mr 47 48 1 3 O'Driscoll, Miss. Bridget 0 NaN 0 0 14311 7.7500 NaN Q miss 48 49 0 3 Samaan, Mr. Youssef 1 NaN 2 0 2662 21.6792 NaN C mr 55 56 1 1 Woolner, Mr. Hugh 1 NaN 0 0 19947 35.5000 C52 S mr 64 65 0 1 Stewart, Mr. Albert A 1 NaN 0 0 PC 17605 27.7208 NaN C mr 65 66 1 3 Moubarek, Master. Gerios 1 NaN 1 1 2661 15.2458 NaN C master 76 77 0 3 Staneff, Mr. Ivan 1 NaN 0 0 349208 7.8958 NaN S mr 77 78 0 3 Moutal, Mr. Rahamin Haim 1 NaN 0 0 374746 8.0500 NaN S mr 82 83 1 3 McDermott, Miss. Brigdet Delia 0 NaN 0 0 330932 7.7875 NaN Q miss 87 88 0 3 Slocovski, Mr. Selman Francis 1 NaN 0 0 SOTON/OQ 392086 8.0500 NaN S mr 95 96 0 3 Shorney, Mr. Charles Joseph 1 NaN 0 0 374910 8.0500 NaN S mr 101 102 0 3 Petroff, Mr. Pastcho (\"Pentcho\") 1 NaN 0 0 349215 7.8958 NaN S mr 107 108 1 3 Moss, Mr. Albert Johan 1 NaN 0 0 312991 7.7750 NaN S mr 109 110 1 3 Moran, Miss. Bertha 0 NaN 1 0 371110 24.1500 NaN Q miss 121 122 0 3 Moore, Mr. Leonard Charles 1 NaN 0 0 A4. 54510 8.0500 NaN S mr 126 127 0 3 McMahon, Mr. Martin 1 NaN 0 0 370372 7.7500 NaN Q mr 128 129 1 3 Peter, Miss. Anna 0 NaN 1 1 2668 22.3583 F E69 C miss 140 141 0 3 Boulos, Mrs. Joseph (Sultana) 0 NaN 0 2 2678 15.2458 NaN C mrs 154 155 0 3 Olsen, Mr. Ole Martin 1 NaN 0 0 Fa 265302 7.3125 NaN S mr ... ... ... ... ... ... ... ... ... ... ... ... ... ... 718 719 0 3 McEvoy, Mr. Michael 1 NaN 0 0 36568 15.5000 NaN Q mr 727 728 1 3 Mannion, Miss. Margareth 0 NaN 0 0 36866 7.7375 NaN Q miss 732 733 0 2 Knight, Mr. Robert J 1 NaN 0 0 239855 0.0000 NaN S mr 738 739 0 3 Ivanoff, Mr. Kanio 1 NaN 0 0 349201 7.8958 NaN S mr 739 740 0 3 Nankoff, Mr. Minko 1 NaN 0 0 349218 7.8958 NaN S mr 740 741 1 1 Hawksford, Mr. Walter James 1 NaN 0 0 16988 30.0000 D45 S mr 760 761 0 3 Garfirth, Mr. John 1 NaN 0 0 358585 14.5000 NaN S mr 766 767 0 1 Brewe, Dr. Arthur Jackson 1 NaN 0 0 112379 39.6000 NaN C rare title 768 769 0 3 Moran, Mr. Daniel J 1 NaN 1 0 371110 24.1500 NaN Q mr 773 774 0 3 Elias, Mr. Dibo 1 NaN 0 0 2674 7.2250 NaN C mr 776 777 0 3 Tobin, Mr. Roger 1 NaN 0 0 383121 7.7500 F38 Q mr 778 779 0 3 Kilgannon, Mr. Thomas J 1 NaN 0 0 36865 7.7375 NaN Q mr 783 784 0 3 Johnston, Mr. Andrew G 1 NaN 1 2 W./C. 6607 23.4500 NaN S mr 790 791 0 3 Keane, Mr. Andrew \"Andy\" 1 NaN 0 0 12460 7.7500 NaN Q mr 792 793 0 3 Sage, Miss. Stella Anna 0 NaN 8 2 CA. 2343 69.5500 NaN S miss 793 794 0 1 Hoyt, Mr. William Fisher 1 NaN 0 0 PC 17600 30.6958 NaN C mr 815 816 0 1 Fry, Mr. Richard 1 NaN 0 0 112058 0.0000 B102 S mr 825 826 0 3 Flynn, Mr. John 1 NaN 0 0 368323 6.9500 NaN Q mr 826 827 0 3 Lam, Mr. Len 1 NaN 0 0 1601 56.4958 NaN S mr 828 829 1 3 McCormack, Mr. Thomas Joseph 1 NaN 0 0 367228 7.7500 NaN Q mr 832 833 0 3 Saad, Mr. Amin 1 NaN 0 0 2671 7.2292 NaN C mr 837 838 0 3 Sirota, Mr. Maurice 1 NaN 0 0 392092 8.0500 NaN S mr 839 840 1 1 Marechal, Mr. Pierre 1 NaN 0 0 11774 29.7000 C47 C mr 846 847 0 3 Sage, Mr. Douglas Bullen 1 NaN 8 2 CA. 2343 69.5500 NaN S mr 849 850 1 1 Goldenberg, Mrs. Samuel L (Edwiga Grabowska) 0 NaN 1 0 17453 89.1042 C92 C mrs 859 860 0 3 Razi, Mr. Raihed 1 NaN 0 0 2629 7.2292 NaN C mr 863 864 0 3 Sage, Miss. Dorothy Edith \"Dolly\" 0 NaN 8 2 CA. 2343 69.5500 NaN S miss 868 869 0 3 van Melkebeke, Mr. Philemon 1 NaN 0 0 345777 9.5000 NaN S mr 878 879 0 3 Laleff, Mr. Kristo 1 NaN 0 0 349217 7.8958 NaN S mr 888 889 0 3 Johnston, Miss. Catherine Helen \"Carrie\" 0 NaN 1 2 W./C. 6607 23.4500 NaN S miss 177 rows × 13 columns In the dataset the age is missing for 177 persons. Use a linear model to calculate the age based on the class, gender and number of siblings/spouses based on the rows with an age. In [19]: LIN_MOD_FEATURES = [ 'Pclass' , 'Sex' , 'SibSp' ] In [20]: LIN_MOD_TARGET = [ 'Age' ] In [21]: from sklearn import linear_model def _create_linear_model ( frame ): \"\"\" Create linear model \"\"\" imput = frame [ frame . Age . notnull ()] features = imput [ LIN_MOD_FEATURES ] target = imput [ LIN_MOD_TARGET ] model = linear_model . LinearRegression () model . fit ( features , target ) return model In [22]: linear_mod = _create_linear_model ( train ) Calculate the predicted age for all the rows: In [23]: train [ 'PredictedAge' ] = linear_mod . predict ( train [ LIN_MOD_FEATURES ]) Merge the predicted age into the dataframe where there is no age yet: In [24]: train [ 'Age' ] = train . apply ( lambda x : x . Age if pd . notnull ( x . Age ) else x . PredictedAge , axis = 1 ) Drop the prediction column since it is not needed anymore: In [25]: train . drop ([ 'PredictedAge' ], axis = 1 , inplace = True ) To estimate the chance of survival we will use a Random Forest Classifier. In [36]: FEATURES = [ 'Pclass' , 'Sex' , 'Age' , 'SibSp' , 'Parch' , 'Title' , ] TARGET = 'Survived' NUM_TREES = 500 MAX_FEATURES = 2 In [42]: from sklearn.ensemble import RandomForestClassifier def _create_random_forest_classifier ( frame ): \"\"\" Build a random forest classifier \"\"\" features = frame [ FEATURES ] target = frame [ TARGET ] model = RandomForestClassifier ( n_estimators = NUM_TREES , max_features = MAX_FEATURES , random_state = 754 ) model . fit ( features , target ) return model In [43]: random_forest_classifier = _create_random_forest_classifier ( train ) As a final step for the MaaS we will save the model and the two mappings to disk. This way we can upload them to the microservice in the next step. In [40]: from sklearn.externals import joblib def _save_variable ( variable , filename ): \"\"\" Save a variable to a file \"\"\" joblib . dump ( variable , filename ) In [41]: _save_variable ( random_forest_classifier , 'random_forest.mdl' ) _save_variable ( title_mapping , 'title_mapping.pkl' ) _save_variable ( sex_mapping , 'sex_mapping.pkl' ) Deploying the model as a service To deploy the model as a service, I am going to use the web framework Flask. This makes it easy to interact with the variables we saved in the previous step and it is straighforward to create a simple web app with only a few routes. The app.py contains all the magic and will be used in the Dockerfile to get the container with the model online. This is the code for the main application: In [44]: %%file app.py #!/usr/bin/env python # # -*- coding: utf-8 -*- \"\"\" Flask API for predicting probability of survival \"\"\" import json import sys from flask import Flask , jsonify , request , render_template , url_for from sklearn.externals import joblib import numpy as np try : saved_model = joblib . load ( 'random_forest.mdl' ) sex_mapping = joblib . load ( 'sex_mapping.pkl' ) title_mapping = joblib . load ( 'title_mapping.pkl' ) except : print ( \"Error loading application. Please run `python create_random_forest.py` first!\" ) sys . exit ( 0 ) app = Flask ( __name__ ) @app . route ( '/' ) def main (): \"\"\" Main page of the API \"\"\" return \"This is the main page\" @app . route ( '/predict' , methods = [ 'GET' ]) def predict (): \"\"\" Predict the probability of survival \"\"\" args = request . args required_args = [ 'class' , 'sex' , 'age' , 'sibsp' , 'parch' , 'title' ] # Simple error handling for the arguments diff = set ( required_args ) . difference ( set ( args . keys ())) if len ( diff ) > 0 : return \"Error: wrong arguments. Missing arguments {} \" . format ( str ( diff )) person_features = np . array ([ args [ 'class' ], sex_mapping [ args [ 'sex' ]], args [ 'age' ], args [ 'sibsp' ], args [ 'parch' ], title_mapping [ args [ 'title' ] . lower ()] ]) . reshape ( 1 , - 1 ) probability = saved_model . predict_proba ( person_features )[:, 1 ][ 0 ] return jsonify ({ 'probabilityOfSurvival' : probability }) if __name__ == '__main__' : app . run ( host = '0.0.0.0' ) Writing app.py Breakdown The first bit try : saved_model = joblib . load ( 'random_forest.mdl' ) sex_mapping = joblib . load ( 'sex_mapping.pkl' ) title_mapping = joblib . load ( 'title_mapping.pkl' ) except : print ( \"Error loading application. Please run `python create_random_forest.py` first!\" ) sys . exit ( 0 ) will import the three variables that we saved during training the model. We can use the mappings to map the input arguments of the API to the proper fields of the model. The next part is default for a Flask application: app = Flask ( __name__ ) @app . route ( '/' ) def main (): \"\"\" Main page of the API \"\"\" return \"This is the main page\" The predict route is slightly more advanced. It will take parameters using the GET method and verify if all six required arguments are present. args = request . args required_args = [ 'class' , 'sex' , 'age' , 'sibsp' , 'parch' , 'title' ] # Simple error handling for the arguments diff = set ( required_args ) . difference ( set ( args . keys ())) if len ( diff ) > 0 : return \"Error: wrong arguments. Missing arguments {} \" . format ( str ( diff )) If all arguments are present, it will create a feature array to feed to the prediction model by using numpy . This is where the mappings come in, as an input for the title we have a string, but this is mapped to the corresponding index of the mapping to match to the feature in the model. person_features = np . array ([ args [ 'class' ], sex_mapping [ args [ 'sex' ]], args [ 'age' ], args [ 'sibsp' ], args [ 'parch' ], title_mapping [ args [ 'title' ] . lower ()] ]) . reshape ( 1 , - 1 ) Finally the probability is calculated based on the features and the probability is fed back using a JSON object. probability = saved_model . predict_proba ( person_features )[:, 1 ][ 0 ] return jsonify ({ 'probabilityOfSurvival' : probability }) The last bit of app.py is the default way of starting the Flask server. I have modified the host from localhost to 0.0.0.0 in order to be able to access the API from any IP address. app . run ( host = '0.0.0.0' ) Docker To run the model as a service, I will use Docker to create a container where the server is running and the endpoint for the prediction is exposed. The Dockerfile is very basic. It will use Python 3, copy the contents to the container, install the requirements and start the server. # Base image FROM python:3 # Copy contents COPY . /app # Change work directory WORKDIR /app # Install the requirements RUN pip install -r requirements.txt # Start the application CMD [ \"python\" , \"app.py\" ] where requirements.txt contains the following: certifi==2018.1.18 click==6.7 Flask==0.12.2 itsdangerous==0.24 Jinja2==2.10 MarkupSafe==1.0 numpy==1.14.0 pandas==0.22.0 python-dateutil==2.6.1 pytz==2017.3 scikit-learn==0.19.1 scipy==1.0.0 six==1.11.0 Werkzeug==0.14.1 The docker-compose.yml will simply build the current Dockerfile and expose port 5000. version: '2' services: flask: build: . ports: - \"5000:5000\" Running docker-compose up -d in this folder will now start the server and by going to the IP of the machine the endpoint should be visible on port 5000 and route predict . Execution Let's put it to the test: create the files, spin up Docker and check the API response. In [45]: %%file Dockerfile # Base image FROM python : 3 # Copy contents COPY . / app # Change work directory WORKDIR / app # Install the requirements RUN pip install - r requirements . txt # Start the application CMD [ \"python\" , \"app.py\" ] Writing Dockerfile In [46]: %%file requirements.txt certifi == 2018.1 . 18 click == 6.7 Flask == 0.12 . 2 itsdangerous == 0.24 Jinja2 == 2.10 MarkupSafe == 1.0 numpy == 1.14 . 0 pandas == 0.22 . 0 python - dateutil == 2.6 . 1 pytz == 2017.3 scikit - learn == 0.19 . 1 scipy == 1.0 . 0 six == 1.11 . 0 Werkzeug == 0.14 . 1 Writing requirements.txt In [47]: %%file docker-compose.yml version : '2' services : flask : build : . ports : - \"5000:5000\" Writing docker-compose.yml In [51]: ! docker-compose up -d WARNING : The Docker Engine you're using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node. To deploy your application across the swarm, use `docker stack deploy`. WARNING : Found orphan containers (notebooks_anaconda_1) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up. Building flask Step 1/5 : FROM python:3 3: Pulling from library/python Digest: sha256:18e515f2cd7fd40c019bce12fda36b9a9c58613cf6fb8d6e58f831ef565a7b81 Status: Downloaded newer image for python:3 ---> d69bc9d9b016 Step 2/5 : COPY . /app ---> b97435c40d8d Step 3/5 : WORKDIR /app Removing intermediate container 15515f492696 ---> 277d4cd7e786 Step 4/5 : RUN pip install -r requirements.txt ---> Running in ba1465b7f6e3 Collecting certifi==2018.1.18 (from -r requirements.txt (line 1)) Downloading https://files.pythonhosted.org/packages/fa/53/0a5562e2b96749e99a3d55d8c7df91c9e4d8c39a9da1f1a49ac9e4f4b39f/certifi-2018.1.18-py2.py3-none-any.whl (151kB) Collecting click==6.7 (from -r requirements.txt (line 2)) Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB) Collecting Flask==0.12.2 (from -r requirements.txt (line 3)) Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB) Collecting itsdangerous==0.24 (from -r requirements.txt (line 4)) Downloading https://files.pythonhosted.org/packages/dc/b4/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4/itsdangerous-0.24.tar.gz (46kB) Collecting Jinja2==2.10 (from -r requirements.txt (line 5)) Downloading https://files.pythonhosted.org/packages/7f/ff/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731/Jinja2-2.10-py2.py3-none-any.whl (126kB) Collecting MarkupSafe==1.0 (from -r requirements.txt (line 6)) Downloading https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz Collecting numpy==1.14.0 (from -r requirements.txt (line 7)) Downloading https://files.pythonhosted.org/packages/dc/ac/5c270dffb864f23315e9c1f9e0a0b300c797b3c170666c031c4de42aacae/numpy-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (17.2MB) Collecting pandas==0.22.0 (from -r requirements.txt (line 8)) Downloading https://files.pythonhosted.org/packages/da/c6/0936bc5814b429fddb5d6252566fe73a3e40372e6ceaf87de3dec1326f28/pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB) Collecting python-dateutil==2.6.1 (from -r requirements.txt (line 9)) Downloading https://files.pythonhosted.org/packages/4b/0d/7ed381ab4fe80b8ebf34411d14f253e1cf3e56e2820ffa1d8844b23859a2/python_dateutil-2.6.1-py2.py3-none-any.whl (194kB) Collecting pytz==2017.3 (from -r requirements.txt (line 10)) Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB) Collecting scikit-learn==0.19.1 (from -r requirements.txt (line 11)) Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB) Collecting scipy==1.0.0 (from -r requirements.txt (line 12)) Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB) Collecting six==1.11.0 (from -r requirements.txt (line 13)) Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl Collecting Werkzeug==0.14.1 (from -r requirements.txt (line 14)) Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB) Building wheels for collected packages: itsdangerous, MarkupSafe Running setup.py bdist_wheel for itsdangerous: started Running setup.py bdist_wheel for itsdangerous: finished with status 'done' Stored in directory: /root/.cache/pip/wheels/2c/4a/61/5599631c1554768c6290b08c02c72d7317910374ca602ff1e5 Running setup.py bdist_wheel for MarkupSafe: started Running setup.py bdist_wheel for MarkupSafe: finished with status 'done' Stored in directory: /root/.cache/pip/wheels/33/56/20/ebe49a5c612fffe1c5a632146b16596f9e64676768661e4e46 Successfully built itsdangerous MarkupSafe Installing collected packages: certifi, click, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, numpy, pytz, six, python-dateutil, pandas, scikit-learn, scipy Successfully installed Flask-0.12.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 certifi-2018.1.18 click-6.7 itsdangerous-0.24 numpy-1.14.0 pandas-0.22.0 python-dateutil-2.6.1 pytz-2017.3 scikit-learn-0.19.1 scipy-1.0.0 six-1.11.0 Removing intermediate container ba1465b7f6e3 ---> efdb1e8577ad Step 5/5 : CMD [\"python\", \"app.py\"] ---> Running in 5f7361303e7b Removing intermediate container 5f7361303e7b ---> 1ad5cf22e305 Successfully built 1ad5cf22e305 Successfully tagged notebooks_flask:latest WARNING : Image for service flask was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`. Creating notebooks_flask_1 ... In [53]: ! docker ps | grep flask 5e7d954d4215 notebooks_flask \"python app.py\" 19 minutes ago Up 19 minutes 0.0.0.0:5000->5000/tcp notebooks_flask_1 Verification The following code will call the API with some parameters and print the result. In [55]: import requests params = { 'class' : 2 , 'age' : 22 , 'sibsp' : 2 , 'parch' : 0 , 'title' : 'mr' , 'sex' : 'male' , } url = 'http://hub.jitsejan.com:5000/predict' r = requests . get ( url , params ) print ( r . url ) print ( r . json ()) http://hub.jitsejan.com:5000/predict?class=2&age=22&sibsp=2&parch=0&title=mr&sex=male {'probabilityOfSurvival': 0.006333333333333333} And that is it! The JSON object can now be returned to the front-end of the web application and be displayed in a fancy way, but that is outside the scope of this notebook. Let's clean up by retrieving the Docker ID and removing the container. In [57]: ! docker stop $( docker ps -aqf \"name=flask\" ) 5e7d954d4215 In [58]: ! docker rm $( docker ps -aqf \"name=flask\" ) 5e7d954d4215 Check my Github for the original notebook.", "tags": "posts", "url": "creating-model-as-a-service.html", "loc": "creating-model-as-a-service.html" }, { "title": "Moving my blog to Github Pages", "text": "Since I am evaluating the use of the different servers that I own, I am trying to move things away from the servers to free tools or platforms. For example, you can host notebooks on Azure for free and Github supports the hosting of static websites on Github pages . Ideally I would like to remove two of the three servers that I own to both reduce costs and improve my knowledge of the different available platforms. Below I describe the steps I took to move my website to the new location. Creating the Github page Create two repositories https://github.com/new jitsejan/jitsejan.github.io-source (This repository contains source for the Pelican blog for my homepage) jitsejan/jitsejan.github.io (This repository contains the Pelican blog for my homepage) Set up the Pelican blog Copy the source repository to the machine: jitsejan@dev16:~/code$ git clone https://github.com/jitsejan/jitsejan.github.io-source.git Cloning into 'jitsejan.github.io-source' ... remote: Counting objects: 4 , done . remote: Compressing objects: 100 % ( 4 /4 ) , done . remote: Total 4 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 Unpacking objects: 100 % ( 4 /4 ) , done . Checking connectivity... done . Verify the repository is correct: jitsejan@dev16:~/code$ cd jitsejan.github.io-source/ jitsejan@dev16:~/code/jitsejan.github.io-source$ git remote -v origin https://github.com/jitsejan/jitsejan.github.io-source.git ( fetch ) origin https://github.com/jitsejan/jitsejan.github.io-source.git ( push ) Add the output repository as a submodule to the source repo: jitsejan@dev16:~/code/jitsejan.github.io-source$ git submodule add https://github.com/jitsejan/jitsejan.github.io.git output Cloning into 'output' ... remote: Counting objects: 4 , done . remote: Compressing objects: 100 % ( 4 /4 ) , done . remote: Total 4 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 Unpacking objects: 100 % ( 4 /4 ) , done . Checking connectivity... done . Create the Pelican application using the quickstart tool: jitsejan@dev16:~/code/jitsejan.github.io-source$ pelican-quickstart Welcome to pelican-quickstart v3.6.3. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [ . ] > What will be the title of this web site? JJs World > Who will be the author of this web site? Jitse-Jan > What will be the default language of this web site? [ en ] > Do you want to specify a URL prefix? e.g., http://example.com ( Y/n ) > What is your URL prefix? ( see above example ; no trailing slash ) https://jitsejan.github.io > Do you want to enable article pagination? ( Y/n ) > How many articles per page do you want? [ 10 ] > What is your time zone? [ Europe/Paris ] Europe/London > Do you want to generate a Fabfile/Makefile to automate generation and publishing? ( Y/n ) > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? ( Y/n ) > Do you want to upload your website using FTP? ( y/N ) n > Do you want to upload your website using SSH? ( y/N ) n > Do you want to upload your website using Dropbox? ( y/N ) n > Do you want to upload your website using S3? ( y/N ) n > Do you want to upload your website using Rackspace Cloud Files? ( y/N ) n > Do you want to upload your website using GitHub Pages? ( y/N ) Y > Is this your personal page ( username.github.io ) ? ( y/N ) Y Error: [ Errno 17 ] File exists: '/home/jitsejan/code/jitsejan.github.io-source/output' Done. Your new project is available at /home/jitsejan/code/jitsejan.github.io-source Change the publishconf.py to not delete the output directory: #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals # This file is only used if you use `make publish` or # explicitly specify it as your config file. import os import sys sys . path . append ( os . curdir ) from pelicanconf import * SITEURL = 'http://jitsejan.github.io' RELATIVE_URLS = False FEED_ALL_ATOM = 'feeds/all.atom.xml' CATEGORY_FEED_ATOM = 'feeds/ %s .atom.xml' DELETE_OUTPUT_DIRECTORY = False # Following items are often useful when publishing #DISQUS_SITENAME = \"\" #GOOGLE_ANALYTICS = \"\" Create the first post: jitsejan@dev16:~/code/jitsejan.github.io-source$ cat content/first-post.md Title: My first post Date: 2018 -04-26 13 :00 Modified: 2018 -04-26 13 :00 Tags: fipo Category: dummy-data Slug: my-first-post Authors: Jitse-Jan Summary: The first post of the Pelican blog on Github pages. I am migrating my website from my VPS to Github pages as an experiment. Build the website and serve its contents: jitsejan@dev16:~/code/jitsejan.github.io-source$ make html && make serve pelican /home/jitsejan/code/pelican-source/content -o /home/jitsejan/code/pelican-source/output -s /home/jitsejan/code/pelican-source/pelicanconf.py WARNING: No valid files found in content. Done: Processed 0 articles, 0 drafts, 0 pages and 0 hidden pages in 0 .07 seconds. cd /home/jitsejan/code/pelican-source/output && python -m pelican.server Generate the static content for the website: jitsejan@dev16:~/code/jitsejan.github.io-source$ make publish pelican /home/jitsejan/code/pelican-source/content -o /home/jitsejan/code/pelican-source/output -s /home/jitsejan/code/pelican-source/publishconf.py Done: Processed 1 article, 0 drafts, 0 pages and 0 hidden pages in 0 .14 seconds. Push the output code to the jitsejan.github.io repository: jitsejan@dev16:~/code/jitsejan.github.io-source$ cd output/ jitsejan@dev16:~/code/jitsejan.github.io-source/output$ git add . jitsejan@dev16:~/code/jitsejan.github.io-source/output$ git commit -m 'My first post' jitsejan@dev16:~/code/jitsejan.github.io-source/output$ git push -u origin master Push the source code to the jitsejan.github.io-source repository: jitsejan@dev16:~/code/jitsejan.github.io-source/output$ cd .. jitsejan@dev16:~/code/jitsejan.github.io-source$ echo '*.pyc' >> .gitignore jitsejan@dev16:~/code/jitsejan.github.io-source$ git add . jitsejan@dev16:~/code/jitsejan.github.io-source$ git commit -m 'My first commit' jitsejan@dev16:~/code/jitsejan.github.io-source$ git push -u origin master Now the homepage is ready on https://jitsejan.github.io/ ! Inspiration link Merge the original Pelican blog Copy the data Clone the old repository and overwrite the contents of the Github pages repo: jitsejan@dev16:~/code$ git clone https://github.com/jitsejan/pelican-blog jitsejan@dev16:~/code$ rm -rf jitsejan.github.io-source/content/ jitsejan@dev16:~/code$ rm -rf ../jitsejan.github.io-source/themes/ jitsejan@dev16:~/code$ mv pelican-blog/* jitsejan.github.io-source Additionally the DELETE_OUTPUT_DIRECTORY had to be set to False again in publishconf.py . Issues Some Python libraries were missing and the pip install had to be done first: jitsejan@dev16:~/code/jitsejan.github.io-source$ pip install -r requirements.txt The less compiler was missing: jitsejan@dev16:~/code$ sudo npm install -g less I had to remove the ipynb plugin and reinstall it by adding it again as a submodule: git submodule add git://github.com/danielfrg/pelican-ipynb.git plugins/ipynb Final steps To make the Github URL work with my own domain, I had to follow several steps: Add the custom domain name ( www.jitsejan.com ) to the Github Page repository in a file CNAME in the root (or via the Settings tab) Update the DNS record for my DreamHost account and modify the two primary nameservers to point to Cloudflare and remove the additional ones. (Nameserver 1: janet.ns.cloudflare.com, Nameserver 2: marty.ns.cloudflare.com) Add DNS records in Cloudflare. One CNAME for www and one CNAME for jitsejan.com to point to jitsejan.github.io . Done. No more server needed for hosting a blog!", "tags": "posts", "url": "moving-blog-to-github-pages.html", "loc": "moving-blog-to-github-pages.html" }, { "title": "MEVN stack experiment with Jupyter", "text": "In this notebook I want to play with Jupyter and show the steps of how to create a MEVN application from a notebook. Normally I would do this in the normal Linux terminal and a text editor, but since we can combine code, explanation and shell commands, I want to create a story in this notebook which hopefully will be of any help of people experimenting with full-stack development. I will create the application, use some simple Linux tricks and use Selenium to test the application. Please note development like this is far from optimal, but I think it is very cool what you can achieve without leaving your notebook, knowing a bit of Docker, Linux and Python. The original notebook can be found here . Objective Setup an application with the following elements MongoDB ExpressJS VueJS NodeJS using Jupyter notebook Docker Scrapy PyMongo Selenium Notebook settings In [1]: %%html < style > img . logo { height : 100 px ; } img . screenshot { max-width : 500 px ; -webkit- filter : drop-shadow ( 5 px 5 px 5 px #222 ); filter : drop-shadow ( 2 px 5 px 5 px #222 ); margin : 50 px auto ; } </ style > Clean up the images from earlier runs. In [2]: rm *. png rm: cannot remove '*.png': No such file or directory In [3]: from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" In [4]: ! jupyter --version 4.4.0 In [5]: ! jupyter notebook --version 5.2.2 Python version In [6]: import platform platform . python_version () Out[6]: '3.5.2' Docker setup Docker version In [7]: ! docker version Client: Version: 17.09.0-ce API version: 1.32 Go version: go1.8.3 Git commit: afdb6d4 Built: Tue Sep 26 22:42:18 2017 OS/Arch: linux/amd64 Server: Version: 17.09.0-ce API version: 1.32 (minimum version 1.12) Go version: go1.8.3 Git commit: afdb6d4 Built: Tue Sep 26 22:40:56 2017 OS/Arch: linux/amd64 Experimental: false In [8]: ! docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3661951201a7 selenium/standalone-chrome:latest \"/opt/bin/entry_po...\" About an hour ago Up About an hour 0.0.0.0:4444->4444/tcp selenium a1358fd1eb57 mongo:latest \"docker-entrypoint...\" 2 hours ago Up 2 hours 0.0.0.0:27017->27017/tcp mongo Docker client Setup the client pip3 install docker In [9]: import docker docker_client = docker . from_env () Available containers In [10]: for cntr in docker_client . containers . list (): print ( \"name= {} (id= {} )\" . format ( cntr . name , cntr . id )) name=selenium (id=3661951201a72dbb9cf29c6246d1650d9d67ec3b27254ad6f748a27aed6a6ea4) name=mongo (id=a1358fd1eb57cdcde386873a8134057da60ea9793c809ec6e9ba74a7867886fd) MongoDB Check if the Mongo Docker container is running, otherwise, start the container. In [11]: mongo_running = False for cntr in docker_client . containers . list (): if 'mongo' in cntr . attrs [ 'Config' ][ 'Image' ]: mongo_running = True container = cntr if mongo_running is False : container = docker_client . containers . run ( \"mongo:latest\" , name = 'mongo' , ports = { '27017' : '27017' }, detach = True ) In [12]: container Out[12]: <Container: a1358fd1eb> Verification that the Mongo container is running: In [13]: ! docker ps | grep mongo a1358fd1eb57 mongo:latest \"docker-entrypoint...\" 2 hours ago Up 2 hours 0.0.0.0:27017->27017/tcp mongo See Documentation to install PyMongo. $ pip install pymongo pip3 install pymongo In [14]: from pymongo import MongoClient mongo_client = MongoClient ( 'localhost' , 27017 ) Data gathering Create the Scrapy pipeline to write the scraping results to MongoDB. In [15]: import pymongo class MongoPipeline ( object ): collection_name = 'games' def __init__ ( self , mongo_uri , mongo_db ): self . mongo_uri = mongo_uri self . mongo_db = mongo_db @classmethod def from_crawler ( cls , crawler ): return cls ( mongo_uri = crawler . settings . get ( 'MONGO_URI' ), mongo_db = crawler . settings . get ( 'MONGO_DATABASE' , 'items' ) ) def open_spider ( self , spider ): self . client = pymongo . MongoClient ( self . mongo_uri ) self . db = self . client [ self . mongo_db ] def close_spider ( self , spider ): self . client . close () def process_item ( self , item , spider ): self . db [ self . collection_name ] . insert_one ( dict ( item )) return item Retrieving data from https://www.nintendo.co.jp/ir/en/finance/software/index.html with the following markup: < ul class = \"sales_layout\" > < li class = \"sales_layout_list\" > < div class = \"ta_l\" > < p class = \"sales_title\" > The Legend of Zelda: < br > Breath of the Wild </ p > < p class = \"sales_value\" >< span > 4.70 </ span > million pcs. </ p > </ div > < div class = \"ta_r\" > < p > < img src = \"./img/data_switch_001.png\" alt = \"\" width = \"110\" height = \"\" class = \"sales_product1\" > </ p > </ div > </ li > </ ul > Using Scrapy we can gather the data in a convenient way. pip3 install scrapy In [16]: import logging import scrapy from scrapy.crawler import CrawlerProcess import re class ConsoleSpider ( scrapy . Spider ): name = \"games\" start_urls = [ \"https://www.nintendo.co.jp/ir/en/finance/software/index.html\" , \"https://www.nintendo.co.jp/ir/en/finance/software/wiiu.html\" , \"https://www.nintendo.co.jp/ir/en/finance/software/3ds.html\" , \"https://www.nintendo.co.jp/ir/en/finance/software/wii.html\" , \"https://www.nintendo.co.jp/ir/en/finance/software/ds.html\" ] custom_settings = { 'LOG_LEVEL' : logging . CRITICAL , 'DOWNLOAD_DELAY' : . 25 , 'RANDOMIZE_DOWNLOAD_DELAY' : True , 'ITEM_PIPELINES' : { '__main__.MongoPipeline' : 300 , }, 'MONGO_URI' : 'mongodb://localhost:27017' , 'MONGO_DATABASE' : 'nintendo' } def parse ( self , response ): for cons in response . css ( 'li.sales_layout_list' ): yield { 'console' : response . css ( 'div.tab div.tabInner span::text' ) . extract_first (), 'name' : cons . css ( 'p.sales_title::text' ) . extract_first () . strip (), 'image' : 'https://www.nintendo.co.jp/ir/en/finance/software' + cons . css ( 'p img::attr(src)' ) . extract_first ()[ 1 :], 'sales' : cons . css ( 'p.sales_value span::text' ) . extract ()[ 0 ] . strip () } In [17]: process = CrawlerProcess ({ 'USER_AGENT' : 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process . crawl ( ConsoleSpider ) process . start () 2018-01-15 19:01:14 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot) 2018-01-15 19:01:14 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g 2 Nov 2017), cryptography 2.1.4, Platform Linux-4.4.0-x86_64-with-Ubuntu-16.04-xenial 2018-01-15 19:01:14 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 50, 'DOWNLOAD_DELAY': 0.25, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'} Out[17]: <Deferred at 0x7f38eda7bc50> Verify the Nintendo database is created In [18]: if 'nintendo' in mongo_client . database_names (): print ( 'Database found!' ) Database found! Verify the games collection is created inside the Nintendo database In [19]: db = mongo_client [ 'nintendo' ] if 'games' in db . collection_names (): print ( 'Collection found!' ) Collection found! Retrieve the games from the collection In [20]: games = list ( db [ 'games' ] . find ({})) In [21]: print ( \"Found {} games\" . format ( len ( games ))) Found 270 games Retrieve the images Delete the old images In [22]: ! rm -rf images/* In [23]: import re updated_games = [] for gameindex , game in enumerate ( games ): image = game [ 'image' ] image_short = image . split ( '/' )[ - 1 ] ! wget --no-check-certificate $image -P images/ game [ 'image' ] = 'images/' + image_short game . pop ( '_id' , None ) updated_games . append ( game ) --2018-01-15 19:01:16-- http://images/data_switch_001.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_001.png [following] --2018-01-15 19:01:16-- https://images/data_switch_001.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:16 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6780a') --2018-01-15 19:01:16-- http://images/data_switch_002.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_002.png [following] --2018-01-15 19:01:16-- https://images/data_switch_002.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:16 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6780b') --2018-01-15 19:01:16-- http://images/data_switch_005.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_005.png [following] --2018-01-15 19:01:16-- https://images/data_switch_005.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:16 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6780c') --2018-01-15 19:01:16-- http://images/data_switch_003.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_003.png [following] --2018-01-15 19:01:16-- https://images/data_switch_003.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:16 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6780d') --2018-01-15 19:01:16-- http://images/data_switch_004.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_004.png [following] --2018-01-15 19:01:16-- https://images/data_switch_004.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:16 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6780e') --2018-01-15 19:01:16-- http://images/data_wiiu_mariokart8.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mariokart8.png [following] --2018-01-15 19:01:16-- https://images/data_wiiu_mariokart8.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:16 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6780f') --2018-01-15 19:01:17-- http://images/data_wiiu_newmariou.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newmariou.png [following] --2018-01-15 19:01:17-- https://images/data_wiiu_newmariou.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:17 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67810') --2018-01-15 19:01:17-- http://images/data_wiiu_mario3dworld.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mario3dworld.png [following] --2018-01-15 19:01:17-- https://images/data_wiiu_mario3dworld.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:17 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67811') --2018-01-15 19:01:17-- http://images/data_wiiu_smashbros.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_smashbros.png [following] --2018-01-15 19:01:17-- https://images/data_wiiu_smashbros.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:17 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67812') --2018-01-15 19:01:17-- http://images/data_wiiu_nintendoland.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_nintendoland.png [following] --2018-01-15 19:01:17-- https://images/data_wiiu_nintendoland.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:17 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67813') --2018-01-15 19:01:18-- http://images/data_wiiu_splatoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_splatoon.jpg [following] --2018-01-15 19:01:18-- https://images/data_wiiu_splatoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:18 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67814') --2018-01-15 19:01:18-- http://images/data_wiiu_supermariomaker.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_supermariomaker.png [following] --2018-01-15 19:01:18-- https://images/data_wiiu_supermariomaker.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:18 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67815') --2018-01-15 19:01:18-- http://images/data_wiiu_newsuperluigiu.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newsuperluigiu.png [following] --2018-01-15 19:01:18-- https://images/data_wiiu_newsuperluigiu.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:18 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67816') --2018-01-15 19:01:18-- http://images/data_wiiu_zeldahd.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_zeldahd.png [following] --2018-01-15 19:01:18-- https://images/data_wiiu_zeldahd.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:18 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67817') --2018-01-15 19:01:18-- http://images/data_wiiu_marioparty10.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_marioparty10.jpg [following] --2018-01-15 19:01:18-- https://images/data_wiiu_marioparty10.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:18 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67818') --2018-01-15 19:01:18-- http://images/data_3ds_pokemonxy.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonxy.jpg [following] --2018-01-15 19:01:19-- https://images/data_3ds_pokemonxy.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:19 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67819') --2018-01-15 19:01:19-- http://images/data_3ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_002.jpg [following] --2018-01-15 19:01:19-- https://images/data_3ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:19 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6781a') --2018-01-15 19:01:19-- http://images/data_3ds_pokemonsunmoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonsunmoon.jpg [following] --2018-01-15 19:01:19-- https://images/data_3ds_pokemonsunmoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:19 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6781b') --2018-01-15 19:01:19-- http://images/data_3ds_pokemonoras.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonoras.jpg [following] --2018-01-15 19:01:19-- https://images/data_3ds_pokemonoras.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:19 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6781c') --2018-01-15 19:01:19-- http://images/data_3ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_003.jpg [following] --2018-01-15 19:01:19-- https://images/data_3ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:19 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6781d') --2018-01-15 19:01:19-- http://images/data_3ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_001.jpg [following] --2018-01-15 19:01:19-- https://images/data_3ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:19 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6781e') --2018-01-15 19:01:20-- http://images/data_3ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_004.jpg [following] --2018-01-15 19:01:20-- https://images/data_3ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:20 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6781f') --2018-01-15 19:01:20-- http://images/data_3ds_smashbros.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_smashbros.jpg [following] --2018-01-15 19:01:20-- https://images/data_3ds_smashbros.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:20 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67820') --2018-01-15 19:01:20-- http://images/data_3ds_tomodachicollection.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_tomodachicollection.jpg [following] --2018-01-15 19:01:20-- https://images/data_3ds_tomodachicollection.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:20 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67821') --2018-01-15 19:01:20-- http://images/data_3ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_009.jpg [following] --2018-01-15 19:01:20-- https://images/data_3ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:20 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67822') --2018-01-15 19:01:21-- http://images/data_wii_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_001.jpg [following] --2018-01-15 19:01:21-- https://images/data_wii_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:21 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67823') --2018-01-15 19:01:21-- http://images/data_wii_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_002.jpg [following] --2018-01-15 19:01:21-- https://images/data_wii_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:21 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67824') --2018-01-15 19:01:22-- http://images/data_wii_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_003.jpg [following] --2018-01-15 19:01:22-- https://images/data_wii_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:22 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67825') --2018-01-15 19:01:22-- http://images/data_wii_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_005.jpg [following] --2018-01-15 19:01:22-- https://images/data_wii_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:22 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67826') --2018-01-15 19:01:22-- http://images/data_wii_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_004.jpg [following] --2018-01-15 19:01:22-- https://images/data_wii_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:22 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67827') --2018-01-15 19:01:22-- http://images/data_wii_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_006.jpg [following] --2018-01-15 19:01:22-- https://images/data_wii_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:22 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67828') --2018-01-15 19:01:23-- http://images/data_wii_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_007.jpg [following] --2018-01-15 19:01:23-- https://images/data_wii_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:23 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67829') --2018-01-15 19:01:23-- http://images/data_wii_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_009.jpg [following] --2018-01-15 19:01:23-- https://images/data_wii_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:23 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6782a') --2018-01-15 19:01:23-- http://images/data_wii_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_008.jpg [following] --2018-01-15 19:01:23-- https://images/data_wii_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:23 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6782b') --2018-01-15 19:01:23-- http://images/data_wii_wiiparty.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_wiiparty.jpg [following] --2018-01-15 19:01:23-- https://images/data_wii_wiiparty.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:23 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6782c') --2018-01-15 19:01:24-- http://images/data_ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_001.jpg [following] --2018-01-15 19:01:24-- https://images/data_ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:24 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6782d') --2018-01-15 19:01:24-- http://images/data_ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_003.jpg [following] --2018-01-15 19:01:24-- https://images/data_ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:24 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6782e') --2018-01-15 19:01:24-- http://images/data_ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_002.jpg [following] --2018-01-15 19:01:24-- https://images/data_ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:24 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6782f') --2018-01-15 19:01:24-- http://images/data_ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_004.jpg [following] --2018-01-15 19:01:24-- https://images/data_ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:24 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67830') --2018-01-15 19:01:24-- http://images/data_ds_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_006.jpg [following] --2018-01-15 19:01:24-- https://images/data_ds_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:24 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67831') --2018-01-15 19:01:24-- http://images/data_ds_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_007.jpg [following] --2018-01-15 19:01:24-- https://images/data_ds_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:24 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67832') --2018-01-15 19:01:25-- http://images/data_ds_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_005.jpg [following] --2018-01-15 19:01:25-- https://images/data_ds_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:25 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67833') --2018-01-15 19:01:25-- http://images/data_ds_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_008.jpg [following] --2018-01-15 19:01:25-- https://images/data_ds_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:25 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67834') --2018-01-15 19:01:25-- http://images/data_ds_010.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_010.jpg [following] --2018-01-15 19:01:25-- https://images/data_ds_010.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:25 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67835') --2018-01-15 19:01:25-- http://images/data_ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_009.jpg [following] --2018-01-15 19:01:25-- https://images/data_ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:25 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67836') --2018-01-15 19:01:25-- http://images/data_switch_001.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_001.png [following] --2018-01-15 19:01:25-- https://images/data_switch_001.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:25 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67837') --2018-01-15 19:01:26-- http://images/data_switch_002.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_002.png [following] --2018-01-15 19:01:26-- https://images/data_switch_002.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:26 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67838') --2018-01-15 19:01:26-- http://images/data_switch_005.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_005.png [following] --2018-01-15 19:01:26-- https://images/data_switch_005.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:26 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67839') --2018-01-15 19:01:27-- http://images/data_switch_003.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_003.png [following] --2018-01-15 19:01:27-- https://images/data_switch_003.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:27 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6783a') --2018-01-15 19:01:27-- http://images/data_switch_004.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_004.png [following] --2018-01-15 19:01:27-- https://images/data_switch_004.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:27 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6783b') --2018-01-15 19:01:27-- http://images/data_wiiu_mariokart8.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mariokart8.png [following] --2018-01-15 19:01:27-- https://images/data_wiiu_mariokart8.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:27 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6783c') --2018-01-15 19:01:27-- http://images/data_wiiu_newmariou.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newmariou.png [following] --2018-01-15 19:01:27-- https://images/data_wiiu_newmariou.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:27 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6783d') --2018-01-15 19:01:27-- http://images/data_wiiu_mario3dworld.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mario3dworld.png [following] --2018-01-15 19:01:27-- https://images/data_wiiu_mario3dworld.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:27 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6783e') --2018-01-15 19:01:28-- http://images/data_wiiu_smashbros.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_smashbros.png [following] --2018-01-15 19:01:28-- https://images/data_wiiu_smashbros.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:28 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6783f') --2018-01-15 19:01:28-- http://images/data_wiiu_nintendoland.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_nintendoland.png [following] --2018-01-15 19:01:28-- https://images/data_wiiu_nintendoland.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:28 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67840') --2018-01-15 19:01:28-- http://images/data_wiiu_splatoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_splatoon.jpg [following] --2018-01-15 19:01:28-- https://images/data_wiiu_splatoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:28 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67841') --2018-01-15 19:01:28-- http://images/data_wiiu_supermariomaker.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_supermariomaker.png [following] --2018-01-15 19:01:28-- https://images/data_wiiu_supermariomaker.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:28 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67842') --2018-01-15 19:01:28-- http://images/data_wiiu_newsuperluigiu.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newsuperluigiu.png [following] --2018-01-15 19:01:28-- https://images/data_wiiu_newsuperluigiu.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:28 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67843') --2018-01-15 19:01:28-- http://images/data_wiiu_zeldahd.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_zeldahd.png [following] --2018-01-15 19:01:28-- https://images/data_wiiu_zeldahd.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:28 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67844') --2018-01-15 19:01:29-- http://images/data_wiiu_marioparty10.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_marioparty10.jpg [following] --2018-01-15 19:01:29-- https://images/data_wiiu_marioparty10.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:29 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67845') --2018-01-15 19:01:29-- http://images/data_3ds_pokemonxy.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonxy.jpg [following] --2018-01-15 19:01:29-- https://images/data_3ds_pokemonxy.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:29 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67846') --2018-01-15 19:01:29-- http://images/data_3ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_002.jpg [following] --2018-01-15 19:01:29-- https://images/data_3ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:29 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67847') --2018-01-15 19:01:29-- http://images/data_3ds_pokemonsunmoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonsunmoon.jpg [following] --2018-01-15 19:01:29-- https://images/data_3ds_pokemonsunmoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:29 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67848') --2018-01-15 19:01:30-- http://images/data_3ds_pokemonoras.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonoras.jpg [following] --2018-01-15 19:01:30-- https://images/data_3ds_pokemonoras.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:30 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67849') --2018-01-15 19:01:30-- http://images/data_3ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_003.jpg [following] --2018-01-15 19:01:30-- https://images/data_3ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:30 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6784a') --2018-01-15 19:01:30-- http://images/data_3ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_001.jpg [following] --2018-01-15 19:01:30-- https://images/data_3ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:30 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6784b') --2018-01-15 19:01:30-- http://images/data_3ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_004.jpg [following] --2018-01-15 19:01:30-- https://images/data_3ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:30 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6784c') --2018-01-15 19:01:30-- http://images/data_3ds_smashbros.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_smashbros.jpg [following] --2018-01-15 19:01:30-- https://images/data_3ds_smashbros.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:30 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6784d') --2018-01-15 19:01:30-- http://images/data_3ds_tomodachicollection.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_tomodachicollection.jpg [following] --2018-01-15 19:01:30-- https://images/data_3ds_tomodachicollection.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:30 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6784e') --2018-01-15 19:01:31-- http://images/data_3ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_009.jpg [following] --2018-01-15 19:01:31-- https://images/data_3ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:31 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6784f') --2018-01-15 19:01:31-- http://images/data_wii_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_001.jpg [following] --2018-01-15 19:01:31-- https://images/data_wii_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:31 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67850') --2018-01-15 19:01:31-- http://images/data_wii_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_002.jpg [following] --2018-01-15 19:01:31-- https://images/data_wii_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:31 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67851') --2018-01-15 19:01:32-- http://images/data_wii_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_003.jpg [following] --2018-01-15 19:01:32-- https://images/data_wii_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:32 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67852') --2018-01-15 19:01:32-- http://images/data_wii_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_005.jpg [following] --2018-01-15 19:01:32-- https://images/data_wii_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:32 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67853') --2018-01-15 19:01:32-- http://images/data_wii_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_004.jpg [following] --2018-01-15 19:01:32-- https://images/data_wii_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:32 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67854') --2018-01-15 19:01:32-- http://images/data_wii_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_006.jpg [following] --2018-01-15 19:01:32-- https://images/data_wii_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:32 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67855') --2018-01-15 19:01:32-- http://images/data_wii_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_007.jpg [following] --2018-01-15 19:01:32-- https://images/data_wii_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:32 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67856') --2018-01-15 19:01:33-- http://images/data_wii_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_009.jpg [following] --2018-01-15 19:01:33-- https://images/data_wii_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:33 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67857') --2018-01-15 19:01:33-- http://images/data_wii_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_008.jpg [following] --2018-01-15 19:01:33-- https://images/data_wii_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:33 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67858') --2018-01-15 19:01:33-- http://images/data_wii_wiiparty.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_wiiparty.jpg [following] --2018-01-15 19:01:33-- https://images/data_wii_wiiparty.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:33 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67859') --2018-01-15 19:01:33-- http://images/data_ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_001.jpg [following] --2018-01-15 19:01:33-- https://images/data_ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:33 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6785a') --2018-01-15 19:01:34-- http://images/data_ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_003.jpg [following] --2018-01-15 19:01:34-- https://images/data_ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:34 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6785b') --2018-01-15 19:01:34-- http://images/data_ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_002.jpg [following] --2018-01-15 19:01:34-- https://images/data_ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:34 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6785c') --2018-01-15 19:01:34-- http://images/data_ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_004.jpg [following] --2018-01-15 19:01:34-- https://images/data_ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:34 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6785d') --2018-01-15 19:01:34-- http://images/data_ds_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_006.jpg [following] --2018-01-15 19:01:34-- https://images/data_ds_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:34 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6785e') --2018-01-15 19:01:35-- http://images/data_ds_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_007.jpg [following] --2018-01-15 19:01:35-- https://images/data_ds_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:35 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6785f') --2018-01-15 19:01:35-- http://images/data_ds_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_005.jpg [following] --2018-01-15 19:01:35-- https://images/data_ds_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:35 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67860') --2018-01-15 19:01:35-- http://images/data_ds_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_008.jpg [following] --2018-01-15 19:01:36-- https://images/data_ds_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:36 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67861') --2018-01-15 19:01:36-- http://images/data_ds_010.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_010.jpg [following] --2018-01-15 19:01:36-- https://images/data_ds_010.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:36 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67862') --2018-01-15 19:01:36-- http://images/data_ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_009.jpg [following] --2018-01-15 19:01:36-- https://images/data_ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:36 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67863') --2018-01-15 19:01:36-- http://images/data_switch_001.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_001.png [following] --2018-01-15 19:01:36-- https://images/data_switch_001.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:36 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67864') --2018-01-15 19:01:36-- http://images/data_switch_002.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_002.png [following] --2018-01-15 19:01:36-- https://images/data_switch_002.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:36 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67865') --2018-01-15 19:01:36-- http://images/data_switch_005.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_005.png [following] --2018-01-15 19:01:36-- https://images/data_switch_005.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:36 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67866') --2018-01-15 19:01:37-- http://images/data_switch_003.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_003.png [following] --2018-01-15 19:01:37-- https://images/data_switch_003.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:37 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67867') --2018-01-15 19:01:37-- http://images/data_switch_004.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_004.png [following] --2018-01-15 19:01:37-- https://images/data_switch_004.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:37 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67868') --2018-01-15 19:01:37-- http://images/data_wiiu_mariokart8.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mariokart8.png [following] --2018-01-15 19:01:37-- https://images/data_wiiu_mariokart8.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:37 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67869') --2018-01-15 19:01:37-- http://images/data_wiiu_newmariou.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newmariou.png [following] --2018-01-15 19:01:37-- https://images/data_wiiu_newmariou.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:37 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6786a') --2018-01-15 19:01:37-- http://images/data_wiiu_mario3dworld.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mario3dworld.png [following] --2018-01-15 19:01:37-- https://images/data_wiiu_mario3dworld.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:37 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6786b') --2018-01-15 19:01:37-- http://images/data_wiiu_smashbros.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_smashbros.png [following] --2018-01-15 19:01:37-- https://images/data_wiiu_smashbros.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:37 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6786c') --2018-01-15 19:01:38-- http://images/data_wiiu_nintendoland.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_nintendoland.png [following] --2018-01-15 19:01:38-- https://images/data_wiiu_nintendoland.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:38 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6786d') --2018-01-15 19:01:38-- http://images/data_wiiu_splatoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_splatoon.jpg [following] --2018-01-15 19:01:38-- https://images/data_wiiu_splatoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:38 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6786e') --2018-01-15 19:01:38-- http://images/data_wiiu_supermariomaker.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_supermariomaker.png [following] --2018-01-15 19:01:38-- https://images/data_wiiu_supermariomaker.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:38 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6786f') --2018-01-15 19:01:38-- http://images/data_wiiu_newsuperluigiu.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newsuperluigiu.png [following] --2018-01-15 19:01:38-- https://images/data_wiiu_newsuperluigiu.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:38 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67870') --2018-01-15 19:01:38-- http://images/data_wiiu_zeldahd.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_zeldahd.png [following] --2018-01-15 19:01:38-- https://images/data_wiiu_zeldahd.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:38 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67871') --2018-01-15 19:01:39-- http://images/data_wiiu_marioparty10.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_marioparty10.jpg [following] --2018-01-15 19:01:39-- https://images/data_wiiu_marioparty10.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:39 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67872') --2018-01-15 19:01:39-- http://images/data_3ds_pokemonxy.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonxy.jpg [following] --2018-01-15 19:01:39-- https://images/data_3ds_pokemonxy.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:39 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67873') --2018-01-15 19:01:40-- http://images/data_3ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_002.jpg [following] --2018-01-15 19:01:40-- https://images/data_3ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:40 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67874') --2018-01-15 19:01:40-- http://images/data_3ds_pokemonsunmoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonsunmoon.jpg [following] --2018-01-15 19:01:40-- https://images/data_3ds_pokemonsunmoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:40 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67875') --2018-01-15 19:01:40-- http://images/data_3ds_pokemonoras.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonoras.jpg [following] --2018-01-15 19:01:40-- https://images/data_3ds_pokemonoras.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:40 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67876') --2018-01-15 19:01:40-- http://images/data_3ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_003.jpg [following] --2018-01-15 19:01:40-- https://images/data_3ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:40 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67877') --2018-01-15 19:01:40-- http://images/data_3ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_001.jpg [following] --2018-01-15 19:01:40-- https://images/data_3ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:40 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67878') --2018-01-15 19:01:41-- http://images/data_3ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_004.jpg [following] --2018-01-15 19:01:41-- https://images/data_3ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:41 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67879') --2018-01-15 19:01:41-- http://images/data_3ds_smashbros.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_smashbros.jpg [following] --2018-01-15 19:01:41-- https://images/data_3ds_smashbros.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:41 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6787a') --2018-01-15 19:01:41-- http://images/data_3ds_tomodachicollection.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_tomodachicollection.jpg [following] --2018-01-15 19:01:41-- https://images/data_3ds_tomodachicollection.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:41 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6787b') --2018-01-15 19:01:42-- http://images/data_3ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_009.jpg [following] --2018-01-15 19:01:42-- https://images/data_3ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:42 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6787c') --2018-01-15 19:01:42-- http://images/data_wii_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_001.jpg [following] --2018-01-15 19:01:42-- https://images/data_wii_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:42 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6787d') --2018-01-15 19:01:42-- http://images/data_wii_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_002.jpg [following] --2018-01-15 19:01:42-- https://images/data_wii_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:42 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6787e') --2018-01-15 19:01:42-- http://images/data_wii_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_003.jpg [following] --2018-01-15 19:01:42-- https://images/data_wii_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:42 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6787f') --2018-01-15 19:01:42-- http://images/data_wii_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_005.jpg [following] --2018-01-15 19:01:42-- https://images/data_wii_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:42 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67880') --2018-01-15 19:01:42-- http://images/data_wii_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_004.jpg [following] --2018-01-15 19:01:42-- https://images/data_wii_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:42 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67881') --2018-01-15 19:01:43-- http://images/data_wii_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_006.jpg [following] --2018-01-15 19:01:43-- https://images/data_wii_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:43 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67882') --2018-01-15 19:01:43-- http://images/data_wii_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_007.jpg [following] --2018-01-15 19:01:43-- https://images/data_wii_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:43 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67883') --2018-01-15 19:01:43-- http://images/data_wii_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_009.jpg [following] --2018-01-15 19:01:43-- https://images/data_wii_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:43 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67884') --2018-01-15 19:01:43-- http://images/data_wii_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_008.jpg [following] --2018-01-15 19:01:43-- https://images/data_wii_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:43 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67885') --2018-01-15 19:01:44-- http://images/data_wii_wiiparty.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_wiiparty.jpg [following] --2018-01-15 19:01:44-- https://images/data_wii_wiiparty.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:44 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67886') --2018-01-15 19:01:44-- http://images/data_ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_001.jpg [following] --2018-01-15 19:01:44-- https://images/data_ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:44 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67887') --2018-01-15 19:01:44-- http://images/data_ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_003.jpg [following] --2018-01-15 19:01:44-- https://images/data_ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:44 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67888') --2018-01-15 19:01:44-- http://images/data_ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_002.jpg [following] --2018-01-15 19:01:44-- https://images/data_ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:44 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67889') --2018-01-15 19:01:45-- http://images/data_ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_004.jpg [following] --2018-01-15 19:01:45-- https://images/data_ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:45 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6788a') --2018-01-15 19:01:45-- http://images/data_ds_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_006.jpg [following] --2018-01-15 19:01:45-- https://images/data_ds_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:45 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6788b') --2018-01-15 19:01:45-- http://images/data_ds_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_007.jpg [following] --2018-01-15 19:01:45-- https://images/data_ds_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:45 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6788c') --2018-01-15 19:01:45-- http://images/data_ds_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_005.jpg [following] --2018-01-15 19:01:45-- https://images/data_ds_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:45 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6788d') --2018-01-15 19:01:45-- http://images/data_ds_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_008.jpg [following] --2018-01-15 19:01:45-- https://images/data_ds_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:45 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6788e') --2018-01-15 19:01:45-- http://images/data_ds_010.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_010.jpg [following] --2018-01-15 19:01:45-- https://images/data_ds_010.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:45 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6788f') --2018-01-15 19:01:46-- http://images/data_ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_009.jpg [following] --2018-01-15 19:01:46-- https://images/data_ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:46 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67890') --2018-01-15 19:01:46-- http://images/data_switch_001.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_001.png [following] --2018-01-15 19:01:46-- https://images/data_switch_001.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:46 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67891') --2018-01-15 19:01:46-- http://images/data_switch_002.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_002.png [following] --2018-01-15 19:01:46-- https://images/data_switch_002.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:46 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67892') --2018-01-15 19:01:46-- http://images/data_switch_005.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_005.png [following] --2018-01-15 19:01:46-- https://images/data_switch_005.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:46 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67893') --2018-01-15 19:01:46-- http://images/data_switch_003.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_003.png [following] --2018-01-15 19:01:46-- https://images/data_switch_003.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:46 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67894') --2018-01-15 19:01:46-- http://images/data_switch_004.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_004.png [following] --2018-01-15 19:01:47-- https://images/data_switch_004.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:47 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67895') --2018-01-15 19:01:47-- http://images/data_wiiu_mariokart8.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mariokart8.png [following] --2018-01-15 19:01:47-- https://images/data_wiiu_mariokart8.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:47 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67896') --2018-01-15 19:01:47-- http://images/data_wiiu_newmariou.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newmariou.png [following] --2018-01-15 19:01:47-- https://images/data_wiiu_newmariou.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:47 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67897') --2018-01-15 19:01:47-- http://images/data_wiiu_mario3dworld.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mario3dworld.png [following] --2018-01-15 19:01:47-- https://images/data_wiiu_mario3dworld.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:47 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67898') --2018-01-15 19:01:47-- http://images/data_wiiu_smashbros.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_smashbros.png [following] --2018-01-15 19:01:47-- https://images/data_wiiu_smashbros.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:47 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d67899') --2018-01-15 19:01:48-- http://images/data_wiiu_nintendoland.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_nintendoland.png [following] --2018-01-15 19:01:48-- https://images/data_wiiu_nintendoland.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:48 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6789a') --2018-01-15 19:01:48-- http://images/data_wiiu_splatoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_splatoon.jpg [following] --2018-01-15 19:01:48-- https://images/data_wiiu_splatoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:48 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6789b') --2018-01-15 19:01:48-- http://images/data_wiiu_supermariomaker.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_supermariomaker.png [following] --2018-01-15 19:01:48-- https://images/data_wiiu_supermariomaker.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:48 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6789c') --2018-01-15 19:01:48-- http://images/data_wiiu_newsuperluigiu.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newsuperluigiu.png [following] --2018-01-15 19:01:48-- https://images/data_wiiu_newsuperluigiu.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:48 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6789d') --2018-01-15 19:01:48-- http://images/data_wiiu_zeldahd.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_zeldahd.png [following] --2018-01-15 19:01:48-- https://images/data_wiiu_zeldahd.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:48 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6789e') --2018-01-15 19:01:48-- http://images/data_wiiu_marioparty10.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_marioparty10.jpg [following] --2018-01-15 19:01:48-- https://images/data_wiiu_marioparty10.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:48 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d6789f') --2018-01-15 19:01:49-- http://images/data_3ds_pokemonxy.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonxy.jpg [following] --2018-01-15 19:01:49-- https://images/data_3ds_pokemonxy.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:49 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a0') --2018-01-15 19:01:49-- http://images/data_3ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_002.jpg [following] --2018-01-15 19:01:49-- https://images/data_3ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:49 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a1') --2018-01-15 19:01:49-- http://images/data_3ds_pokemonsunmoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonsunmoon.jpg [following] --2018-01-15 19:01:49-- https://images/data_3ds_pokemonsunmoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:49 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a2') --2018-01-15 19:01:49-- http://images/data_3ds_pokemonoras.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonoras.jpg [following] --2018-01-15 19:01:49-- https://images/data_3ds_pokemonoras.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:49 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a3') --2018-01-15 19:01:49-- http://images/data_3ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_003.jpg [following] --2018-01-15 19:01:49-- https://images/data_3ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:49 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a4') --2018-01-15 19:01:49-- http://images/data_3ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_001.jpg [following] --2018-01-15 19:01:49-- https://images/data_3ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:49 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a5') --2018-01-15 19:01:50-- http://images/data_3ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_004.jpg [following] --2018-01-15 19:01:50-- https://images/data_3ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:50 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a6') --2018-01-15 19:01:50-- http://images/data_3ds_smashbros.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_smashbros.jpg [following] --2018-01-15 19:01:50-- https://images/data_3ds_smashbros.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:50 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a7') --2018-01-15 19:01:50-- http://images/data_3ds_tomodachicollection.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_tomodachicollection.jpg [following] --2018-01-15 19:01:50-- https://images/data_3ds_tomodachicollection.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:50 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a8') --2018-01-15 19:01:50-- http://images/data_3ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_009.jpg [following] --2018-01-15 19:01:50-- https://images/data_3ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:50 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678a9') --2018-01-15 19:01:50-- http://images/data_wii_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_001.jpg [following] --2018-01-15 19:01:50-- https://images/data_wii_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:50 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678aa') --2018-01-15 19:01:51-- http://images/data_wii_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_002.jpg [following] --2018-01-15 19:01:51-- https://images/data_wii_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:51 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ab') --2018-01-15 19:01:51-- http://images/data_wii_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_003.jpg [following] --2018-01-15 19:01:51-- https://images/data_wii_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:51 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ac') --2018-01-15 19:01:52-- http://images/data_wii_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_005.jpg [following] --2018-01-15 19:01:52-- https://images/data_wii_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:52 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ad') --2018-01-15 19:01:52-- http://images/data_wii_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_004.jpg [following] --2018-01-15 19:01:52-- https://images/data_wii_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:52 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ae') --2018-01-15 19:01:52-- http://images/data_wii_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_006.jpg [following] --2018-01-15 19:01:52-- https://images/data_wii_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:52 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678af') --2018-01-15 19:01:52-- http://images/data_wii_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_007.jpg [following] --2018-01-15 19:01:52-- https://images/data_wii_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:52 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b0') --2018-01-15 19:01:53-- http://images/data_wii_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_009.jpg [following] --2018-01-15 19:01:53-- https://images/data_wii_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:53 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b1') --2018-01-15 19:01:53-- http://images/data_wii_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_008.jpg [following] --2018-01-15 19:01:53-- https://images/data_wii_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:53 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b2') --2018-01-15 19:01:53-- http://images/data_wii_wiiparty.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_wiiparty.jpg [following] --2018-01-15 19:01:53-- https://images/data_wii_wiiparty.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:53 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b3') --2018-01-15 19:01:53-- http://images/data_ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_001.jpg [following] --2018-01-15 19:01:53-- https://images/data_ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:53 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b4') --2018-01-15 19:01:53-- http://images/data_ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_003.jpg [following] --2018-01-15 19:01:53-- https://images/data_ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:53 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b5') --2018-01-15 19:01:54-- http://images/data_ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_002.jpg [following] --2018-01-15 19:01:54-- https://images/data_ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:54 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b6') --2018-01-15 19:01:54-- http://images/data_ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_004.jpg [following] --2018-01-15 19:01:54-- https://images/data_ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:54 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b7') --2018-01-15 19:01:54-- http://images/data_ds_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_006.jpg [following] --2018-01-15 19:01:54-- https://images/data_ds_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:54 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b8') --2018-01-15 19:01:54-- http://images/data_ds_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_007.jpg [following] --2018-01-15 19:01:54-- https://images/data_ds_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:54 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678b9') --2018-01-15 19:01:54-- http://images/data_ds_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_005.jpg [following] --2018-01-15 19:01:54-- https://images/data_ds_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:54 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ba') --2018-01-15 19:01:54-- http://images/data_ds_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_008.jpg [following] --2018-01-15 19:01:54-- https://images/data_ds_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:54 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678bb') --2018-01-15 19:01:55-- http://images/data_ds_010.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_010.jpg [following] --2018-01-15 19:01:55-- https://images/data_ds_010.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:55 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678bc') --2018-01-15 19:01:55-- http://images/data_ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_009.jpg [following] --2018-01-15 19:01:55-- https://images/data_ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:55 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678bd') --2018-01-15 19:01:55-- http://images/data_switch_001.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_001.png [following] --2018-01-15 19:01:55-- https://images/data_switch_001.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:55 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678be') --2018-01-15 19:01:55-- http://images/data_switch_002.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_002.png [following] --2018-01-15 19:01:55-- https://images/data_switch_002.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:55 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678bf') --2018-01-15 19:01:55-- http://images/data_switch_005.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_005.png [following] --2018-01-15 19:01:55-- https://images/data_switch_005.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:55 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c0') --2018-01-15 19:01:55-- http://images/data_switch_003.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_003.png [following] --2018-01-15 19:01:55-- https://images/data_switch_003.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:55 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c1') --2018-01-15 19:01:56-- http://images/data_switch_004.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_switch_004.png [following] --2018-01-15 19:01:56-- https://images/data_switch_004.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:56 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c2') --2018-01-15 19:01:56-- http://images/data_wiiu_mariokart8.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mariokart8.png [following] --2018-01-15 19:01:56-- https://images/data_wiiu_mariokart8.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:56 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c3') --2018-01-15 19:01:56-- http://images/data_wiiu_newmariou.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newmariou.png [following] --2018-01-15 19:01:56-- https://images/data_wiiu_newmariou.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:56 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c4') --2018-01-15 19:01:56-- http://images/data_wiiu_mario3dworld.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_mario3dworld.png [following] --2018-01-15 19:01:56-- https://images/data_wiiu_mario3dworld.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:56 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c5') --2018-01-15 19:01:56-- http://images/data_wiiu_smashbros.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_smashbros.png [following] --2018-01-15 19:01:57-- https://images/data_wiiu_smashbros.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:57 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c6') --2018-01-15 19:01:57-- http://images/data_wiiu_nintendoland.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_nintendoland.png [following] --2018-01-15 19:01:57-- https://images/data_wiiu_nintendoland.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:57 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c7') --2018-01-15 19:01:57-- http://images/data_wiiu_splatoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_splatoon.jpg [following] --2018-01-15 19:01:57-- https://images/data_wiiu_splatoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:57 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c8') --2018-01-15 19:01:57-- http://images/data_wiiu_supermariomaker.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_supermariomaker.png [following] --2018-01-15 19:01:57-- https://images/data_wiiu_supermariomaker.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:57 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678c9') --2018-01-15 19:01:58-- http://images/data_wiiu_newsuperluigiu.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_newsuperluigiu.png [following] --2018-01-15 19:01:58-- https://images/data_wiiu_newsuperluigiu.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:58 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ca') --2018-01-15 19:01:58-- http://images/data_wiiu_zeldahd.png Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_zeldahd.png [following] --2018-01-15 19:01:58-- https://images/data_wiiu_zeldahd.png Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:58 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678cb') --2018-01-15 19:01:58-- http://images/data_wiiu_marioparty10.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wiiu_marioparty10.jpg [following] --2018-01-15 19:01:58-- https://images/data_wiiu_marioparty10.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:58 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678cc') --2018-01-15 19:01:59-- http://images/data_ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_001.jpg [following] --2018-01-15 19:01:59-- https://images/data_ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:59 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678cd') --2018-01-15 19:01:59-- http://images/data_ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_003.jpg [following] --2018-01-15 19:01:59-- https://images/data_ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:59 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ce') --2018-01-15 19:01:59-- http://images/data_ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_002.jpg [following] --2018-01-15 19:01:59-- https://images/data_ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:59 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678cf') --2018-01-15 19:01:59-- http://images/data_ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_004.jpg [following] --2018-01-15 19:01:59-- https://images/data_ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:01:59 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d0') --2018-01-15 19:02:00-- http://images/data_ds_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_006.jpg [following] --2018-01-15 19:02:00-- https://images/data_ds_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:00 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d1') --2018-01-15 19:02:00-- http://images/data_ds_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_007.jpg [following] --2018-01-15 19:02:00-- https://images/data_ds_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:00 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d2') --2018-01-15 19:02:00-- http://images/data_ds_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_005.jpg [following] --2018-01-15 19:02:00-- https://images/data_ds_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:00 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d3') --2018-01-15 19:02:00-- http://images/data_ds_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_008.jpg [following] --2018-01-15 19:02:00-- https://images/data_ds_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:00 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d4') --2018-01-15 19:02:00-- http://images/data_ds_010.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_010.jpg [following] --2018-01-15 19:02:00-- https://images/data_ds_010.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:00 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d5') --2018-01-15 19:02:01-- http://images/data_ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_ds_009.jpg [following] --2018-01-15 19:02:01-- https://images/data_ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:01 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d6') --2018-01-15 19:02:01-- http://images/data_3ds_pokemonxy.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonxy.jpg [following] --2018-01-15 19:02:01-- https://images/data_3ds_pokemonxy.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:01 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d7') --2018-01-15 19:02:01-- http://images/data_3ds_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_002.jpg [following] --2018-01-15 19:02:01-- https://images/data_3ds_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:01 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d8') --2018-01-15 19:02:01-- http://images/data_3ds_pokemonsunmoon.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonsunmoon.jpg [following] --2018-01-15 19:02:01-- https://images/data_3ds_pokemonsunmoon.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:01 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678d9') --2018-01-15 19:02:01-- http://images/data_3ds_pokemonoras.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_pokemonoras.jpg [following] --2018-01-15 19:02:01-- https://images/data_3ds_pokemonoras.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:01 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678da') --2018-01-15 19:02:01-- http://images/data_3ds_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_003.jpg [following] --2018-01-15 19:02:01-- https://images/data_3ds_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:01 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678db') --2018-01-15 19:02:02-- http://images/data_3ds_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_001.jpg [following] --2018-01-15 19:02:02-- https://images/data_3ds_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:02 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678dc') --2018-01-15 19:02:02-- http://images/data_3ds_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_004.jpg [following] --2018-01-15 19:02:02-- https://images/data_3ds_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:02 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678dd') --2018-01-15 19:02:02-- http://images/data_3ds_smashbros.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_smashbros.jpg [following] --2018-01-15 19:02:02-- https://images/data_3ds_smashbros.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:02 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678de') --2018-01-15 19:02:02-- http://images/data_3ds_tomodachicollection.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_tomodachicollection.jpg [following] --2018-01-15 19:02:02-- https://images/data_3ds_tomodachicollection.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:02 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678df') --2018-01-15 19:02:02-- http://images/data_3ds_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_3ds_009.jpg [following] --2018-01-15 19:02:03-- https://images/data_3ds_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:03 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e0') --2018-01-15 19:02:03-- http://images/data_wii_001.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_001.jpg [following] --2018-01-15 19:02:03-- https://images/data_wii_001.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:03 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e1') --2018-01-15 19:02:03-- http://images/data_wii_002.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_002.jpg [following] --2018-01-15 19:02:03-- https://images/data_wii_002.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:03 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e2') --2018-01-15 19:02:03-- http://images/data_wii_003.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_003.jpg [following] --2018-01-15 19:02:03-- https://images/data_wii_003.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:03 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e3') --2018-01-15 19:02:03-- http://images/data_wii_005.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_005.jpg [following] --2018-01-15 19:02:03-- https://images/data_wii_005.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:03 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e4') --2018-01-15 19:02:04-- http://images/data_wii_004.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_004.jpg [following] --2018-01-15 19:02:04-- https://images/data_wii_004.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:04 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e5') --2018-01-15 19:02:04-- http://images/data_wii_006.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_006.jpg [following] --2018-01-15 19:02:04-- https://images/data_wii_006.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:04 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e6') --2018-01-15 19:02:04-- http://images/data_wii_007.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_007.jpg [following] --2018-01-15 19:02:04-- https://images/data_wii_007.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:04 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e7') --2018-01-15 19:02:04-- http://images/data_wii_009.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_009.jpg [following] --2018-01-15 19:02:04-- https://images/data_wii_009.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:04 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e8') --2018-01-15 19:02:04-- http://images/data_wii_008.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_008.jpg [following] --2018-01-15 19:02:04-- https://images/data_wii_008.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:04 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678e9') --2018-01-15 19:02:05-- http://images/data_wii_wiiparty.jpg Resolving images (images)... 167.114.148.224 Connecting to images (images)|167.114.148.224|:80... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://images/data_wii_wiiparty.jpg [following] --2018-01-15 19:02:05-- https://images/data_wii_wiiparty.jpg Connecting to images (images)|167.114.148.224|:443... connected. WARNING: no certificate subject alternative name matches requested host name 'images'. HTTP request sent, awaiting response... 502 Bad Gateway 2018-01-15 19:02:05 ERROR 502: Bad Gateway. Out[23]: ObjectId('5a5d4dd96d99ba7541d678ea') --2018-01-15 19:02:05-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_switch_001.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 123250 (120K) [image/png] Saving to: 'images/data_switch_001.png' data_switch_001.png 100%[===================>] 120.36K --.-KB/s in 0.02s 2018-01-15 19:02:05 (5.85 MB/s) - 'images/data_switch_001.png' saved [123250/123250] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d00') --2018-01-15 19:02:05-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_switch_002.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 136768 (134K) [image/png] Saving to: 'images/data_switch_002.png' data_switch_002.png 100%[===================>] 133.56K --.-KB/s in 0.02s 2018-01-15 19:02:06 (5.42 MB/s) - 'images/data_switch_002.png' saved [136768/136768] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d01') --2018-01-15 19:02:06-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_switch_005.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 116191 (113K) [image/png] Saving to: 'images/data_switch_005.png' data_switch_005.png 100%[===================>] 113.47K --.-KB/s in 0.02s 2018-01-15 19:02:06 (5.30 MB/s) - 'images/data_switch_005.png' saved [116191/116191] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d02') --2018-01-15 19:02:06-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_switch_003.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 97341 (95K) [image/png] Saving to: 'images/data_switch_003.png' data_switch_003.png 100%[===================>] 95.06K --.-KB/s in 0.02s 2018-01-15 19:02:07 (4.36 MB/s) - 'images/data_switch_003.png' saved [97341/97341] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d03') --2018-01-15 19:02:07-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_switch_004.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 113103 (110K) [image/png] Saving to: 'images/data_switch_004.png' data_switch_004.png 100%[===================>] 110.45K --.-KB/s in 0.02s 2018-01-15 19:02:08 (5.34 MB/s) - 'images/data_switch_004.png' saved [113103/113103] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d04') --2018-01-15 19:02:08-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_mariokart8.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 41726 (41K) [image/png] Saving to: 'images/data_wiiu_mariokart8.png' data_wiiu_mariokart 100%[===================>] 40.75K --.-KB/s in 0.01s 2018-01-15 19:02:08 (3.78 MB/s) - 'images/data_wiiu_mariokart8.png' saved [41726/41726] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d05') --2018-01-15 19:02:08-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_newmariou.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 39007 (38K) [image/png] Saving to: 'images/data_wiiu_newmariou.png' data_wiiu_newmariou 100%[===================>] 38.09K --.-KB/s in 0.01s 2018-01-15 19:02:08 (3.79 MB/s) - 'images/data_wiiu_newmariou.png' saved [39007/39007] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d06') --2018-01-15 19:02:09-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_mario3dworld.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 40182 (39K) [image/png] Saving to: 'images/data_wiiu_mario3dworld.png' data_wiiu_mario3dwo 100%[===================>] 39.24K --.-KB/s in 0.01s 2018-01-15 19:02:09 (3.92 MB/s) - 'images/data_wiiu_mario3dworld.png' saved [40182/40182] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d07') --2018-01-15 19:02:09-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_smashbros.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 42978 (42K) [image/png] Saving to: 'images/data_wiiu_smashbros.png' data_wiiu_smashbros 100%[===================>] 41.97K --.-KB/s in 0.01s 2018-01-15 19:02:10 (4.12 MB/s) - 'images/data_wiiu_smashbros.png' saved [42978/42978] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d08') --2018-01-15 19:02:10-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_nintendoland.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 37299 (36K) [image/png] Saving to: 'images/data_wiiu_nintendoland.png' data_wiiu_nintendol 100%[===================>] 36.42K --.-KB/s in 0.01s 2018-01-15 19:02:10 (3.62 MB/s) - 'images/data_wiiu_nintendoland.png' saved [37299/37299] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d09') --2018-01-15 19:02:10-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_splatoon.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 28908 (28K) [image/jpeg] Saving to: 'images/data_wiiu_splatoon.jpg' data_wiiu_splatoon. 100%[===================>] 28.23K --.-KB/s in 0.01s 2018-01-15 19:02:11 (2.32 MB/s) - 'images/data_wiiu_splatoon.jpg' saved [28908/28908] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d0a') --2018-01-15 19:02:11-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_supermariomaker.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 36527 (36K) [image/png] Saving to: 'images/data_wiiu_supermariomaker.png' data_wiiu_supermari 100%[===================>] 35.67K --.-KB/s in 0.01s 2018-01-15 19:02:11 (3.59 MB/s) - 'images/data_wiiu_supermariomaker.png' saved [36527/36527] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d0b') --2018-01-15 19:02:11-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_newsuperluigiu.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 39589 (39K) [image/png] Saving to: 'images/data_wiiu_newsuperluigiu.png' data_wiiu_newsuperl 100%[===================>] 38.66K --.-KB/s in 0.01s 2018-01-15 19:02:12 (3.82 MB/s) - 'images/data_wiiu_newsuperluigiu.png' saved [39589/39589] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d0c') --2018-01-15 19:02:12-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_zeldahd.png Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 44785 (44K) [image/png] Saving to: 'images/data_wiiu_zeldahd.png' data_wiiu_zeldahd.p 100%[===================>] 43.74K --.-KB/s in 0.01s 2018-01-15 19:02:12 (4.33 MB/s) - 'images/data_wiiu_zeldahd.png' saved [44785/44785] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d0d') --2018-01-15 19:02:12-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wiiu_marioparty10.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 20514 (20K) [image/jpeg] Saving to: 'images/data_wiiu_marioparty10.jpg' data_wiiu_mariopart 100%[===================>] 20.03K --.-KB/s in 0s 2018-01-15 19:02:13 (296 MB/s) - 'images/data_wiiu_marioparty10.jpg' saved [20514/20514] Out[23]: ObjectId('5a5d4eda6d99ba784beb0d0e') --2018-01-15 19:02:13-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_pokemonxy.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 32213 (31K) [image/jpeg] Saving to: 'images/data_3ds_pokemonxy.jpg' data_3ds_pokemonxy. 100%[===================>] 31.46K --.-KB/s in 0.02s 2018-01-15 19:02:13 (1.53 MB/s) - 'images/data_3ds_pokemonxy.jpg' saved [32213/32213] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d0f') --2018-01-15 19:02:13-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_002.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 12917 (13K) [image/jpeg] Saving to: 'images/data_3ds_002.jpg' data_3ds_002.jpg 100%[===================>] 12.61K --.-KB/s in 0s 2018-01-15 19:02:13 (358 MB/s) - 'images/data_3ds_002.jpg' saved [12917/12917] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d10') --2018-01-15 19:02:14-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_pokemonsunmoon.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 37022 (36K) [image/jpeg] Saving to: 'images/data_3ds_pokemonsunmoon.jpg' data_3ds_pokemonsun 100%[===================>] 36.15K --.-KB/s in 0.02s 2018-01-15 19:02:14 (1.76 MB/s) - 'images/data_3ds_pokemonsunmoon.jpg' saved [37022/37022] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d11') --2018-01-15 19:02:14-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_pokemonoras.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 20879 (20K) [image/jpeg] Saving to: 'images/data_3ds_pokemonoras.jpg' data_3ds_pokemonora 100%[===================>] 20.39K --.-KB/s in 0s 2018-01-15 19:02:14 (191 MB/s) - 'images/data_3ds_pokemonoras.jpg' saved [20879/20879] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d12') --2018-01-15 19:02:15-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_003.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10823 (11K) [image/jpeg] Saving to: 'images/data_3ds_003.jpg' data_3ds_003.jpg 100%[===================>] 10.57K --.-KB/s in 0s 2018-01-15 19:02:15 (214 MB/s) - 'images/data_3ds_003.jpg' saved [10823/10823] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d13') --2018-01-15 19:02:15-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_001.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 16348 (16K) [image/jpeg] Saving to: 'images/data_3ds_001.jpg' data_3ds_001.jpg 100%[===================>] 15.96K --.-KB/s in 0s 2018-01-15 19:02:16 (246 MB/s) - 'images/data_3ds_001.jpg' saved [16348/16348] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d14') --2018-01-15 19:02:16-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_004.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 11078 (11K) [image/jpeg] Saving to: 'images/data_3ds_004.jpg' data_3ds_004.jpg 100%[===================>] 10.82K --.-KB/s in 0s 2018-01-15 19:02:16 (276 MB/s) - 'images/data_3ds_004.jpg' saved [11078/11078] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d15') --2018-01-15 19:02:16-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_smashbros.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 11635 (11K) [image/jpeg] Saving to: 'images/data_3ds_smashbros.jpg' data_3ds_smashbros. 100%[===================>] 11.36K --.-KB/s in 0s 2018-01-15 19:02:17 (252 MB/s) - 'images/data_3ds_smashbros.jpg' saved [11635/11635] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d16') --2018-01-15 19:02:17-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_tomodachicollection.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 23405 (23K) [image/jpeg] Saving to: 'images/data_3ds_tomodachicollection.jpg' data_3ds_tomodachic 100%[===================>] 22.86K --.-KB/s in 0.01s 2018-01-15 19:02:17 (2.27 MB/s) - 'images/data_3ds_tomodachicollection.jpg' saved [23405/23405] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d17') --2018-01-15 19:02:17-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_3ds_009.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8640 (8.4K) [image/jpeg] Saving to: 'images/data_3ds_009.jpg' data_3ds_009.jpg 100%[===================>] 8.44K --.-KB/s in 0s 2018-01-15 19:02:18 (241 MB/s) - 'images/data_3ds_009.jpg' saved [8640/8640] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d18') --2018-01-15 19:02:18-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_001.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 5365 (5.2K) [image/jpeg] Saving to: 'images/data_wii_001.jpg' data_wii_001.jpg 100%[===================>] 5.24K --.-KB/s in 0s 2018-01-15 19:02:18 (839 MB/s) - 'images/data_wii_001.jpg' saved [5365/5365] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d19') --2018-01-15 19:02:18-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_002.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 6852 (6.7K) [image/jpeg] Saving to: 'images/data_wii_002.jpg' data_wii_002.jpg 100%[===================>] 6.69K --.-KB/s in 0s 2018-01-15 19:02:18 (992 MB/s) - 'images/data_wii_002.jpg' saved [6852/6852] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d1a') --2018-01-15 19:02:19-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_003.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 5926 (5.8K) [image/jpeg] Saving to: 'images/data_wii_003.jpg' data_wii_003.jpg 100%[===================>] 5.79K --.-KB/s in 0s 2018-01-15 19:02:19 (885 MB/s) - 'images/data_wii_003.jpg' saved [5926/5926] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d1b') --2018-01-15 19:02:19-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_005.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8798 (8.6K) [image/jpeg] Saving to: 'images/data_wii_005.jpg' data_wii_005.jpg 100%[===================>] 8.59K --.-KB/s in 0s 2018-01-15 19:02:19 (226 MB/s) - 'images/data_wii_005.jpg' saved [8798/8798] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d1c') --2018-01-15 19:02:20-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_004.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4984 (4.9K) [image/jpeg] Saving to: 'images/data_wii_004.jpg' data_wii_004.jpg 100%[===================>] 4.87K --.-KB/s in 0s 2018-01-15 19:02:20 (716 MB/s) - 'images/data_wii_004.jpg' saved [4984/4984] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d1d') --2018-01-15 19:02:20-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_006.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4221 (4.1K) [image/jpeg] Saving to: 'images/data_wii_006.jpg' data_wii_006.jpg 100%[===================>] 4.12K --.-KB/s in 0s 2018-01-15 19:02:21 (706 MB/s) - 'images/data_wii_006.jpg' saved [4221/4221] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d1e') --2018-01-15 19:02:21-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_007.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 6807 (6.6K) [image/jpeg] Saving to: 'images/data_wii_007.jpg' data_wii_007.jpg 100%[===================>] 6.65K --.-KB/s in 0s 2018-01-15 19:02:21 (958 MB/s) - 'images/data_wii_007.jpg' saved [6807/6807] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d1f') --2018-01-15 19:02:21-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_009.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8262 (8.1K) [image/jpeg] Saving to: 'images/data_wii_009.jpg' data_wii_009.jpg 100%[===================>] 8.07K --.-KB/s in 0s 2018-01-15 19:02:21 (235 MB/s) - 'images/data_wii_009.jpg' saved [8262/8262] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d20') --2018-01-15 19:02:22-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_008.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8344 (8.1K) [image/jpeg] Saving to: 'images/data_wii_008.jpg' data_wii_008.jpg 100%[===================>] 8.15K --.-KB/s in 0s 2018-01-15 19:02:22 (137 MB/s) - 'images/data_wii_008.jpg' saved [8344/8344] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d21') --2018-01-15 19:02:22-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_wii_wiiparty.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 15776 (15K) [image/jpeg] Saving to: 'images/data_wii_wiiparty.jpg' data_wii_wiiparty.j 100%[===================>] 15.41K --.-KB/s in 0s 2018-01-15 19:02:22 (430 MB/s) - 'images/data_wii_wiiparty.jpg' saved [15776/15776] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d22') --2018-01-15 19:02:23-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_ds_001.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 11788 (12K) [image/jpeg] Saving to: 'images/data_ds_001.jpg' data_ds_001.jpg 100%[===================>] 11.51K --.-KB/s in 0s 2018-01-15 19:02:23 (321 MB/s) - 'images/data_ds_001.jpg' saved [11788/11788] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d23') --2018-01-15 19:02:23-- https://www.nintendo.co.jp/ir/en/finance/softwaremg/data_ds_003.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 404 Not Found 2018-01-15 19:02:23 ERROR 404: Not Found. Out[23]: ObjectId('5a5d4edb6d99ba784beb0d24') --2018-01-15 19:02:23-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_ds_002.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 8290 (8.1K) [image/jpeg] Saving to: 'images/data_ds_002.jpg' data_ds_002.jpg 100%[===================>] 8.10K --.-KB/s in 0s 2018-01-15 19:02:24 (221 MB/s) - 'images/data_ds_002.jpg' saved [8290/8290] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d25') --2018-01-15 19:02:24-- https://www.nintendo.co.jp/ir/en/finance/softwaremg/data_ds_004.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 404 Not Found 2018-01-15 19:02:24 ERROR 404: Not Found. Out[23]: ObjectId('5a5d4edb6d99ba784beb0d26') --2018-01-15 19:02:25-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_ds_006.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 16311 (16K) [image/jpeg] Saving to: 'images/data_ds_006.jpg' data_ds_006.jpg 100%[===================>] 15.93K --.-KB/s in 0s 2018-01-15 19:02:25 (246 MB/s) - 'images/data_ds_006.jpg' saved [16311/16311] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d27') --2018-01-15 19:02:26-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_ds_007.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 14806 (14K) [image/jpeg] Saving to: 'images/data_ds_007.jpg' data_ds_007.jpg 100%[===================>] 14.46K --.-KB/s in 0s 2018-01-15 19:02:26 (283 MB/s) - 'images/data_ds_007.jpg' saved [14806/14806] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d28') --2018-01-15 19:02:26-- https://www.nintendo.co.jp/ir/en/finance/softwaremg/data_ds_005.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 404 Not Found 2018-01-15 19:02:26 ERROR 404: Not Found. Out[23]: ObjectId('5a5d4edb6d99ba784beb0d29') --2018-01-15 19:02:26-- https://www.nintendo.co.jp/ir/en/finance/software/img/data_ds_008.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 18925 (18K) [image/jpeg] Saving to: 'images/data_ds_008.jpg' data_ds_008.jpg 100%[===================>] 18.48K --.-KB/s in 0s 2018-01-15 19:02:27 (202 MB/s) - 'images/data_ds_008.jpg' saved [18925/18925] Out[23]: ObjectId('5a5d4edb6d99ba784beb0d2a') --2018-01-15 19:02:27-- https://www.nintendo.co.jp/ir/en/finance/softwaremg/data_ds_010.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 404 Not Found 2018-01-15 19:02:27 ERROR 404: Not Found. Out[23]: ObjectId('5a5d4edb6d99ba784beb0d2b') --2018-01-15 19:02:27-- https://www.nintendo.co.jp/ir/en/finance/softwaremg/data_ds_009.jpg Resolving www.nintendo.co.jp (www.nintendo.co.jp)... 23.52.165.106 Connecting to www.nintendo.co.jp (www.nintendo.co.jp)|23.52.165.106|:443... connected. HTTP request sent, awaiting response... 404 Not Found 2018-01-15 19:02:28 ERROR 404: Not Found. Out[23]: ObjectId('5a5d4edb6d99ba784beb0d2c') Verify the downloaded images Run the ls images to show the available images and save the list to lsimages . In [24]: lsimages = ! ls images lsimages Out[24]: ['data_3ds_001.jpg', 'data_3ds_002.jpg', 'data_3ds_003.jpg', 'data_3ds_004.jpg', 'data_3ds_009.jpg', 'data_3ds_pokemonoras.jpg', 'data_3ds_pokemonsunmoon.jpg', 'data_3ds_pokemonxy.jpg', 'data_3ds_smashbros.jpg', 'data_3ds_tomodachicollection.jpg', 'data_ds_001.jpg', 'data_ds_002.jpg', 'data_ds_006.jpg', 'data_ds_007.jpg', 'data_ds_008.jpg', 'data_switch_001.png', 'data_switch_002.png', 'data_switch_003.png', 'data_switch_004.png', 'data_switch_005.png', 'data_wii_001.jpg', 'data_wii_002.jpg', 'data_wii_003.jpg', 'data_wii_004.jpg', 'data_wii_005.jpg', 'data_wii_006.jpg', 'data_wii_007.jpg', 'data_wii_008.jpg', 'data_wii_009.jpg', 'data_wii_wiiparty.jpg', 'data_wiiu_mario3dworld.png', 'data_wiiu_mariokart8.png', 'data_wiiu_marioparty10.jpg', 'data_wiiu_newmariou.png', 'data_wiiu_newsuperluigiu.png', 'data_wiiu_nintendoland.png', 'data_wiiu_smashbros.png', 'data_wiiu_splatoon.jpg', 'data_wiiu_supermariomaker.png', 'data_wiiu_zeldahd.png'] Show the first image In [25]: from IPython.display import Image from IPython.core.display import HTML Image ( url = 'images/' + lsimages . list [ 0 ]) Out[25]: Delete the old games from the database In [26]: result = db . games . delete_many ({}) Insert the updated games In [27]: result = db . games . insert_many ( updated_games ) Back-end Create the API with ExpressJS and Mongoose . First make sure NodeJS is installed. curl -sL https://deb.nodesource.com/setup_9.x | sudo -E bash - sudo apt-get install -y nodejs In [28]: ! node --version v9.4.0 In [29]: ! npm --version 5.6.0 Create Node package and install Express generator Install the ExpressJS generator after creating the server folder. In [30]: ! rm -rf server In [31]: ! mkdir server && cd server && npm init -y && npm install express-generator --save Wrote to /home/jitsejan/code/pelican-blog/content/notebooks/server/package.json: { \"name\": \"server\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\" } npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN server@1.0.0 No description npm WARN server@1.0.0 No repository field. + express-generator@4.15.5 added 6 packages in 0.764s Check ExpressJS version In [32]: ! cd server && npm express --version 5.6.0 Create the scaffold for the server Note that we use npx instead of npm because we run NPM from a local folder. In [33]: ! cd server && npx express -y --force --git --view ejs . create : . create : ./package.json create : ./app.js create : ./.gitignore create : ./public create : ./routes create : ./routes/index.js create : ./routes/users.js create : ./views create : ./views/index.ejs create : ./views/error.ejs create : ./bin create : ./bin/www create : ./public/javascripts create : ./public/images create : ./public/stylesheets create : ./public/stylesheets/style.css install dependencies: $ cd . && npm install run the app: $ DEBUG=server:* npm start In [34]: ls server / app.js node_modules / package.json routes / bin / package-lock.json public / views / In [35]: ! cat server/package.json { \"name\": \"server\", \"version\": \"0.0.0\", \"private\": true, \"scripts\": { \"start\": \"node ./bin/www\" }, \"dependencies\": { \"body-parser\": \"~1.18.2\", \"cookie-parser\": \"~1.4.3\", \"debug\": \"~2.6.9\", \"ejs\": \"~2.5.7\", \"express\": \"~4.15.5\", \"morgan\": \"~1.9.0\", \"serve-favicon\": \"~2.4.5\" } } Install dependencies In [36]: ! cd server && npm install added 57 packages and removed 5 packages in 1.734s32minfo lifecycle server@0.0.0~prepare: ser Run the app In [37]: import subprocess proc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) proc . pid Out[37]: 31494 Request the ExpressJS app In [38]: import requests import time time . sleep ( 5 ) resp = requests . get ( 'http://localhost:3000' ) resp . status_code Out[38]: 200 Stop the ExpressJS app Use lsof to find the process that uses port 3000 and kill it. In [39]: process =! lsof -i:3000 -t expressid = int ( process [ 0 ]) expressid Out[39]: 31506 In [40]: ! kill -9 $expressid Check if the server is down In [41]: try : resp = requests . get ( 'http://localhost:3000' ) except : print ( \"Server is down!\" ) Server is down! Connection from ExpressJS to MongoDB Mongoose will be used to connect the Node application to the database. Install package In [42]: ! cd server && npm install --save mongoose + mongoose@4.13.9m......] - postinstall: info lifecycle mongoose@4.13.9~postinstall: added 28 packages in 2.103s Check installation In [43]: ! cat server/package.json | grep mongoose \"mongoose\": \"&#94;4.13.9\", Add the Mongoose connection Add the following to the top of server/app.js . var mongoose = require ( 'mongoose' ); mongoose . connect ( 'mongodb://localhost:27017/nintendo' ); var db = mongoose . connection ; db . on ( \"error\" , console . error . bind ( console , \"Connection error\" )); db . once ( \"open\" , function ( callback ){ console . log ( \"Connection successful\" ) }); In [44]: %%file server/app.js var express = require ( 'express' ); var path = require ( 'path' ); var favicon = require ( 'serve-favicon' ); var logger = require ( 'morgan' ); var cookieParser = require ( 'cookie-parser' ); var bodyParser = require ( 'body-parser' ); // Mongoose connection var mongoose = require ( 'mongoose' ); mongoose . connect ( 'mongodb://localhost:27017/nintendo' ); var db = mongoose . connection ; db . on ( \"error\" , console . error . bind ( console , \"Connection error\" )); db . once ( \"open\" , function ( callback ){ console . log ( \"Connection successful\" ) }); var index = require ( './routes/index' ); var users = require ( './routes/users' ); var app = express (); // view engine setup app . set ( 'views' , path . join ( __dirname , 'views' )); app . set ( 'view engine' , 'ejs' ); // uncomment after placing your favicon in / public // app . use ( favicon ( path . join ( __dirname , 'public' , 'favicon.ico' ))); app . use ( logger ( 'dev' )); app . use ( bodyParser . json ()); app . use ( bodyParser . urlencoded ({ extended : false })); app . use ( cookieParser ()); app . use ( express . static ( path . join ( __dirname , 'public' ))); app . use ( '/' , index ); app . use ( '/users' , users ); // catch 404 and forward to error handler app . use ( function ( req , res , next ) { var err = new Error ( 'Not Found' ); err . status = 404 ; next ( err ); }); // error handler app . use ( function ( err , req , res , next ) { // set locals , only providing error in development res . locals . message = err . message ; res . locals . error = req . app . get ( 'env' ) === 'development' ? err : {}; // render the error page res . status ( err . status || 500 ); res . render ( 'error' ); }); module . exports = app ; Overwriting server/app.js In [45]: ! head -n 20 server/app.js var express = require('express'); var path = require('path'); var favicon = require('serve-favicon'); var logger = require('morgan'); var cookieParser = require('cookie-parser'); var bodyParser = require('body-parser'); // Mongoose connection var mongoose = require('mongoose'); mongoose.connect('mongodb://localhost:27017/nintendo'); var db = mongoose.connection; db.on(\"error\", console.error.bind(console, \"Connection error\")); db.once(\"open\", function(callback){ console.log(\"Connection successful\") }); var index = require('./routes/index'); var users = require('./routes/users'); var app = express(); Check if MongoDB initializes In [46]: import subprocess proc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) In [47]: import time time . sleep ( 5 ) for line in proc . stdout : print ( str ( line )) if 'Connection successful' in str ( line ): print ( \"Success!\" ) break b'\\n' b'> server@0.0.0 start /home/jitsejan/code/pelican-blog/content/notebooks/server\\n' b'> node ./bin/www\\n' b'\\n' b'Connection successful\\n' Success! In [48]: process =! lsof -i:3000 -t expressid = int ( process [ 0 ]) ! kill -9 $expressid Create new model Create the Mongoose model for the games that we gathered in the earlier step. In [49]: ! mkdir server/models In [50]: %%file server/models/game.js var mongoose = require ( \"mongoose\" ); var Schema = mongoose . Schema ; var GameSchema = new Schema ({ console : { type : String }, name : { type : String }, image : { type : String }, sales : { type : String } }); module . exports = mongoose . model ( \"Game\" , GameSchema ); Writing server/models/game.js Adding new route Create the route to access the data of the games. In the scaffold we already have the index.js and users.js , so lets create the games.js to setup the routes for the new pages. Existing routes In [51]: ! cat server/routes/index.js var express = require('express'); var router = express.Router(); /* GET home page. */ router.get('/', function(req, res, next) { res.render('index', { title: 'Express' }); }); module.exports = router; In [52]: ! cat server/routes/users.js var express = require('express'); var router = express.Router(); /* GET users listing. */ router.get('/', function(req, res, next) { res.send('respond with a resource'); }); module.exports = router; Add the route requirement Add the following to server/app.js : ... var consoles = require ( './routes/games.js' ); ... app . use ( '/games' , games ); ... With sed we can insert text on a certain position in a file. In [53]: ! sed -i \"18i var games = require('./routes/games');\" server/app.js Enable the routes in the app: In [54]: ! sed -i \"36i app.use('/games', games);\" server/app.js and create the route file server/routes/games.js : In [55]: %%file server/routes/games.js var express = require ( 'express' ); var router = express . Router (); var Game = require ( \"../models/game\" ); router . get ( '/' , ( req , res ) => { Game . find ({}, '' , function ( error , games ){ if ( error ) { game . error ( error ); } res . send ({ games : games }) }) . sort ({ _id : - 1 }) }) module . exports = router ; Writing server/routes/games.js Start the server and verify the new route which should return a JSON object. In [56]: import json import requests import time proc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 5 ) resp = requests . get ( 'http://localhost:3000/games' ) . json () print ( json . dumps ( resp , indent = 4 )) { \"games\": [ { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e3a\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" }, { \"image\": \"images/data_ds_010.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e39\", \"sales\": \"11.75\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_ds_008.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e38\", \"sales\": \"12.72\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_005.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e37\", \"sales\": \"14.88\", \"name\": \"Brain Age 2:\" }, { \"image\": \"images/data_ds_007.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e36\", \"sales\": \"15.64\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_006.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e35\", \"sales\": \"17.67\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_004.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e34\", \"sales\": \"19.01\", \"name\": \"Brain Age:\" }, { \"image\": \"images/data_ds_002.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e33\", \"sales\": \"23.60\", \"name\": \"Mario Kart DS\" }, { \"image\": \"images/data_ds_003.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e32\", \"sales\": \"23.96\", \"name\": \"nintendogs\" }, { \"image\": \"images/data_ds_001.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e31\", \"sales\": \"30.80\", \"name\": \"New Super Mario Bros.\" }, { \"image\": \"images/data_wii_wiiparty.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e30\", \"sales\": \"9.29\", \"name\": \"Wii Party\" }, { \"image\": \"images/data_wii_008.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e2f\", \"sales\": \"12.76\", \"name\": \"Super Mario Galaxy\" }, { \"image\": \"images/data_wii_009.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e2e\", \"sales\": \"13.25\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wii_007.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e2d\", \"sales\": \"21.13\", \"name\": \"Wii Fit Plus\" }, { \"image\": \"images/data_wii_006.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e2c\", \"sales\": \"22.67\", \"name\": \"Wii Fit\" }, { \"image\": \"images/data_wii_004.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e2b\", \"sales\": \"28.02\", \"name\": \"Wii Play\" }, { \"image\": \"images/data_wii_005.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e2a\", \"sales\": \"30.11\", \"name\": \"New\" }, { \"image\": \"images/data_wii_003.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e29\", \"sales\": \"33.06\", \"name\": \"Wii Sports Resort\" }, { \"image\": \"images/data_wii_002.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e28\", \"sales\": \"37.02\", \"name\": \"Mario Kart Wii\" }, { \"image\": \"images/data_wii_001.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e27\", \"sales\": \"82.83\", \"name\": \"Wii Sports\" }, { \"image\": \"images/data_3ds_009.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e26\", \"sales\": \"5.45\", \"name\": \"Luigi's Mansion:\" }, { \"image\": \"images/data_3ds_tomodachicollection.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e25\", \"sales\": \"5.93\", \"name\": \"Tomodachi Life\" }, { \"image\": \"images/data_3ds_smashbros.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e24\", \"sales\": \"8.91\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_3ds_004.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e23\", \"sales\": \"11.23\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_3ds_001.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e22\", \"sales\": \"11.40\", \"name\": \"SUPER MARIO\" }, { \"image\": \"images/data_3ds_003.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e21\", \"sales\": \"11.73\", \"name\": \"New\" }, { \"image\": \"images/data_3ds_pokemonoras.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e20\", \"sales\": \"13.85\", \"name\": \"Pok\\u00e9mon Omega Ruby/\" }, { \"image\": \"images/data_3ds_pokemonsunmoon.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e1f\", \"sales\": \"15.91\", \"name\": \"Pok\\u00e9mon Sun/\" }, { \"image\": \"images/data_3ds_002.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e1e\", \"sales\": \"15.95\", \"name\": \"Mario Kart 7\" }, { \"image\": \"images/data_3ds_pokemonxy.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e1d\", \"sales\": \"16.20\", \"name\": \"Pok\\u00e9mon X/Pok\\u00e9mon Y\" }, { \"image\": \"images/data_wiiu_marioparty10.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e1c\", \"sales\": \"2.13\", \"name\": \"Mario Party 10\" }, { \"image\": \"images/data_wiiu_zeldahd.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e1b\", \"sales\": \"2.23\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wiiu_newsuperluigiu.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e1a\", \"sales\": \"2.99\", \"name\": \"New Super Luigi U\" }, { \"image\": \"images/data_wiiu_supermariomaker.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e19\", \"sales\": \"3.98\", \"name\": \"Super Mario Maker\" }, { \"image\": \"images/data_wiiu_splatoon.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e18\", \"sales\": \"4.87\", \"name\": \"Splatoon\" }, { \"image\": \"images/data_wiiu_nintendoland.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e17\", \"sales\": \"5.18\", \"name\": \"Nintendo Land\" }, { \"image\": \"images/data_wiiu_smashbros.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e16\", \"sales\": \"5.26\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wiiu_mario3dworld.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e15\", \"sales\": \"5.70\", \"name\": \"Super Mario 3D World\" }, { \"image\": \"images/data_wiiu_newmariou.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e14\", \"sales\": \"5.73\", \"name\": \"New\" }, { \"image\": \"images/data_wiiu_mariokart8.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0e13\", \"sales\": \"8.38\", \"name\": \"Mario Kart 8\" }, { \"image\": \"images/data_switch_004.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0e12\", \"sales\": \"1.35\", \"name\": \"ARMS\" }, { \"image\": \"images/data_switch_003.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0e11\", \"sales\": \"1.37\", \"name\": \"1-2-Switch\" }, { \"image\": \"images/data_switch_005.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0e10\", \"sales\": \"3.61\", \"name\": \"Splatoon 2\" }, { \"image\": \"images/data_switch_002.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0e0f\", \"sales\": \"4.42\", \"name\": \"Mario Kart 8 Deluxe\" }, { \"image\": \"images/data_switch_001.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0e0e\", \"sales\": \"4.70\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wii_wiiparty.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e0d\", \"sales\": \"9.29\", \"name\": \"Wii Party\" }, { \"image\": \"images/data_wii_008.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e0c\", \"sales\": \"12.76\", \"name\": \"Super Mario Galaxy\" }, { \"image\": \"images/data_wii_009.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e0b\", \"sales\": \"13.25\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wii_007.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e0a\", \"sales\": \"21.13\", \"name\": \"Wii Fit Plus\" }, { \"image\": \"images/data_wii_006.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e09\", \"sales\": \"22.67\", \"name\": \"Wii Fit\" }, { \"image\": \"images/data_wii_004.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e08\", \"sales\": \"28.02\", \"name\": \"Wii Play\" }, { \"image\": \"images/data_wii_005.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e07\", \"sales\": \"30.11\", \"name\": \"New\" }, { \"image\": \"images/data_wii_003.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e06\", \"sales\": \"33.06\", \"name\": \"Wii Sports Resort\" }, { \"image\": \"images/data_wii_002.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e05\", \"sales\": \"37.02\", \"name\": \"Mario Kart Wii\" }, { \"image\": \"images/data_wii_001.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0e04\", \"sales\": \"82.83\", \"name\": \"Wii Sports\" }, { \"image\": \"images/data_3ds_009.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e03\", \"sales\": \"5.45\", \"name\": \"Luigi's Mansion:\" }, { \"image\": \"images/data_3ds_tomodachicollection.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e02\", \"sales\": \"5.93\", \"name\": \"Tomodachi Life\" }, { \"image\": \"images/data_3ds_smashbros.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e01\", \"sales\": \"8.91\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_3ds_004.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0e00\", \"sales\": \"11.23\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_3ds_001.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dff\", \"sales\": \"11.40\", \"name\": \"SUPER MARIO\" }, { \"image\": \"images/data_3ds_003.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dfe\", \"sales\": \"11.73\", \"name\": \"New\" }, { \"image\": \"images/data_3ds_pokemonoras.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dfd\", \"sales\": \"13.85\", \"name\": \"Pok\\u00e9mon Omega Ruby/\" }, { \"image\": \"images/data_3ds_pokemonsunmoon.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dfc\", \"sales\": \"15.91\", \"name\": \"Pok\\u00e9mon Sun/\" }, { \"image\": \"images/data_3ds_002.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dfb\", \"sales\": \"15.95\", \"name\": \"Mario Kart 7\" }, { \"image\": \"images/data_3ds_pokemonxy.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dfa\", \"sales\": \"16.20\", \"name\": \"Pok\\u00e9mon X/Pok\\u00e9mon Y\" }, { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df9\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" }, { \"image\": \"images/data_ds_010.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df8\", \"sales\": \"11.75\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_ds_008.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df7\", \"sales\": \"12.72\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_005.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df6\", \"sales\": \"14.88\", \"name\": \"Brain Age 2:\" }, { \"image\": \"images/data_ds_007.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df5\", \"sales\": \"15.64\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_006.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df4\", \"sales\": \"17.67\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_004.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df3\", \"sales\": \"19.01\", \"name\": \"Brain Age:\" }, { \"image\": \"images/data_ds_002.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df2\", \"sales\": \"23.60\", \"name\": \"Mario Kart DS\" }, { \"image\": \"images/data_ds_003.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df1\", \"sales\": \"23.96\", \"name\": \"nintendogs\" }, { \"image\": \"images/data_ds_001.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0df0\", \"sales\": \"30.80\", \"name\": \"New Super Mario Bros.\" }, { \"image\": \"images/data_wiiu_marioparty10.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0def\", \"sales\": \"2.13\", \"name\": \"Mario Party 10\" }, { \"image\": \"images/data_wiiu_zeldahd.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dee\", \"sales\": \"2.23\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wiiu_newsuperluigiu.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0ded\", \"sales\": \"2.99\", \"name\": \"New Super Luigi U\" }, { \"image\": \"images/data_wiiu_supermariomaker.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dec\", \"sales\": \"3.98\", \"name\": \"Super Mario Maker\" }, { \"image\": \"images/data_wiiu_splatoon.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0deb\", \"sales\": \"4.87\", \"name\": \"Splatoon\" }, { \"image\": \"images/data_wiiu_nintendoland.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dea\", \"sales\": \"5.18\", \"name\": \"Nintendo Land\" }, { \"image\": \"images/data_wiiu_smashbros.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0de9\", \"sales\": \"5.26\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wiiu_mario3dworld.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0de8\", \"sales\": \"5.70\", \"name\": \"Super Mario 3D World\" }, { \"image\": \"images/data_wiiu_newmariou.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0de7\", \"sales\": \"5.73\", \"name\": \"New\" }, { \"image\": \"images/data_wiiu_mariokart8.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0de6\", \"sales\": \"8.38\", \"name\": \"Mario Kart 8\" }, { \"image\": \"images/data_switch_004.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0de5\", \"sales\": \"1.35\", \"name\": \"ARMS\" }, { \"image\": \"images/data_switch_003.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0de4\", \"sales\": \"1.37\", \"name\": \"1-2-Switch\" }, { \"image\": \"images/data_switch_005.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0de3\", \"sales\": \"3.61\", \"name\": \"Splatoon 2\" }, { \"image\": \"images/data_switch_002.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0de2\", \"sales\": \"4.42\", \"name\": \"Mario Kart 8 Deluxe\" }, { \"image\": \"images/data_switch_001.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0de1\", \"sales\": \"4.70\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0de0\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" }, { \"image\": \"images/data_ds_010.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0ddf\", \"sales\": \"11.75\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_ds_008.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dde\", \"sales\": \"12.72\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_005.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0ddd\", \"sales\": \"14.88\", \"name\": \"Brain Age 2:\" }, { \"image\": \"images/data_ds_007.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0ddc\", \"sales\": \"15.64\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_006.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0ddb\", \"sales\": \"17.67\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_004.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dda\", \"sales\": \"19.01\", \"name\": \"Brain Age:\" }, { \"image\": \"images/data_ds_002.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dd9\", \"sales\": \"23.60\", \"name\": \"Mario Kart DS\" }, { \"image\": \"images/data_ds_003.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dd8\", \"sales\": \"23.96\", \"name\": \"nintendogs\" }, { \"image\": \"images/data_ds_001.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dd7\", \"sales\": \"30.80\", \"name\": \"New Super Mario Bros.\" }, { \"image\": \"images/data_wii_wiiparty.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd6\", \"sales\": \"9.29\", \"name\": \"Wii Party\" }, { \"image\": \"images/data_wii_008.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd5\", \"sales\": \"12.76\", \"name\": \"Super Mario Galaxy\" }, { \"image\": \"images/data_wii_009.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd4\", \"sales\": \"13.25\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wii_007.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd3\", \"sales\": \"21.13\", \"name\": \"Wii Fit Plus\" }, { \"image\": \"images/data_wii_006.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd2\", \"sales\": \"22.67\", \"name\": \"Wii Fit\" }, { \"image\": \"images/data_wii_004.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd1\", \"sales\": \"28.02\", \"name\": \"Wii Play\" }, { \"image\": \"images/data_wii_005.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dd0\", \"sales\": \"30.11\", \"name\": \"New\" }, { \"image\": \"images/data_wii_003.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dcf\", \"sales\": \"33.06\", \"name\": \"Wii Sports Resort\" }, { \"image\": \"images/data_wii_002.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dce\", \"sales\": \"37.02\", \"name\": \"Mario Kart Wii\" }, { \"image\": \"images/data_wii_001.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0dcd\", \"sales\": \"82.83\", \"name\": \"Wii Sports\" }, { \"image\": \"images/data_3ds_009.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dcc\", \"sales\": \"5.45\", \"name\": \"Luigi's Mansion:\" }, { \"image\": \"images/data_3ds_tomodachicollection.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dcb\", \"sales\": \"5.93\", \"name\": \"Tomodachi Life\" }, { \"image\": \"images/data_3ds_smashbros.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dca\", \"sales\": \"8.91\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_3ds_004.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc9\", \"sales\": \"11.23\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_3ds_001.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc8\", \"sales\": \"11.40\", \"name\": \"SUPER MARIO\" }, { \"image\": \"images/data_3ds_003.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc7\", \"sales\": \"11.73\", \"name\": \"New\" }, { \"image\": \"images/data_3ds_pokemonoras.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc6\", \"sales\": \"13.85\", \"name\": \"Pok\\u00e9mon Omega Ruby/\" }, { \"image\": \"images/data_3ds_pokemonsunmoon.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc5\", \"sales\": \"15.91\", \"name\": \"Pok\\u00e9mon Sun/\" }, { \"image\": \"images/data_3ds_002.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc4\", \"sales\": \"15.95\", \"name\": \"Mario Kart 7\" }, { \"image\": \"images/data_3ds_pokemonxy.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0dc3\", \"sales\": \"16.20\", \"name\": \"Pok\\u00e9mon X/Pok\\u00e9mon Y\" }, { \"image\": \"images/data_wiiu_marioparty10.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dc2\", \"sales\": \"2.13\", \"name\": \"Mario Party 10\" }, { \"image\": \"images/data_wiiu_zeldahd.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dc1\", \"sales\": \"2.23\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wiiu_newsuperluigiu.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dc0\", \"sales\": \"2.99\", \"name\": \"New Super Luigi U\" }, { \"image\": \"images/data_wiiu_supermariomaker.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dbf\", \"sales\": \"3.98\", \"name\": \"Super Mario Maker\" }, { \"image\": \"images/data_wiiu_splatoon.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dbe\", \"sales\": \"4.87\", \"name\": \"Splatoon\" }, { \"image\": \"images/data_wiiu_nintendoland.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dbd\", \"sales\": \"5.18\", \"name\": \"Nintendo Land\" }, { \"image\": \"images/data_wiiu_smashbros.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dbc\", \"sales\": \"5.26\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wiiu_mario3dworld.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dbb\", \"sales\": \"5.70\", \"name\": \"Super Mario 3D World\" }, { \"image\": \"images/data_wiiu_newmariou.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0dba\", \"sales\": \"5.73\", \"name\": \"New\" }, { \"image\": \"images/data_wiiu_mariokart8.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0db9\", \"sales\": \"8.38\", \"name\": \"Mario Kart 8\" }, { \"image\": \"images/data_switch_004.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0db8\", \"sales\": \"1.35\", \"name\": \"ARMS\" }, { \"image\": \"images/data_switch_003.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0db7\", \"sales\": \"1.37\", \"name\": \"1-2-Switch\" }, { \"image\": \"images/data_switch_005.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0db6\", \"sales\": \"3.61\", \"name\": \"Splatoon 2\" }, { \"image\": \"images/data_switch_002.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0db5\", \"sales\": \"4.42\", \"name\": \"Mario Kart 8 Deluxe\" }, { \"image\": \"images/data_switch_001.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0db4\", \"sales\": \"4.70\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0db3\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" }, { \"image\": \"images/data_ds_010.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0db2\", \"sales\": \"11.75\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_ds_008.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0db1\", \"sales\": \"12.72\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_005.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0db0\", \"sales\": \"14.88\", \"name\": \"Brain Age 2:\" }, { \"image\": \"images/data_ds_007.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0daf\", \"sales\": \"15.64\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_006.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dae\", \"sales\": \"17.67\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_004.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dad\", \"sales\": \"19.01\", \"name\": \"Brain Age:\" }, { \"image\": \"images/data_ds_002.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dac\", \"sales\": \"23.60\", \"name\": \"Mario Kart DS\" }, { \"image\": \"images/data_ds_003.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0dab\", \"sales\": \"23.96\", \"name\": \"nintendogs\" }, { \"image\": \"images/data_ds_001.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0daa\", \"sales\": \"30.80\", \"name\": \"New Super Mario Bros.\" }, { \"image\": \"images/data_wii_wiiparty.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da9\", \"sales\": \"9.29\", \"name\": \"Wii Party\" }, { \"image\": \"images/data_wii_008.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da8\", \"sales\": \"12.76\", \"name\": \"Super Mario Galaxy\" }, { \"image\": \"images/data_wii_009.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da7\", \"sales\": \"13.25\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wii_007.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da6\", \"sales\": \"21.13\", \"name\": \"Wii Fit Plus\" }, { \"image\": \"images/data_wii_006.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da5\", \"sales\": \"22.67\", \"name\": \"Wii Fit\" }, { \"image\": \"images/data_wii_004.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da4\", \"sales\": \"28.02\", \"name\": \"Wii Play\" }, { \"image\": \"images/data_wii_005.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da3\", \"sales\": \"30.11\", \"name\": \"New\" }, { \"image\": \"images/data_wii_003.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da2\", \"sales\": \"33.06\", \"name\": \"Wii Sports Resort\" }, { \"image\": \"images/data_wii_002.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da1\", \"sales\": \"37.02\", \"name\": \"Mario Kart Wii\" }, { \"image\": \"images/data_wii_001.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0da0\", \"sales\": \"82.83\", \"name\": \"Wii Sports\" }, { \"image\": \"images/data_3ds_009.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d9f\", \"sales\": \"5.45\", \"name\": \"Luigi's Mansion:\" }, { \"image\": \"images/data_3ds_tomodachicollection.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d9e\", \"sales\": \"5.93\", \"name\": \"Tomodachi Life\" }, { \"image\": \"images/data_3ds_smashbros.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d9d\", \"sales\": \"8.91\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_3ds_004.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d9c\", \"sales\": \"11.23\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_3ds_001.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d9b\", \"sales\": \"11.40\", \"name\": \"SUPER MARIO\" }, { \"image\": \"images/data_3ds_003.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d9a\", \"sales\": \"11.73\", \"name\": \"New\" }, { \"image\": \"images/data_3ds_pokemonoras.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d99\", \"sales\": \"13.85\", \"name\": \"Pok\\u00e9mon Omega Ruby/\" }, { \"image\": \"images/data_3ds_pokemonsunmoon.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d98\", \"sales\": \"15.91\", \"name\": \"Pok\\u00e9mon Sun/\" }, { \"image\": \"images/data_3ds_002.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d97\", \"sales\": \"15.95\", \"name\": \"Mario Kart 7\" }, { \"image\": \"images/data_3ds_pokemonxy.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d96\", \"sales\": \"16.20\", \"name\": \"Pok\\u00e9mon X/Pok\\u00e9mon Y\" }, { \"image\": \"images/data_wiiu_marioparty10.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d95\", \"sales\": \"2.13\", \"name\": \"Mario Party 10\" }, { \"image\": \"images/data_wiiu_zeldahd.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d94\", \"sales\": \"2.23\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wiiu_newsuperluigiu.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d93\", \"sales\": \"2.99\", \"name\": \"New Super Luigi U\" }, { \"image\": \"images/data_wiiu_supermariomaker.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d92\", \"sales\": \"3.98\", \"name\": \"Super Mario Maker\" }, { \"image\": \"images/data_wiiu_splatoon.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d91\", \"sales\": \"4.87\", \"name\": \"Splatoon\" }, { \"image\": \"images/data_wiiu_nintendoland.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d90\", \"sales\": \"5.18\", \"name\": \"Nintendo Land\" }, { \"image\": \"images/data_wiiu_smashbros.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d8f\", \"sales\": \"5.26\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wiiu_mario3dworld.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d8e\", \"sales\": \"5.70\", \"name\": \"Super Mario 3D World\" }, { \"image\": \"images/data_wiiu_newmariou.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d8d\", \"sales\": \"5.73\", \"name\": \"New\" }, { \"image\": \"images/data_wiiu_mariokart8.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d8c\", \"sales\": \"8.38\", \"name\": \"Mario Kart 8\" }, { \"image\": \"images/data_switch_004.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d8b\", \"sales\": \"1.35\", \"name\": \"ARMS\" }, { \"image\": \"images/data_switch_003.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d8a\", \"sales\": \"1.37\", \"name\": \"1-2-Switch\" }, { \"image\": \"images/data_switch_005.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d89\", \"sales\": \"3.61\", \"name\": \"Splatoon 2\" }, { \"image\": \"images/data_switch_002.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d88\", \"sales\": \"4.42\", \"name\": \"Mario Kart 8 Deluxe\" }, { \"image\": \"images/data_switch_001.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d87\", \"sales\": \"4.70\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d86\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" }, { \"image\": \"images/data_ds_010.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d85\", \"sales\": \"11.75\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_ds_008.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d84\", \"sales\": \"12.72\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_005.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d83\", \"sales\": \"14.88\", \"name\": \"Brain Age 2:\" }, { \"image\": \"images/data_ds_007.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d82\", \"sales\": \"15.64\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_006.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d81\", \"sales\": \"17.67\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_004.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d80\", \"sales\": \"19.01\", \"name\": \"Brain Age:\" }, { \"image\": \"images/data_ds_002.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d7f\", \"sales\": \"23.60\", \"name\": \"Mario Kart DS\" }, { \"image\": \"images/data_ds_003.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d7e\", \"sales\": \"23.96\", \"name\": \"nintendogs\" }, { \"image\": \"images/data_ds_001.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d7d\", \"sales\": \"30.80\", \"name\": \"New Super Mario Bros.\" }, { \"image\": \"images/data_wii_wiiparty.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d7c\", \"sales\": \"9.29\", \"name\": \"Wii Party\" }, { \"image\": \"images/data_wii_008.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d7b\", \"sales\": \"12.76\", \"name\": \"Super Mario Galaxy\" }, { \"image\": \"images/data_wii_009.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d7a\", \"sales\": \"13.25\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wii_007.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d79\", \"sales\": \"21.13\", \"name\": \"Wii Fit Plus\" }, { \"image\": \"images/data_wii_006.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d78\", \"sales\": \"22.67\", \"name\": \"Wii Fit\" }, { \"image\": \"images/data_wii_004.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d77\", \"sales\": \"28.02\", \"name\": \"Wii Play\" }, { \"image\": \"images/data_wii_005.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d76\", \"sales\": \"30.11\", \"name\": \"New\" }, { \"image\": \"images/data_wii_003.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d75\", \"sales\": \"33.06\", \"name\": \"Wii Sports Resort\" }, { \"image\": \"images/data_wii_002.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d74\", \"sales\": \"37.02\", \"name\": \"Mario Kart Wii\" }, { \"image\": \"images/data_wii_001.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d73\", \"sales\": \"82.83\", \"name\": \"Wii Sports\" }, { \"image\": \"images/data_3ds_009.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d72\", \"sales\": \"5.45\", \"name\": \"Luigi's Mansion:\" }, { \"image\": \"images/data_3ds_tomodachicollection.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d71\", \"sales\": \"5.93\", \"name\": \"Tomodachi Life\" }, { \"image\": \"images/data_3ds_smashbros.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d70\", \"sales\": \"8.91\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_3ds_004.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d6f\", \"sales\": \"11.23\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_3ds_001.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d6e\", \"sales\": \"11.40\", \"name\": \"SUPER MARIO\" }, { \"image\": \"images/data_3ds_003.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d6d\", \"sales\": \"11.73\", \"name\": \"New\" }, { \"image\": \"images/data_3ds_pokemonoras.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d6c\", \"sales\": \"13.85\", \"name\": \"Pok\\u00e9mon Omega Ruby/\" }, { \"image\": \"images/data_3ds_pokemonsunmoon.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d6b\", \"sales\": \"15.91\", \"name\": \"Pok\\u00e9mon Sun/\" }, { \"image\": \"images/data_3ds_002.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d6a\", \"sales\": \"15.95\", \"name\": \"Mario Kart 7\" }, { \"image\": \"images/data_3ds_pokemonxy.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d69\", \"sales\": \"16.20\", \"name\": \"Pok\\u00e9mon X/Pok\\u00e9mon Y\" }, { \"image\": \"images/data_wiiu_marioparty10.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d68\", \"sales\": \"2.13\", \"name\": \"Mario Party 10\" }, { \"image\": \"images/data_wiiu_zeldahd.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d67\", \"sales\": \"2.23\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wiiu_newsuperluigiu.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d66\", \"sales\": \"2.99\", \"name\": \"New Super Luigi U\" }, { \"image\": \"images/data_wiiu_supermariomaker.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d65\", \"sales\": \"3.98\", \"name\": \"Super Mario Maker\" }, { \"image\": \"images/data_wiiu_splatoon.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d64\", \"sales\": \"4.87\", \"name\": \"Splatoon\" }, { \"image\": \"images/data_wiiu_nintendoland.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d63\", \"sales\": \"5.18\", \"name\": \"Nintendo Land\" }, { \"image\": \"images/data_wiiu_smashbros.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d62\", \"sales\": \"5.26\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wiiu_mario3dworld.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d61\", \"sales\": \"5.70\", \"name\": \"Super Mario 3D World\" }, { \"image\": \"images/data_wiiu_newmariou.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d60\", \"sales\": \"5.73\", \"name\": \"New\" }, { \"image\": \"images/data_wiiu_mariokart8.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d5f\", \"sales\": \"8.38\", \"name\": \"Mario Kart 8\" }, { \"image\": \"images/data_switch_004.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d5e\", \"sales\": \"1.35\", \"name\": \"ARMS\" }, { \"image\": \"images/data_switch_003.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d5d\", \"sales\": \"1.37\", \"name\": \"1-2-Switch\" }, { \"image\": \"images/data_switch_005.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d5c\", \"sales\": \"3.61\", \"name\": \"Splatoon 2\" }, { \"image\": \"images/data_switch_002.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d5b\", \"sales\": \"4.42\", \"name\": \"Mario Kart 8 Deluxe\" }, { \"image\": \"images/data_switch_001.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d5a\", \"sales\": \"4.70\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d59\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" }, { \"image\": \"images/data_ds_010.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d58\", \"sales\": \"11.75\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_ds_008.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d57\", \"sales\": \"12.72\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_005.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d56\", \"sales\": \"14.88\", \"name\": \"Brain Age 2:\" }, { \"image\": \"images/data_ds_007.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d55\", \"sales\": \"15.64\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_006.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d54\", \"sales\": \"17.67\", \"name\": \"Pok\\u00e9mon\" }, { \"image\": \"images/data_ds_004.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d53\", \"sales\": \"19.01\", \"name\": \"Brain Age:\" }, { \"image\": \"images/data_ds_002.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d52\", \"sales\": \"23.60\", \"name\": \"Mario Kart DS\" }, { \"image\": \"images/data_ds_003.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d51\", \"sales\": \"23.96\", \"name\": \"nintendogs\" }, { \"image\": \"images/data_ds_001.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0d50\", \"sales\": \"30.80\", \"name\": \"New Super Mario Bros.\" }, { \"image\": \"images/data_wii_wiiparty.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d4f\", \"sales\": \"9.29\", \"name\": \"Wii Party\" }, { \"image\": \"images/data_wii_008.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d4e\", \"sales\": \"12.76\", \"name\": \"Super Mario Galaxy\" }, { \"image\": \"images/data_wii_009.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d4d\", \"sales\": \"13.25\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wii_007.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d4c\", \"sales\": \"21.13\", \"name\": \"Wii Fit Plus\" }, { \"image\": \"images/data_wii_006.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d4b\", \"sales\": \"22.67\", \"name\": \"Wii Fit\" }, { \"image\": \"images/data_wii_004.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d4a\", \"sales\": \"28.02\", \"name\": \"Wii Play\" }, { \"image\": \"images/data_wii_005.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d49\", \"sales\": \"30.11\", \"name\": \"New\" }, { \"image\": \"images/data_wii_003.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d48\", \"sales\": \"33.06\", \"name\": \"Wii Sports Resort\" }, { \"image\": \"images/data_wii_002.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d47\", \"sales\": \"37.02\", \"name\": \"Mario Kart Wii\" }, { \"image\": \"images/data_wii_001.jpg\", \"console\": \"Wii\", \"_id\": \"5a5d4f246d99ba784beb0d46\", \"sales\": \"82.83\", \"name\": \"Wii Sports\" }, { \"image\": \"images/data_3ds_009.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d45\", \"sales\": \"5.45\", \"name\": \"Luigi's Mansion:\" }, { \"image\": \"images/data_3ds_tomodachicollection.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d44\", \"sales\": \"5.93\", \"name\": \"Tomodachi Life\" }, { \"image\": \"images/data_3ds_smashbros.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d43\", \"sales\": \"8.91\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_3ds_004.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d42\", \"sales\": \"11.23\", \"name\": \"Animal Crossing:\" }, { \"image\": \"images/data_3ds_001.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d41\", \"sales\": \"11.40\", \"name\": \"SUPER MARIO\" }, { \"image\": \"images/data_3ds_003.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d40\", \"sales\": \"11.73\", \"name\": \"New\" }, { \"image\": \"images/data_3ds_pokemonoras.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d3f\", \"sales\": \"13.85\", \"name\": \"Pok\\u00e9mon Omega Ruby/\" }, { \"image\": \"images/data_3ds_pokemonsunmoon.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d3e\", \"sales\": \"15.91\", \"name\": \"Pok\\u00e9mon Sun/\" }, { \"image\": \"images/data_3ds_002.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d3d\", \"sales\": \"15.95\", \"name\": \"Mario Kart 7\" }, { \"image\": \"images/data_3ds_pokemonxy.jpg\", \"console\": \"Nintendo 3DS\", \"_id\": \"5a5d4f246d99ba784beb0d3c\", \"sales\": \"16.20\", \"name\": \"Pok\\u00e9mon X/Pok\\u00e9mon Y\" }, { \"image\": \"images/data_wiiu_marioparty10.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d3b\", \"sales\": \"2.13\", \"name\": \"Mario Party 10\" }, { \"image\": \"images/data_wiiu_zeldahd.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d3a\", \"sales\": \"2.23\", \"name\": \"The Legend of Zelda:\" }, { \"image\": \"images/data_wiiu_newsuperluigiu.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d39\", \"sales\": \"2.99\", \"name\": \"New Super Luigi U\" }, { \"image\": \"images/data_wiiu_supermariomaker.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d38\", \"sales\": \"3.98\", \"name\": \"Super Mario Maker\" }, { \"image\": \"images/data_wiiu_splatoon.jpg\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d37\", \"sales\": \"4.87\", \"name\": \"Splatoon\" }, { \"image\": \"images/data_wiiu_nintendoland.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d36\", \"sales\": \"5.18\", \"name\": \"Nintendo Land\" }, { \"image\": \"images/data_wiiu_smashbros.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d35\", \"sales\": \"5.26\", \"name\": \"Super Smash Bros.\" }, { \"image\": \"images/data_wiiu_mario3dworld.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d34\", \"sales\": \"5.70\", \"name\": \"Super Mario 3D World\" }, { \"image\": \"images/data_wiiu_newmariou.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d33\", \"sales\": \"5.73\", \"name\": \"New\" }, { \"image\": \"images/data_wiiu_mariokart8.png\", \"console\": \"Wii U\", \"_id\": \"5a5d4f246d99ba784beb0d32\", \"sales\": \"8.38\", \"name\": \"Mario Kart 8\" }, { \"image\": \"images/data_switch_004.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d31\", \"sales\": \"1.35\", \"name\": \"ARMS\" }, { \"image\": \"images/data_switch_003.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d30\", \"sales\": \"1.37\", \"name\": \"1-2-Switch\" }, { \"image\": \"images/data_switch_005.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d2f\", \"sales\": \"3.61\", \"name\": \"Splatoon 2\" }, { \"image\": \"images/data_switch_002.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d2e\", \"sales\": \"4.42\", \"name\": \"Mario Kart 8 Deluxe\" }, { \"image\": \"images/data_switch_001.png\", \"console\": \"Nintendo Switch\", \"_id\": \"5a5d4f246d99ba784beb0d2d\", \"sales\": \"4.70\", \"name\": \"The Legend of Zelda:\" } ] } Lets grab the ID of the first game of the data to verify the route in the next step. In [57]: game_id = resp [ 'games' ][ 0 ][ '_id' ] game_id Out[57]: '5a5d4f246d99ba784beb0e3a' Kill the application again. In [58]: process =! fuser 3000 /tcp | awk '{print $1}' expressid = int ( process [ 1 ]) ! kill -9 $expressid Add another route to the games.js to get information for a single game. router . get ( '/:id' , ( req , res ) => { var db = req . db ; Game . findById ( req . params . id , '' , function ( error , game ) { if ( error ) { console . error ( error ); } res . send ( game ) }) }) In [59]: %%file server/routes/games.js var express = require ( 'express' ); var router = express . Router (); var Game = require ( \"../models/game\" ); router . get ( '/' , ( req , res ) => { Game . find ({}, '' , function ( error , games ){ if ( error ) { game . error ( error ); } res . send ({ games : games }) }) . sort ({ _id : - 1 }) }) router . get ( '/:id' , ( req , res ) => { var db = req . db ; Game . findById ( req . params . id , '' , function ( error , game ) { if ( error ) { console . error ( error ); } res . send ( game ) }) }) module . exports = router ; Overwriting server/routes/games.js Verify the game detail route As a final step to verify the API we check if we can get the detailed information for the game ID we saved in the previous step. In [60]: import time proc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 5 ) resp = requests . get ( 'http://localhost:3000/games/' + game_id ) . json () print ( json . dumps ( resp , indent = 4 )) { \"image\": \"images/data_ds_009.jpg\", \"console\": \"Nintendo DS\", \"_id\": \"5a5d4f246d99ba784beb0e3a\", \"sales\": \"11.06\", \"name\": \"Super Mario 64 DS\" } In [61]: process =! fuser 3000 /tcp | awk '{print $1}' expressid = int ( process [ 1 ]) ! kill -9 $expressid Conclusion For the back-end we have created an API using Mongoose and ExpressJS with the following two routes: All games Game detail Front-end The next step is to create the front-end that can talk to the back-end and visualize the information. As a Javascript framework we are going to use VueJS. We will install the Vue CLI . Create the client folder and install Vue In [62]: ! rm -rf client In [63]: ! mkdir client && cd client && npm init -y && npm install vue-cli --save Wrote to /home/jitsejan/code/pelican-blog/content/notebooks/client/package.json: { \"name\": \"client\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\" } npm WARN deprecated coffee-script@1.12.7: CoffeeScript on NPM has moved to \"coffeescript\" (no hyphen) npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN client@1.0.0 No description npm WARN client@1.0.0 No repository field. + vue-cli@2.9.2 added 264 packages in 7.753s In [64]: ls - la client total 100 drwxrwxr-x 3 jitsejan jitsejan 4096 Jan 15 19:03 . / drwxrwxr-x 6 jitsejan jitsejan 12288 Jan 15 19:03 .. / drwxrwxr-x 239 jitsejan jitsejan 12288 Jan 15 19:03 node_modules / -rw-rw-r-- 1 jitsejan jitsejan 68307 Jan 15 19:03 package-lock.json -rw-rw-r-- 1 jitsejan jitsejan 269 Jan 15 19:03 package.json Check version of Vue In [65]: ! cd client && npm vue --version 5.6.0 Create the scaffold To create the scaffold we will use the vue-cli . However, this requires us to give direct input to the command line which is tricky because we need to answer different questions when creating the scaffold using the tool. The easiest way to automate this task is to use the Linux tool Expect . Make sure the tool is installed on your system. $ sudo apt-get install expect -y Note: if you do not want to use this trick, you can also use the external terminal and run $ vue init webpack in the client directory or simply clone the repository to your local file system. The following script contains the answers for the prompts created by the vue init . In [66]: %%file client/init_expect_script.sh #!/usr/bin/expect -f spawn npx vue init webpack expect \"Generate project in current directory?\" { send \"Y \\r \" } expect \"Project name\" send \"vueclient \\r \" expect \"Project description\" send \"An experiment with Jupyter and MEVN \\r \" expect \"Author\" send \"Jupyter \\r\\n \" expect \"Vue build\" send \" \\r\\n \" expect \"Install vue-router?\" send \"Y \\r \" expect \"Use ESLint to lint your code?\" send \"Y \\r \" expect \"Pick an ESLint preset\" send \" \\r\\n \" expect \"Set up unit tests\" send \"Y \\r \" expect \"Pick a test runner\" send \" \\r\\n \" expect \"Setup e2e tests with Nightwatch?\" send \"Y \\r \" expect \"Should we run `npm install` for you after the project has been created? (recommended)\" send \" \\r\\n \" interact Writing client/init_expect_script.sh In [67]: ! chmod +x client/init_expect_script.sh In [68]: ! cd client && ./init_expect_script.sh spawn npx vue init webpack ? Generate project in current directory? (Y/n) ? Generate project in current directory? (Y/n) Y ? Generate project in current directory? Yes ��� downloading template ��� downloading template ��� downloading template ��� downloading template ��� downloading template ? Project name (client) ? Project name (client) v ? Project name (client) vu ? Project name (client) vue ? Project name (client) vuec ? Project name (client) vuecl ? Project name (client) vuecli ? Project name (client) vueclie ? Project name (client) vueclien ? Project name (client) vueclient ? Project name vueclient ? Project description (A Vue.js project) ? Project description (A Vue.js project) A ? Project description (A Vue.js project) An ? Project description (A Vue.js project) An ? Project description (A Vue.js project) An e ? Project description (A Vue.js project) An ex ? Project description (A Vue.js project) An exp ? Project description (A Vue.js project) An expe ? Project description (A Vue.js project) An exper ? Project description (A Vue.js project) An experi ? Project description (A Vue.js project) An experim ? Project description (A Vue.js project) An experime ? Project description (A Vue.js project) An experimen ? Project description (A Vue.js project) An experiment ? Project description (A Vue.js project) An experiment ? Project description (A Vue.js project) An experiment w ? Project description (A Vue.js project) An experiment wi ? Project description (A Vue.js project) An experiment wit ? Project description (A Vue.js project) An experiment with ? Project description (A Vue.js project) An experiment with ? Project description (A Vue.js project) An experiment with J ? Project description (A Vue.js project) An experiment with Ju ? Project description (A Vue.js project) An experiment with Jup ? Project description (A Vue.js project) An experiment with Jupy ? Project description (A Vue.js project) An experiment with Jupyt ? Project description (A Vue.js project) An experiment with Jupyte ? Project description (A Vue.js project) An experiment with Jupyter ? Project description (A Vue.js project) An experiment with Jupyter ? Project description (A Vue.js project) An experiment with Jupyter a ? Project description (A Vue.js project) An experiment with Jupyter an ? Project description (A Vue.js project) An experiment with Jupyter and ? Project description (A Vue.js project) An experiment with Jupyter and ? Project description (A Vue.js project) An experiment with Jupyter and M ? Project description (A Vue.js project) An experiment with Jupyter and ME ? Project description (A Vue.js project) An experiment with Jupyter and MEV ? Project description (A Vue.js project) An experiment with Jupyter and MEVN ? Project description An experiment with Jupyter and MEVN ? Author ? Author J ? Author Ju ? Author Jup ? Author Jupy ? Author Jupyt ? Author Jupyte ? Author Jupyter ? Author Jupyter ? Vue build (Use arrow keys) ��� Runtime + Compiler: recommended for most users Runtime-only: about 6KB lighter min+gzip, but templates (or any Vue-specific H TML) are ONLY allowed in .vue files - render functions are required elsewhere ? Vue build standalone ? Install vue-router? (Y/n) ? Install vue-router? (Y/n) Y ? Install vue-router? Yes ? Use ESLint to lint your code? (Y/n) ? Use ESLint to lint your code? (Y/n) Y ? Use ESLint to lint your code? Yes ? Pick an ESLint preset (Use arrow keys) ��� Standard (https://github.com/standard/standard) Airbnb (https://github.com/airbnb/javascript) none (configure it yourself) ? Pick an ESLint preset Standard ? Set up unit tests (Y/n) ? Set up unit tests (Y/n) Y ? Set up unit tests Yes ? Pick a test runner (Use arrow keys) ��� Jest Karma and Mocha none (configure it yourself) ? Pick a test runner jest ? Setup e2e tests with Nightwatch? (Y/n) ? Setup e2e tests with Nightwatch? (Y/n) Y ? Setup e2e tests with Nightwatch? Yes ? Should we run `npm install` for you after the project has been created? (recom mended) (Use arrow keys) ��� Yes, use NPM Yes, use Yarn No, I will handle that myself ? Should we run `npm install` for you after the project has been created? (recom mended) npm vue-cli �� Generated \"client\". # Installing project dependencies ... # ======================== ...... ] \\ install:babel-preset-stage-2: info lifecycle babel-prese > chromedriver@2.34.1 install /home/jitsejan/code/pelican-blog/content/notebooks/client/node_modules/chromedriver > node install.js Downloading https://chromedriver.storage.googleapis.com/2.34/chromedriver_linux64.zip Saving to /tmp/chromedriver/chromedriver_linux64.zip Received 781K... Received 1568K... Received 2352K... Received 3136K... Received 3642K total. Extracting zip contents Copying to target path /home/jitsejan/code/pelican-blog/content/notebooks/client/node_modules/chromedriver/lib/chromedriver Fixing file permissions Done. ChromeDriver binary available at /home/jitsejan/code/pelican-blog/content/notebooks/client/node_modules/chromedriver/lib/chromedriver/chromedriver 7m ...... ] / postinstall:read-pkg-up: info lifecycle read-pkg-up@2.0. > uglifyjs-webpack-plugin@0.4.6 postinstall /home/jitsejan/code/pelican-blog/content/notebooks/client/node_modules/webpack/node_modules/uglifyjs-webpack-plugin > node lib/post_install.js npm .... ] | prepare:vueclient: info lifecycle vueclient@1.0.0~prepar[0m WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.1.3 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.3: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) added 1403 packages, removed 79 packages and moved 7 packages in 56.615s Running eslint --fix to comply with chosen preset rules... # ======================== > vueclient@1.0.0 lint /home/jitsejan/code/pelican-blog/content/notebooks/client > eslint --ext .js,.vue src test/unit test/e2e/specs \"--fix\" # Project initialization finished! # ======================== To get started: npm run dev Documentation can be found at https://vuejs-templates.github.io/webpack In [69]: ls client README.md index.html package-lock.json static / build / init_expect_script.sh * package.json test / config / node_modules / src / Temporary fix There seems to be an issue with webpack which disables Node to start the application. For now the version of webpack-dev-server 2.9.7 seems to not throw any errors. In [70]: ! cd client && npm install webpack-dev-server@2.9.7 --save-dev npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.1.3 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.3: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) + webpack-dev-server@2.9.7 added 2 packages, removed 145 packages, updated 2 packages and moved 1 package in 12.321s In [71]: import time proc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) Check the default page The default page of Vue should be available on port 8080. Lets use Selenium to visit the webpage and make a screenshot. Setup Selenium First create a Docker instance that runs Selenium to avoid the cumbersome installation on the local Linux machine. In [72]: selenium_running = False for cntr in docker_client . containers . list (): if 'selenium' in cntr . attrs [ 'Config' ][ 'Image' ]: selenium_running = True container = cntr if selenium_running is False : container = docker_client . containers . run ( \"selenium/standalone-chrome:latest\" , name = 'selenium' , ports = { '4444' : '4444' }, detach = True ) In [73]: time . sleep ( 10 ) Install Selenium with pip pip install selenium In [74]: ! pip3 install selenium Collecting selenium Using cached selenium-3.8.1-py2.py3-none-any.whl Installing collected packages: selenium Successfully installed selenium-3.8.1 You are using pip version 8.1.1, however version 9.0.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. In [75]: from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , DesiredCapabilities . CHROME ) driver . get ( 'http://prod.jitsejan.com:8080' ) driver . save_screenshot ( 'vue_frontpage.png' ) Out[75]: True In [76]: from IPython.display import Image Image ( \"vue_frontpage.png\" ) Out[76]: Kill the process on port 8080 after verifying the front page. In [77]: process =! fuser 8080 /tcp | awk '{print $1}' vueid = int ( process [ 1 ]) ! kill -9 $vueid As we can see from the screenshot, we cannot access the page because webpack has changed some configuration. In order to circument this we need to add the host to the script in package.json and change \"dev\" : \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\" , to \"dev\" : \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js --host 0.0.0.0\" , which can be done with sed : In [78]: ! sed -i '/dev.*--inline/s/conf.js/conf.js --host 0.0.0.0/' client/package.json In [79]: cat client / package . json | grep dev \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js --host 0.0.0.0\", \"start\": \"npm run dev\", \"devDependencies\": { \"webpack-dev-server\": \"&#94;2.9.7\", Additionally we need to disable the host check in build/webpack.dev.conf.js by adding disableHostCheck : true to the devServer . In [80]: ! sed -i \"26i\\ \\ \\ \\ disableHostCheck: true,\" client/build/webpack.dev.conf.js In [81]: ! grep -C3 disableHostCheck client/build/webpack.dev.conf.js --color = auto // these devServer options should be customized in /config/index.js devServer: { clientLogLevel: 'warning', disableHostCheck : true, historyApiFallback: { rewrites: [ { from: /.*/, to: path.posix.join(config.dev.assetsPublicPath, 'index.html') }, Retry In [82]: proc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) In [83]: driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , DesiredCapabilities . CHROME ) driver . get ( 'http://prod.jitsejan.com:8080' ) driver . get_screenshot_as_file ( 'vue_front.png' ) Out[83]: True In [84]: Image ( \"vue_front.png\" ) Out[84]: In [85]: process =! fuser 8080 /tcp | awk '{print $1}' vueid = int ( process [ 1 ]) ! kill -9 $vueid Create games page In [86]: ls client / src / components / HelloWorld.vue Create the component Add the Vue template to the Games component. < template > < div class = \"games\" > This page will list all the games. </ div > </ template > < script > export default { name : 'Games' , data () { return {} } } </ script > In [87]: %%file client/src/components/Games.vue < template > < div class = \"games\" > This page will list all the games . </ div > </ template > < script > export default { name : 'Games' , data () { return {} } } </ script > Writing client/src/components/Games.vue In [88]: ls client / src / components / Games.vue HelloWorld.vue Create the route In [89]: cat client / src / router / index . js import Vue from 'vue' import Router from 'vue-router' import HelloWorld from '@/components/HelloWorld' Vue.use(Router) export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld } ] }) Add the import of the Games component to the router file. import Games from '@/components/Games' and add the path to the games page indicating to which component the page will link. ... { path : '/games' , name : 'Games' , component : Games } ... In [90]: %%file client/src/router/index.js import Vue from 'vue' import Router from 'vue-router' import HelloWorld from '@/components/HelloWorld' import Games from '@/components/Games' Vue . use ( Router ) export default new Router ({ routes : [ { path : '/' , name : 'HelloWorld' , component : HelloWorld }, { path : '/games' , name : 'Games' , component : Games } ] }) Overwriting client/src/router/index.js Verify the new route In [91]: proc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , DesiredCapabilities . CHROME ) driver . get ( 'http://prod.jitsejan.com:8080/ui/#/games' ) driver . get_screenshot_as_file ( 'new_route.png' ) process =! fuser 8080 /tcp | awk '{print $1}' vueid = int ( process [ 1 ]) ! kill -9 $vueid Out[91]: True In [92]: Image ( \"new_route.png\" ) Out[92]: Connect to back-end We have the route complete, time to connect to the data from the database. For this we will use axios . Install axios In [93]: ! cd client && npm install --save axios npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.1.3 (node_modules/fsevents): npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.3: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"linux\",\"arch\":\"x64\"}) + axios@0.17.1 added 3 packages in 10.18s Setup connection with back-end Create a services folder that will contain the API Javascript files. In [94]: mkdir - p client / src / services In [95]: %%file client/src/services/api.js import axios from 'axios' export default () => { return axios . create ({ baseURL : ` http : // prod . jitsejan . com : 3000 ` }) } Writing client/src/services/api.js Create service to retrieve the games In [96]: %%file client/src/services/GamesService.js import api from '@/services/api' export default { fetchGames () { return api () . get ( 'games' ) } } Writing client/src/services/GamesService.js Add the service to the Games component This component shows all the games available in the database. Note that this is the file that should be updated if you would like a more fancy layout, for example by adding Bootstrap or other frameworks. < template > < div class = \"games\" > < div class = \"games-div\" v-for = \"game in games\" :key = \"game._id\" > < p > < span >< b > {{ game.name }} </ b ></ span >< br /> < span > {{ game.console }} </ span >< br /> < span > {{ game.sales }} </ span > < a :href = \"'/ui/#/games/' + game._id\" > Details </ a > </ p > </ div > </ div > </ template > < script > import GamesService from '@/services/GamesService' export default { name : 'Games' , data () { return { games : [] } }, mounted () { this . getGames () }, methods : { async getGames () { const response = await GamesService . fetchGames () this . games = response . data . games } } } </ script > In [97]: %%file client/src/components/Games.vue < template > < div class = \"games\" > < div class = \"games-div\" v - for = \"game in games\" : key = \"game._id\" > < p > < span >< b > {{ game . name }} </ b ></ span >< br /> < span > {{ game . console }} </ span >< br /> < span > {{ game . sales }} </ span > < a : href = \"'/ui/#/games/' + game._id\" > Details </ a > </ p > </ div > </ div > </ template > < script > import GamesService from '@/services/GamesService' export default { name : 'Games' , data () { return { games : [] } }, mounted () { this . getGames () }, methods : { async getGames () { const response = await GamesService . fetchGames () this . games = response . data . games } } } </ script > Overwriting client/src/components/Games.vue Start both the Express back-end and the Vue front-end to verify if the data from the API is retrieved with the updated component. Small trick: by setting the logging preferences when starting the webdriver with Selenium, we can actually retrieve the logs you would normally see with the developer tools in the browser. In [98]: vueproc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) expressproc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) desired = DesiredCapabilities . CHROME desired [ 'loggingPrefs' ] = { 'browser' : 'ALL' } driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , desired_capabilities = desired ) driver . get ( 'http://prod.jitsejan.com:8080/ui/#/games' ) driver . get_screenshot_as_file ( 'new_route_with_data.png' ) vueprocess =! fuser 8080 /tcp | awk '{print $1}' vueid = int ( vueprocess [ 1 ]) ! kill -9 $vueid expressprocess =! fuser 3000 /tcp | awk '{print $1}' expressid = int ( expressprocess [ 1 ]) ! kill -9 $expressid Out[98]: True In [99]: logs = driver . get_log ( 'browser' ) logs Out[99]: [{'level': 'INFO', 'message': 'webpack-internal:///./node_modules/webpack/hot/log.js 22:11 \"[HMR] Waiting for update signal from WDS...\"', 'source': 'console-api', 'timestamp': 1516064762951}, {'level': 'INFO', 'message': 'webpack-internal:///./node_modules/vue/dist/vue.esm.js 8437:44 \"Download the Vue Devtools extension for a better development experience:\\\\nhttps://github.com/vuejs/vue-devtools\"', 'source': 'console-api', 'timestamp': 1516064763382}, {'level': 'SEVERE', 'message': \"http://prod.jitsejan.com:8080/ui/#/games - Failed to load http://prod.jitsejan.com:3000/games: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://prod.jitsejan.com:8080' is therefore not allowed access.\", 'source': 'javascript', 'timestamp': 1516064763546}, {'level': 'SEVERE', 'message': 'webpack-internal:///./node_modules/axios/lib/core/createError.js 15:14 Uncaught Error: Network Error', 'source': 'javascript', 'timestamp': 1516064763549}, {'level': 'SEVERE', 'message': 'webpack-internal:///./node_modules/webpack-dev-server/client/index.js?http://0.0.0.0:8080 171:8 \"[WDS] Disconnected!\"', 'source': 'console-api', 'timestamp': 1516064766361}] In [100]: Image ( \"new_route_with_data.png\" ) Out[100]: As we can see in the logs, we have an issue accessing the back-end because of the Access-Control-Allow-Origin limitation. To circumvent this, we need to make sure that we are allowed to access the API; hece install the package Cors and enable it in the Express application. In [101]: ! cd server && npm install --save cors + cors@2.8.4m ...... ] / postinstall: info lifecycle cors@2.8.4~postinstall: cors added 2 packages in 1.115s Add the requirement for the cors package and enable it in the Express app by adding the following to server/app.js . ... var cors = require ( 'cors' ) ... app . use ( cors ()) ... In [102]: %%file server/app.js var express = require ( 'express' ); var path = require ( 'path' ); var favicon = require ( 'serve-favicon' ); var logger = require ( 'morgan' ); var cookieParser = require ( 'cookie-parser' ); var bodyParser = require ( 'body-parser' ); // Mongoose connection var mongoose = require ( 'mongoose' ); mongoose . connect ( 'mongodb://localhost:27017/nintendo' ); var db = mongoose . connection ; db . on ( \"error\" , console . error . bind ( console , \"Connection error\" )); db . once ( \"openUri\" , function ( callback ){ console . log ( \"Connection successful\" ) }); var cors = require ( \"cors\" ) var index = require ( './routes/index' ); var users = require ( './routes/users' ); var games = require ( './routes/games' ); var app = express (); app . use ( cors ()); // view engine setup app . set ( 'views' , path . join ( __dirname , 'views' )); app . set ( 'view engine' , 'ejs' ); // uncomment after placing your favicon in / public // app . use ( favicon ( path . join ( __dirname , 'public' , 'favicon.ico' ))); app . use ( logger ( 'dev' )); app . use ( bodyParser . json ()); app . use ( bodyParser . urlencoded ({ extended : false })); app . use ( cookieParser ()); app . use ( express . static ( path . join ( __dirname , 'public' ))); app . use ( '/' , index ); app . use ( '/users' , users ); app . use ( '/games' , games ); // catch 404 and forward to error handler app . use ( function ( req , res , next ) { var err = new Error ( 'Not Found' ); err . status = 404 ; next ( err ); }); // error handler app . use ( function ( err , req , res , next ) { // set locals , only providing error in development res . locals . message = err . message ; res . locals . error = req . app . get ( 'env' ) === 'development' ? err : {}; // render the error page res . status ( err . status || 500 ); res . render ( 'error' ); }); module . exports = app ; Overwriting server/app.js Now we are connected and allowed to retrieve the data. We use Selenium and wait for the games-div to appear. In [103]: from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.by import By vueproc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) expressproc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) desired = DesiredCapabilities . CHROME desired [ 'loggingPrefs' ] = { 'browser' : 'ALL' } driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , desired_capabilities = desired ) driver . get ( 'http://prod.jitsejan.com:8080/ui/#/games' ) WebDriverWait ( driver , 5 ) . until ( EC . presence_of_element_located (( By . CLASS_NAME , 'games-div' ))) driver . refresh () driver . save_screenshot ( 'allgames.png' ) Out[103]: <selenium.webdriver.remote.webelement.WebElement (session=\"ba1712b1e5ede5037de7ff15d2b109fc\", element=\"0.8736937714035513-1\")> Out[103]: True In [104]: Image ( \"allgames.png\" ) Out[104]: Clean up In [105]: driver . quit () vueprocess =! fuser 8080 /tcp | awk '{print $1}' vueid = int ( vueprocess [ 1 ]) ! kill -9 $vueid expressprocess =! fuser 3000 /tcp | awk '{print $1}' expressid = int ( expressprocess [ 1 ]) ! kill -9 $expressid Create detail page for the game Create the component and connect to the API by importing the service and execute the promise. Retrieving the data for a game can also be solved by extending the GamesServices.js we created earlier by adding another fetch function, but since every problem can be solved in different ways, I chose to use the method as shown in the following file. < template > < div class = \"games-detail\" > < h1 > Details for {{ data.name }} </ h1 > < img :src = \"'/static/' + data.image\" />< br /> Console: {{ data.console }} < br /> Sales: {{ data.sales}} million < br /> </ div > </ template > < script > import api from '@/services/api' export default { name : 'GamesDetail' , data () { return { data : '' } }, created () { api (). get ( 'games/' + this . $route . params . id ) . then ( response => { this . data = response . data }) . catch ( e => { this . errors . push ( e ) }) } } </ script > In [106]: %%file client/src/components/GamesDetail.vue < template > < div class = \"games-detail\" > < h1 > Details for {{ data . name }} </ h1 > < img : src = \"'/static/' + data.image\" />< br /> Console : {{ data . console }} < br /> Sales : {{ data . sales }} million < br /> </ div > </ template > < script > import api from '@/services/api' export default { name : 'GamesDetail' , data () { return { data : '' } }, created () { api () . get ( 'games/' + this . $ route . params . id ) . then ( response => { this . data = response . data }) . catch ( e => { this . errors . push ( e ) }) } } </ script > Writing client/src/components/GamesDetail.vue Add the route for the detail page by adding the following to client/src/router/index.js : ... import GamesDetail from '@/components/GamesDetail' ... { path : '/games/:id' , name : 'GamesDetail' , component : GamesDetail } In [107]: %%file client/src/router/index.js import Vue from 'vue' import Router from 'vue-router' import HelloWorld from '@/components/HelloWorld' import Games from '@/components/Games' import GamesDetail from '@/components/GamesDetail' Vue . use ( Router ) export default new Router ({ routes : [ { path : '/' , name : 'HelloWorld' , component : HelloWorld }, { path : '/games' , name : 'Games' , component : Games }, { path : '/games/:id' , name : 'GamesDetail' , component : GamesDetail } ] }) Overwriting client/src/router/index.js Start the processes In [108]: vueproc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) expressproc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) Verify the detail page for a given ID. In [109]: desired = DesiredCapabilities . CHROME desired [ 'loggingPrefs' ] = { 'browser' : 'ALL' } driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , desired_capabilities = desired ) driver . get ( 'http://prod.jitsejan.com:8080/ui/#/games/' + game_id ) driver . save_screenshot ( 'gamedetail.png' ) Out[109]: True In [110]: Image ( \"gamedetail.png\" ) Out[110]: Note: not all images from the source are working properly, but that could easily be fixed by improving the webscraper and verifying the images. In [111]: vueprocess =! fuser 8080 /tcp | awk '{print $1}' vueid = int ( vueprocess [ 1 ]) ! kill -9 $vueid expressprocess =! fuser 3000 /tcp | awk '{print $1}' expressid = int ( expressprocess [ 1 ]) ! kill -9 $expressid Finalize To wrap this notebook up, lets update the generic App.vue to make it easier to navigate between the pages by adding a menu to client/src/App.vue . Copy the following text to the client/src/App.vue . < template > < div id = \"app\" > < router-link :to = \"{ name: 'HelloWorld' }\" > Home </ router-link > < router-link :to = \"{ name: 'Games'}\" > Games </ router-link > < router-view /> </ div > </ template > < script > export default { name : 'app' } </ script > < style > # app { font-family : 'Avenir' , Helvetica , Arial , sans-serif ; -webkit- font-smoothing : antialiased ; -moz- osx-font-smoothing : grayscale ; text-align : center ; color : #2c3e50 ; margin-top : 60 px ; } </ style > In [112]: %%file client/src/App.vue < template > < div id = \"app\" > < router - link : to = \"{ name: 'HelloWorld' }\" > Home </ router - link > < router - link : to = \"{ name: 'Games'}\" > Games </ router - link > < router - view /> </ div > </ template > < script > export default { name : 'app' } </ script > < style > #app { font - family : 'Avenir' , Helvetica , Arial , sans - serif ; - webkit - font - smoothing : antialiased ; - moz - osx - font - smoothing : grayscale ; text - align : center ; color : #2c3e50; margin - top : 60 px ; } </ style > Overwriting client/src/App.vue In [113]: vueproc = subprocess . Popen ( \"cd client && npm run dev &\" , shell = True , stdout = subprocess . PIPE ) expressproc = subprocess . Popen ( \"cd server && npm start &\" , shell = True , stdout = subprocess . PIPE ) time . sleep ( 10 ) In [114]: desired = DesiredCapabilities . CHROME desired [ 'loggingPrefs' ] = { 'browser' : 'ALL' } driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , desired_capabilities = desired ) driver . get ( 'http://prod.jitsejan.com:8080' ) driver . save_screenshot ( 'menuadded.png' ) Out[114]: True In [115]: Image ( \"menuadded.png\" ) Out[115]: Clean up In [116]: %%bash rm -rf client rm -rf server rm -rf iamges Future work In this example application I did not use any fancy styling or advanced HTML structures to make the website more appealing. Additionally, the API is only used to retrieve data, while obviously it could also used to create data, update it or delete it (CRUD), but for the scope of this notebook I kept it to a minimum. As a third improvement you could choose to put everything of this notebook in one Docker compose file, where MongoDB, Selenium, ExpressJS and VueJS are all containerized, but since I am not planning to port this application to another server, it is fine to simply install the files locally and remove them again after the notebook has finished.", "tags": "posts", "url": "mevn-stack-experiment-with-jupyter.html", "loc": "mevn-stack-experiment-with-jupyter.html" }, { "title": "Creating block diagram using Python in Jupyter notebook", "text": "Creating block diagram Install the blockdiag library We will use the blockdiag library to easily create a block diagram to insert in the Jupyter notebook. In [1]: ! pip install blockdiag Collecting blockdiag Using cached blockdiag-1.5.3-py2.py3-none-any.whl Requirement already satisfied: webcolors in /anaconda/lib/python3.6/site-packages (from blockdiag) Requirement already satisfied: Pillow in /anaconda/lib/python3.6/site-packages (from blockdiag) Requirement already satisfied: setuptools in /anaconda/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg (from blockdiag) Requirement already satisfied: funcparserlib in /anaconda/lib/python3.6/site-packages (from blockdiag) Requirement already satisfied: olefile in /anaconda/lib/python3.6/site-packages (from Pillow->blockdiag) Installing collected packages: blockdiag Successfully installed blockdiag-1.5.3 Diagram definition First we create the folder: In [2]: ! mkdir diagrams mkdir: diagrams: File exists and we create a new file inside the diagrams folder with the definition of the graph: In [3]: %%file diagrams/block_diagram blockdiag { // Define class ( list of attributes ) class emphasis [ color = lightblue , style = dashed ]; class redline [ color = red , style = dotted ]; Extract -> Transform -> Load ; // Set class to node Transform [ class = \" emphasis \"]; // Set class to edge Transform -> Load [ class = \" redline \"]; } Overwriting diagrams/block_diagram Convert the diagram to PNG Using the blockdiag library, we can convert the diagram from the definition to a PNG by running the blockdiag command from the command line with the graph definition as first argument: In [4]: ! blockdiag diagrams/block_diagram Display the PNG By using the display functionality of the Jupyter notebook we can show the generated PNG: In [5]: from IPython.display import Image Image ( \"diagrams/block_diagram.png\" ) Out[5]: Convert to slides Jupyter has the functionality to convert a Jupyter notebook to a fancy looking presentation. By using nbconvert and with the output set to slides , the notebook will be translated to an HTML page, which will be transformed into a beautiful presentation by adding the RevealJS library. In [6]: ! jupyter nbconvert creating_block_diagram.ipynb --to slides [NbConvertApp] Converting notebook creating_block_diagram.ipynb to slides [NbConvertApp] Writing 297425 bytes to creating_block_diagram.slides.html Check for resulting HTML After conversion we should see both the .ipynb and .html in the directory. In [7]: ! ls -la creating_block_diagram.slides.html -rw-r--r-- 1 jitsejan staff 297489 Jan 15 00:43 creating_block_diagram.slides.html Retrieve RevealJS To use Jupyter slides we need to download RevealJS to the root directory of the notebooks. We do this by cloning the reveal.js Git repository directly in the current folder. In [ ]: ! git clone https://github.com/hakimel/reveal.js.git fatal: destination path 'reveal.js' already exists and is not an empty directory. Start Python webserver Use http.server for Python 3 and SimpleHTTPServer when using Python 2 to start the server to view the HTML page. In [ ]: ! python -m http.server 8000 Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... Check port 8000 and open the HTML page containing the slides. In this case the HTML of this notebook will be visible on http://localhost:8000/creating_block_diagram.slides.html#/ .", "tags": "posts", "url": "creating-block-diagram.html", "loc": "creating-block-diagram.html" }, { "title": "Create Spark dataframe column with lag", "text": "Create a lagged column in a PySpark dataframe: from pyspark.sql.functions import monotonically_increasing_id , lag from pyspark.sql.window import Window # Add ID to be used by the window function df = df . withColumn ( 'id' , monotonically_increasing_id ()) # Set the window w = Window . orderBy ( \"id\" ) # Create the lagged value value_lag = lag ( 'value' ) . over ( w ) # Add the lagged values to a new column df = df . withColumn ( 'prev_value' , value_lag )", "tags": "posts", "url": "create-spark-dataframe-column-with-lag.html", "loc": "create-spark-dataframe-column-with-lag.html" }, { "title": "MEVN Stack - Setting up MongoDB, Express and VueJS", "text": "MEVN Stack This stack consists of the following elements: MongoDB ExpressJS VueJS NodeJS Objectives Setup front-end with VueJS Setup back-end with ExpressJS Setup the connection between the front-end and back-end using Axios Setup the connection between ExpressJS and MongoDB Prerequisites Install the last version of node and npm . jitsejan@dev:~/code$ curl -sL https://deb.nodesource.com/setup_9.x | sudo -E bash - jitsejan@dev:~/code$ sudo apt-get install -y nodejs jitsejan@dev:~/code$ node --version v9.2.0 jitsejan@dev:~/code$ npm --version 5 .5.1 Setup Create the folder for the application and move inside it. jitsejan@dev:~/code$ mkdir mongo-express-vue-node && cd $_ Client Install the Vue CLI to easily create the scaffold for a Vue application. jitsejan@dev:~/code$ sudo npm install -g vue-cli jitsejan@dev:~/code$ vue --version 2 .9.1 Use the webpack template to create a Vue app with a webpack boilerplate. jitsejan@dev:~/code/mongo-express-vue-node$ vue init webpack client ? Project name client ? Project description MEVN - Vue.js client ? Author Jitse-Jan <jitsejan@gmail.com> ? Vue build standalone ? Install vue-router? Yes ? Use ESLint to lint your code? Yes ? Pick an ESLint preset Standard ? Setup unit tests Yes ? Pick a test runner karma ? Setup e2e tests with Nightwatch? Yes vue-cli · Generated \"client\" . To get started: cd client npm install npm run dev Documentation can be found at https://vuejs-templates.github.io/webpack jitsejan@dev:~/code/mongo-express-vue-node$ cd client jitsejan@dev:~/code/mongo-express-vue-node/client$ npm install To make the app accessible from the VPS, first change the host for the webpack-dev-server in client/package.json from ... \"scripts\" : { \"dev\" : \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\" , ... } , ... to ... \"scripts\" : { \"dev\" : \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js --host 0.0.0.0\" , ... } , ... and make sure the disableHostCheck is set to true in client/build/webpack.dev.conf.js : ... devServer: { ... disableHostCheck: true ... } ... We can now start the application by running the following command: jitsejan@dev:~/code/mongo-express-vue-node/client$ npm run dev and use curl to check the page: jitsejan@dev:~/code/mongo-express-vue-node/client$ curl dev.jitsejan.com:8080 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 322 100 322 0 0 1477 0 --:--:-- --:--:-- --:--:-- 1586 <!DOCTYPE html> <html> <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1.0\" > <title>client</title> </head> <body> <div id = \"app\" ></div> <!-- built files will be auto injected --> <script type = \"text/javascript\" src = \"/app.js\" ></script></body> </html> which is indeed the content of the index.html of the client application. Server As a back-end we will use ExpressJS to connect the front-end to the data and deal with all server related functionality. To (again) not setup all the files by ourselves, but use a generator to create the scaffold, I will use the Express generator . Create boilerplate jitsejan@dev:~/code/mongo-express-vue-node$ sudo npm install express-generator -g jitsejan@dev:~/code/mongo-express-vue-node$ express --version 4 .15.5 Using the generator, we can create the boilerplate for the server, with a .gitignore by using --git and with ejs support by setting the view argument. jitsejan@dev:~/code/mongo-express-vue-node$ express --git --view ejs server create : server create : server/package.json create : server/app.js create : server/.gitignore create : server/public create : server/routes create : server/routes/index.js create : server/routes/users.js create : server/views create : server/views/index.ejs create : server/views/error.ejs create : server/bin create : server/bin/www create : server/public/javascripts create : server/public/images create : server/public/stylesheets create : server/public/stylesheets/style.css install dependencies: $ cd server && npm install run the app: $ DEBUG = server:* npm start Lets navigate inside the server folder, install the packages and start the server. jitsejan@dev:~/code/mongo-express-vue-node$ cd server/ jitsejan@dev:~/code/mongo-express-vue-node/server$ npm install jitsejan@dev:~/code/mongo-express-vue-node/server$ DEBUG = server:* npm start > server@0.0.0 start /home/jitsejan/code/mongo-express-vue-node/server > node ./bin/www server:server Listening on port 3000 +0ms Using curl we can retrieve the content of the application by requesting the server on port 3000. jitsejan@dev:~/code/mongo-express-vue-node$ curl dev.jitsejan.com:3000 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 207 100 207 0 0 880 0 --:--:-- --:--:-- --:--:-- 945 <!DOCTYPE html> <html> <head> <title>Express</title> <link rel = 'stylesheet' href = '/stylesheets/style.css' /> </head> <body> <h1>Express</h1> <p>Welcome to Express</p> </body> </html> Setup API route Add a new route to server/app.js and add some fake data to be returned: ... app . get ( '/characters' , ( req , res ) => { res . send ( { 'characters' : [ { name : \"Mario\" , color : \"red\" }, { name : \"Luigi\" , color : \"green\" } ] } ) }) ... Start the server to test the new route by running the following command: jitsejan@dev:~/code/mongo-express-vue-node/server$ DEBUG = server:* npm start which will result in the curl response as shown below using jq for a nicer layout. jitsejan@dev:~$ curl dev.jitsejan.com:3000/characters | jq '.' % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 65 100 65 0 0 1029 0 --:--:-- --:--:-- --:--:-- 1031 \"characters\" : [ { \"name\" : \"Mario\" , \"color\" : \"red\" } , { \"name\" : \"Luigi\" , \"color\" : \"green\" } ] Connect client and server Finally, we need to connect the VueJS front with the ExpressJS back-end. To do this, we will use axios at the client-side to talk to the API from the server-side. First install axios for the client: jitsejan@dev:~/code/mongo-express-vue-node/client$ npm install --save axios Setup characters component Create a new Vue component to show the characters from the API. Add the content to client/src/components/Characters.vue : < < template > < div class = \"characters\" > This file will list all the characters. < div v-for = \"character in characters\" :key = \"character.name\" > < p > < span >< b > {{ character.name }} </ b ></ span >< br /> < span > {{ character.color }} </ span >< br /> </ p > </ div > </ div > </ template > < script > import CharactersService from '@/services/CharactersService' export default { name : 'Characters' , data () { return { characters : [] } }, mounted () { this . getCharacters () }, methods : { async getCharacters () { const response = await CharactersService . fetchCharacters () this . characters = response . data . characters console . log ( response . data ) } } } </ script > with client/src/services/CharactersService.js contains import api from '@/services/api' export default { fetchCharacters () { return api () . get ( 'characters' ) } } and client/src/services/api.js contains import axios from 'axios' export default () => { return axios . create ({ baseURL : ` http : // localhost : 3000 ` }) } Add a route to client/src/router/index.js for the characters view by adding the import of the component and defining the route parameters. ... import Characters from '@/components/Characters' ... { path : '/characters' , name : 'Characters' , component : Characters } ... Extend main application To visit the available routes, add the router-links to the main application. Change the template in client/src/App.vue : < template > < div id = \"app\" > < router-link :to = \"{ name: 'Home' }\" > Home </ router-link > < router-link :to = \"{ name: 'Characters'}\" > Characters </ router-link > < router-view /> </ div > </ template > ... Main page: Character page: For the most code, check the Github repo .", "tags": "posts", "url": "mevn-stack-tutorial.html", "loc": "mevn-stack-tutorial.html" }, { "title": "Using Pythons filter to select columns in dataframe", "text": "A simple trick to select columns from a dataframe: # Create the filter condition condition = lambda col : col not in DESIRED_COLUMNS # Filter the dataframe filtered_df = df . drop ( * filter ( condition , df . columns ))", "tags": "posts", "url": "using-pythons-filter-on-dataframe.html", "loc": "using-pythons-filter-on-dataframe.html" }, { "title": "Using Vue.js in a Jupyter notebook", "text": "Objectives Generate data using webcrawling with requests from Canada's Top 100 Use of Scrapy Use of Pandas Integrate VueJS in a notebook Create simple table with filter functionality Scraping data Approach To scrape the data, we will use the Scrapy library. Instead of writing our own scrapers, it is faster for this tutorial to simply use a proper library that was build to scrape for you. Load the main page Find all company links For each company link, open the corresponding page For each company page, find all ratings Markup for companies links < div id = \"winners\" class = \"page-section\" > ... < li >< span >< a target = \"_blank\" href = \"http://content.eluta.ca/top-employer-3m-canada\" > 3M Canada Company </ a ></ span ></ li > ... </ div > This corresponds with the Python code from the CompanySpider class: for href in response . css ( 'div#winners a::attr(href)' ) . extract (): Markup for ratings < h3 class = \"rating-row\" > < span class = \"nocolor\" > Physical Workplace </ span > < span class = \"rating\" > < span class = \"score\" title = \"Great-West Life Assurance Company, The's physical workplace is rated as exceptional. \" > A+ </ span > </ span > </ h3 > Python crawler The crawler in Scrapy is defined in the following code snippet. import logging import scrapy from scrapy.crawler import CrawlerProcess class CompanySpider ( scrapy . Spider ): name = \"companies\" start_urls = [ \"http://www.canadastop100.com/national/\" ] custom_settings = { 'LOG_LEVEL' : logging . CRITICAL , 'FEED_FORMAT' : 'json' , 'FEED_URI' : 'canadastop100.json' } def parse ( self , response ): for href in response . css ( 'div#winners a::attr(href)' ) . extract (): yield scrapy . Request ( response . urljoin ( href ), callback = self . parse_company ) def parse_company ( self , response ): name = response . css ( 'div.side-panel-wrap div.widget h4::text' ) . extract_first () for rating in response . css ( 'h3.rating-row' )[ 1 :]: yield { 'name' : name , 'title' : rating . css ( 'span.nocolor::text' ) . extract_first (), 'value' : rating . css ( 'span.rating span.score::text' ) . extract_first (), } Make sure the output file does not exist in the directory where the script is going to be executed. rm canadastop100.json Next we need to define the crawling processor with the following: process = CrawlerProcess ({ 'USER_AGENT' : 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process . crawl ( CompanySpider ) process . start () Executing this will give the following result: 2017 - 10 - 06 12 : 09 : 45 [ scrapy . utils . log ] INFO : Scrapy 1.4.0 started ( bot : scrapybot ) 2017 - 10 - 06 12 : 09 : 45 [ scrapy . utils . log ] INFO : Overridden settings : {' USER_AGENT ' : ' Mozilla / 4.0 ( compatible ; MSIE 7.0 ; Windows NT 5.1 ) '} Preparing data import pandas as pd Read the output file from the scraper. df = pd . read_json ( 'canadastop100.json' ) df . head () name title value 0 Bell Canada Physical Workplace A+ 1 Bell Canada Work Atmosphere & Communications A 2 Bell Canada Financial Benefits & Compensation A 3 Bell Canada Health & Family-Friendly Benefits B 4 Bell Canada Vacation & Personal Time-Off B Get the unique names in the database. len ( df [ 'name' ] . unique ()) 101 Filter out the companies without a title. df = df [ df [ 'title' ] . notnull ()] The unique elements in the value column are given by df [ 'value' ] . unique () and result in array(['A+', 'A', 'B', 'B+', 'B-', 'A-', 'C+'], dtype=object) Lets map these values to a number to make it easier to work them in the dataset. Define the mapping: mapping = { 'A+' : 10 , 'A' : 9 , 'A-' : 8 , 'B+' : 7 , 'B' : 6 , 'B-' : 5 , 'C+' : 4 } and apply the mapping to the value column: df [ 'value' ] = df [ 'value' ] . map ( mapping ) Now we need to transpose the dataframe, since we want a matrix with the companies per row and the different scores as a column. df = df . pivot ( index = 'name' , columns = 'title' , values = 'value' ) We add a column to get the total score: df [ 'Total Score' ] = df . sum ( axis = 1 ) The dataframe has the following layout after adding the extra column: df . head () title Community Involvement Employee Engagement & Performance Financial Benefits & Compensation Health & Family-Friendly Benefits Physical Workplace Training & Skills Development Vacation & Personal Time-Off Work Atmosphere & Communications Total Score name 3M Canada Company 10 7 9 9 10 9 6 10 70 Aboriginal Peoples Television Network Inc. / APTN 9 6 7 9 7 9 9 9 65 Accenture Inc. 10 9 7 9 7 7 6 9 64 Agrium Inc. 10 7 7 6 10 10 8 9 67 Air Canada 10 6 9 7 9 10 4 6 61 As a last step we need to attach the dataframe to the body of the notebook by using some JavaScript. We import the proper libraries from IPython.display import HTML , Javascript , display and attach the dataframe, after converting, to the window. Javascript ( \"\"\"§ window.companyData= {} ; \"\"\" . format ( df . reset_index () . to_json ( orient = 'records' ))) <IPython.core.display.Javascript object> Write to JSON file on disk if you want. This can be used in turn to move to the server where the VueJS application will be deployed. df . reset_index () . to_json ( 'canadastop100.json' , orient = 'records' ) Visualizing data Next step is to visualize the data using VueJS. VueJS can be included from https://cdnjs.cloudflare.com/ajax/libs/vue/2.4.0/vue. This notebook will make use of the example of the grid-component from the official documentation to create a table representing the crawled data. Add the requirement to the notebook. %% javascript require . config ({ paths : { vue : \"https://cdnjs.cloudflare.com/ajax/libs/vue/2.4.0/vue\" } }); <IPython.core.display.Javascript object> Define the template for displaying the data in a table using the x-template script type and the VueJS syntax. %% html < script type = \"text/x-template\" id = \"data-template\" > < table class = \"canada\" > < thead > < tr > < th v - for = \"key in columns\" @click = \"sortBy(key)\" : class = \"{ active: sortKey == key }\" > {{ key | capitalize }} < span class = \"arrow\" : class = \"sortOrders[key] > 0 ? 'asc' : 'dsc'\" > </ span > </ th > </ tr > </ thead > < tbody > < tr v - for = \"entry in filteredData\" > < td v - for = \"key in columns\" > {{ entry [ key ]}} </ td > </ tr > </ tbody > </ table > </ script > Define the main HTML that contains the template we defined earlier. %% html < div id = \"vue-app\" > < form id = \"search\" > Search < input name = \"query\" v - model = \"searchQuery\" > </ form > < data - grid : data = \"gridData\" : columns = \"gridColumns\" : filter - key = \"searchQuery\" > </ data - grid > </ div > Search Initialize the VueJS application using Javascript by extracting the data from the window, attaching the component with the table for the data and creating a new Vue instance. %% javascript require ([ 'vue' ], function ( Vue ) { console . log ( Vue . version ); var companyData = window . companyData ; console . log ( JSON . stringify ( companyData )); Vue . component ( 'data-grid' , { template : '#data-template' , props : { data : Array , columns : Array , filterKey : String }, data : function () { var sortOrders = {} this . columns . forEach ( function ( key ) { sortOrders [ key ] = 1 }) return { sortKey : '' , sortOrders : sortOrders } }, computed : { filteredData : function () { var sortKey = this . sortKey var filterKey = this . filterKey && this . filterKey . toLowerCase () var order = this . sortOrders [ sortKey ] || 1 var data = this . data if ( filterKey ) { data = data . filter ( function ( row ) { return Object . keys ( row ) . some ( function ( key ) { return String ( row [ key ]) . toLowerCase () . indexOf ( filterKey ) > - 1 }) }) } if ( sortKey ) { data = data . slice () . sort ( function ( a , b ) { a = a [ sortKey ] b = b [ sortKey ] return ( a === b ? 0 : a > b ? 1 : - 1 ) * order }) } return data } }, filters : { capitalize : function ( str ) { return str . charAt ( 0 ) . toUpperCase () + str . slice ( 1 ) } }, methods : { sortBy : function ( key ) { this . sortKey = key this . sortOrders [ key ] = this . sortOrders [ key ] * - 1 } } }) var vueApp = new Vue ({ el : '#vue-app' , data : { searchQuery : '' , gridColumns : Object . keys ( companyData [ 0 ]), gridData : companyData } }) }); <IPython.core.display.Javascript object> Attach a style to make the table more attractive. %% html < style > table . canada { border : 2 px solid rgb ( 102 , 153 , 255 ); border - radius : 3 px ; background - color : #fff; } table . canada th { background - color : rgb ( 102 , 153 , 255 ); color : rgba ( 255 , 255 , 255 , 0.66 ); cursor : pointer ; - webkit - user - select : none ; - moz - user - select : none ; - ms - user - select : none ; user - select : none ; } table . canada td { background - color : #f9f9f9; } table . canada th , table . canada td { min - width : 120 px ; padding : 10 px 20 px ; } table . canada th . active { color : #fff; } table . canada th . active . arrow { opacity : 1 ; } . arrow { display : inline - block ; vertical - align : middle ; width : 0 ; height : 0 ; margin - left : 5 px ; opacity : 0.66 ; } . arrow . asc { border - left : 4 px solid transparent ; border - right : 4 px solid transparent ; border - bottom : 4 px solid #fff; } . arrow . dsc { border - left : 4 px solid transparent ; border - right : 4 px solid transparent ; border - top : 4 px solid #fff; } </ style > The result can also be tested on the jsfiddle that I have created. The source for the page can be found in my Vue repository and is visible on my bl.ocks.org . The notebook can be found on my Github and the final result is shown on this page.", "tags": "posts", "url": "using-vuejs-in-jupyter-notebook.html", "loc": "using-vuejs-in-jupyter-notebook.html" }, { "title": "Hadoop Experiment - Using Pig", "text": "Pig Using the Pig language , we can make a script to perform the MapReduce actions similar to the previous post . Note that I will be using the same CSV file as before. gamedata_01.pig gamedata = LOAD 'nesgamedata.csv' AS ( index : int , name : chararray , grade : chararray , publisher : chararray , reader_rating : chararray , number_of_votes : int , publish_year : int , total_grade : chararray ); DESCRIBE gamedata ; DUMP gamedata ; [ root@quickstart gamedata ] # pig -f gamedata_01.pig ... ( 269 ,Winter Games,12,Epyx,13,24,1987,12.96 ) ( 270 ,Wizards and Warriors,9,Rare,6,55,1987,6.053571428571429 ) ( 271 ,World Games,6,Epyx,9,8,1986,8.666666666666666 ) ( 272 ,Wrath of the Black Manta,7,Taito,6,31,1989,6.03125 ) ( 273 ,Wrecking Crew,10,Nintendo,8,18,1985,8.105263157894736 ) ( 274 ,Xevious,5,Namco,6,36,1988,5.972972972972973 ) ( 275 ,Xexyz,10,Hudson Soft,5,26,1989,5.185185185185185 ) ( 276 ,Yoshi,5,Nintendo,6,41,1992,5.976190476190476 ) ( 277 ,Yoshi ' s Cookie,5,Nintendo,7,23,1993,6.916666666666667 ) ( 278 ,Zanac,2,Pony,3,21,1986,2.9545454545454546 ) ( 279 ,Zelda II: The Adventure of Link,3,Nintendo,4,112,1989,3.9911504424778763 ) ( 280 ,Zelda, The Legend of,3,Nintendo,3,140,1986,3.0 ) ( 281 ,Zombie Nation,4,Kaze,8,26,1991,7.851851851851852 ) Now lets calculate the average rating given by users for each different rating given by the author of the website for all Nintendo games. gamedata_02.pig gamedata = LOAD 'nesgamedata.csv' AS ( index : int , name : chararray , grade : int , publisher : chararray , reader_rating : int , number_of_votes : int , publish_year : int , total_grade : float ); gamesNintendo = FILTER gamedata BY publisher == 'Nintendo' ; gamesRatings = GROUP gamesNintendo BY grade ; averaged = FOREACH gamesRatings GENERATE group as rating , AVG ( gamesNintendo . total_grade ) AS avgRating ; DUMP averaged ; Run the script on the Hadoop machine: [ root@quickstart gamedata ] # pig -f gamedata_02.pig ... ( 1 ,2.321279764175415 ) ( 2 ,3.3024109601974487 ) ( 3 ,3.7930258750915526 ) ( 4 ,3.0212767124176025 ) ( 5 ,5.381512546539307 ) ( 6 ,5.773015689849854 ) ( 7 ,6.020833492279053 ) ( 8 ,9.833333015441895 ) ( 9 ,6.624411582946777 ) ( 10 ,8.105262756347656 ) ( 12 ,8.070609092712402 ) ( 13 ,10.066511631011963 ) From this we can observe that on average the users do not really agree with the author on the ratings. Often the author gives higher grades to a game than the users.", "tags": "posts", "url": "hadoop-experiment-pig-scripting.html", "loc": "hadoop-experiment-pig-scripting.html" }, { "title": "Running Truffle in a Docker container", "text": "Introduction In my earlier post about creating a decentralized application using Truffle and Metamask I used my own machine to install all dependencies and start developing. Since I am switching to Docker for most of my projects because of the flexibility, reproducability and safety of containerized environments, I also convert the Truffle project to Docker. As an inspiration I studied the example made by Douglas von Kohorn. Docker Docker installation Make sure Docker and docker-compose are installed on the machine. jitsejan@ssdnodes-jj-kvm:~$ sudo apt-get install docker.io docker-compose jitsejan@ssdnodes-jj-kvm:~$ docker -v Docker version 1 .12.6, build 78d1802 jitsejan@ssdnodes-jj-kvm:~$ docker-compose -v docker-compose version 1 .8.0, build unknown TestRPC image First of all, we need to setup a test network to play around with the Truffle app that we are going to create. Since we do not want to use the official Ethereum blockchain network, we use testrpc as created by Tim Coulter. See Github for the official repository. jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ nano Dockerfile The Dockerfile contains the few steps that are needed to create the TestRPC image. It will retrieve the Linux distro with node installed, install the node module ethereumjs-testrpc and open up port 8545, which is the default port for testrpc. # Node image FROM node:latest # Maintainer MAINTAINER Jitse-Jan van Waterschoot <jitsejan@gmail.com> # Install the packages RUN npm install -g --save ethereumjs-testrpc # Expose port EXPOSE 8545 # Start TestRPC ENTRYPOINT [ \"testrpc\" ] From the Dockerfile we will create the image on the local machine and push it to Docker.io. jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker build . -t jitsejan/testrpc jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you dont have a Docker ID, head over to https://hub.docker.com to create one. Username ( jitsejan ) : Password: Login Succeeded jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker push jitsejan/testrpc jitsejan@ssdnodes-jj-kvm:~/docker/testrpc$ docker images | grep testrpc jitsejan/testrpc latest a5cae5578720 10 minutes ago 716 MB The image can now be found at hub.docker.com . Truffle image Next we need to create the environment where we can develop our Truffle application. We will use again a Dockerfile, but in this case we will install truffle . # Node image FROM node:latest # Maintainer MAINTAINER Jitse-Jan van Waterschoot <jitsejan@gmail.com> # Create code directory RUN mkdir /code # Set working directory WORKDIR /code # Install Truffle RUN npm install -g truffle We can build and push the image again, finally the image will be available locally to be used by Docker. Again I use the docker push to add the image to my hub.docker.com . jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker images | grep truffle-application jitsejan/truffle-application latest 1987e7928f6b 58 minutes ago 693 .3 MB Docker compose The docker-compose.yml contains the information to start the two images, map the ports and start the testrpc network. It will try to retrieve the two Docker images locally and otherwise retrieve them from hub.docker.com. The testrpc is started with host 0.0.0.0 in order for the truffle3 container to access the network. version : '2' services : testrpc : image : jitsejan/testrpc command : bash -c \"testrpc -h 0.0.0.0\" ports : - \"7000:8545\" truffle3 : image : jitsejan/truffle-application command : bash stdin_open : true tty : true ports : - \"7001:8080\" volumes : - ./:/code Docker start Once the docker-compose.yml is in place, we can start the testrpc and truffle3 container by running the following command: jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker-compose -f docker-compose.yml up -d and verify if both containers are running: jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cda7d1c5c2a0 jitsejan/truffle-application \"bash\" 6 minutes ago Up About a minute 0 .0.0.0:7001->8080/tcp truffleapplication_truffle3_1 36f0bf0d4d25 jitsejan/testrpc \"testrpc bash -c 'tes\" 9 minutes ago Up About a minute 0 .0.0.0:7000->8545/tcp truffleapplication_testrpc_1 Creating the app Now we connect to the Truffle machine and create the application. Versions of the tools jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ docker attach truffleapplication_truffle3_1 root@cda7d1c5c2a0:/code# npm -v 5 .3.0 root@cda7d1c5c2a0:/code# node -v v8.4.0 root@cda7d1c5c2a0:/code# truffle version Truffle v3.4.11 ( core: 3 .4.11 ) Solidity v0.4.15 ( solc-js ) Initialize a Truffle application Use the unbox function of Truffle to create a sample webpack application. root@cda7d1c5c2a0:/code# truffle unbox webpack root@cda7d1c5c2a0:/code# ls Dockerfile box-img-lg.png contracts migrations package-lock.json test truffle.js app box-img-sm.png docker-compose.yml node_modules package.json tmp-276YgmHxiGG8WL webpack.config.js Compile the contracts and migrate them to the network. root@cda7d1c5c2a0:/code# truffle compile Compiling ./contracts/ConvertLib.sol... Compiling ./contracts/MetaCoin.sol... Compiling ./contracts/Migrations.sol... Writing artifacts to ./build/contracts root@cda7d1c5c2a0:/code# truffle migrate Could not connect to your Ethereum client. Please check that your Ethereum client: - is running - is accepting RPC connections ( i.e., \"--rpc\" option is used in geth ) - is accessible over the network - is properly configured in your Truffle configuration file ( truffle.js ) In order to be able to migrate, we need to modify truffle.js . It is easier to change this is on the host machine instead of the Docker machine, since most probably there is a text editor available. jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ ls app box-img-sm.png contracts Dockerfile node_modules package-lock.json tmp-276YgmHxiGG8WL webpack.config.js box-img-lg.png build docker-compose.yml migrations package.json test truffle.js Lets edit the truffle.js and update the network settings. jitsejan @ssdnodes - jj - kvm : ~/ docker / truffle - application $ sudo nano truffle . js Change the host from localhost to testrpc, since that is the name we assigned in our Docker setup. // Allows us to use ES6 in our migrations and tests. require ( 'babel-register' ) module . exports = { networks : { development : { host : 'testrpc' , port : 8545 , network_id : '*' // Match any network id } } } With the updated network settings, lets try to migrate the contracts again to the network. root@cda7d1c5c2a0:/code# truffle migrate Using network 'development' . Running migration: 1_initial_migration.js Deploying Migrations... ... 0x14946649e8489d36a12a5bbaeb61a306b6e2e52c9f33ff4d56d40f6f1472640c Migrations: 0x32e2f019bbdea7786e9b1d1c577f14bac8693464 Saving successful migration to network... ... 0x16bfd6088ce282fa7859de9b347e3cdefc3536ad987b6c0dbe12c37dda31be57 Saving artifacts... Running migration: 2_deploy_contracts.js Deploying ConvertLib... ... 0xba333153ec8f1e5c0d51455d4dbf58e50e8cd20f9a1b7176dd846fce24aa336c ConvertLib: 0x97063aa24f9913c98c76f93f64d30a2d5475a7df Linking ConvertLib to MetaCoin Deploying MetaCoin... ... 0xa11480d9333044537a8a0e1798f6afa33ad74a80f739982e55bf02ae6a3915f2 MetaCoin: 0x749764ff660f2572405f3a1674488077666b866a Saving successful migration to network... ... 0x6d837a4071eb419f31fe055f0278889382af9fc010552ced0d8060f584054835 Saving artifacts... Now we need to build and deploy the application. root@cda7d1c5c2a0:/code# npm run build root@cda7d1c5c2a0:/code# npm run dev This will output that the application is running on localhost:8080, but visiting this link shows an error. We need to modify the settings for webpack and add the host for the development server. Change package.json jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ sudo nano package.json and change the following line \"dev\" : \"webpack-dev-server\" to \"dev\" : \"webpack-dev-server --host 0.0.0.0\" to make the server accept external traffic. Run again root@d546db65c693:/code# npm run dev and now the application is served on http://0.0.0.0:8080/. Visit the IP of the machine on which you are developing on port 7001 (remember we mapped the port from 8080 to 7001 in the docker-compose file) and the MetaCoin application should be visible. However, an error pops up. There was an error fetching your accounts. Why? Because the Truffle app cannot connect to the Ethereum test network yet. In order for the connection to work, we need to modify the code of the app.js inside the Truffle application and change the IP for the Web3 client to the external IP, and the port to 7000 since we mapped port 8545 to 7000 in docker-compose. Open the app.js with nano jitsejan@ssdnodes-jj-kvm:~/docker/truffle-application$ sudo nano app/javascripts/app.js and change the line window.web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://localhost:8545\" )) ; to window.web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://EXTERNAL_IP:7000\" )) ; replacing the EXTERNAL_IP with the IP of the development machine where the Docker container with testrpc is running. Again run root@d546db65c693:/code# npm run dev and this time no error should pop up when the page is visited. If everything went find you should see the MetaCoin application with 10000 META. Hopefully this can serve as a base to create your own Dapp using Truffle. My final code can be found on my Github .", "tags": "posts", "url": "truffle-in-docker.html", "loc": "truffle-in-docker.html" }, { "title": "Hadoop Experiment - Spark with Pyspark in a Jupyter notebook", "text": "Docker setup I will use the Docker image from Jupyter. It contains Spark and Jupyter and makes developing and testing pyspark very easy. The Dockerfile will retrieve the Jupyter pyspark notebook image, add the Python requirements file and install the dependencies. It will start the Notebook server using Jupyter Lab on the given port. The resulting image can be found on my Docker repo . # Dockerfile FROM jupyter/pyspark-notebook ADD requirements.txt ./ RUN pip install -r requirements.txt CMD [\"start.sh\", \"jupyter\", \"lab\", \"--notebook-dir=/opt/notebooks\", \"--ip='*'\", \"--no-browser\", \"--allow-root\", \"--port=8559\"] To start the container, I use the following docker-compose.yml version: '2' services: pyspark: image: jitsejan/pyspark volumes: - ./notebooks:/opt/notebooks - ./data:/opt/data ports: - \"8559:8559\" Using Pyspark from pyspark import SparkConf , SparkContext import collections Configure the Spark connection conf = SparkConf () . setMaster ( \"local\" ) . setAppName ( \"GameRatings\" ) sc = SparkContext ( conf = conf ) Verify that the Spark context is working by creating a random RDD of 1000 values and pick 5 values. rdd = sc . parallelize ( range ( 1000 )) rdd . takeSample ( False , 5 ) [820, 967, 306, 62, 448] Next we can create an RDD from the data from the previous Hadoop notebook. lines = sc.textFile(\"../data/nesgamedata.csv\") Experiment one Lets calculate the average rating of the voters compared to the votes of the author. def parseLine ( line ): fields = line . split ( ' \\t ' ) index = int ( fields [ 0 ]) name = fields [ 1 ] grade = float ( fields [ 2 ]) publisher = fields [ 3 ] reader_rating = float ( fields [ 4 ]) number_of_votes = int ( fields [ 5 ]) publish_year = int ( fields [ 6 ]) total_grade = float ( fields [ 7 ]) return ( grade , total_grade , name , publisher , reader_rating , number_of_votes , publish_year ) We return the grade and the total grade as a tuple from the parseLine function. games_rdd = lines . map ( parseLine ) games_rdd . take ( 2 ) [(12.0, 10.044444444444444, '10-Yard Fight', 'Nintendo', 10.0, 44, 1985), (11.0, 8.044776119402986, '1942', 'Capcom', 8.0, 66, 1985)] Add a 1 to each line so we can sum the total_grades . games_mapped = games_rdd . mapValues ( lambda x : ( x , 1 )) games_mapped . take ( 2 ) [(12.0, (10.044444444444444, 1)), (11.0, (8.044776119402986, 1))] Sum all total_grades by using the key grade . For each row this will sum the grades and it will sum the 1's that we've added. games_reduced = games_mapped . reduceByKey ( lambda x , y : ( x [ 0 ] + y [ 0 ], x [ 1 ] + y [ 1 ])) games_reduced . take ( 2 ) [(12.0, (70.26902777777778, 7)), (11.0, (193.88550134591918, 25))] Calculate the average total_grade for each grade . average_grade = games_reduced . mapValues ( lambda x : x [ 0 ] / x [ 1 ]) results = average_grade . collect () for result in results : print ( result ) (12.0, 10.03843253968254) (11.0, 7.755420053836767) (5.0, 5.05270714136425) (13.0, 10.26375094258694) (7.0, 6.212705308155384) (4.0, 4.909347517549459) (8.0, 7.0328696656678105) (9.0, 6.519739721431783) (6.0, 5.63766311790758) (2.0, 3.495086595355065) (3.0, 4.1090931800649315) (10.0, 6.9165990377786555) (1.0, 2.321295734457781) Experiment two Filter out all Nintendo games, where the publisher is the 4-th element in the row. nintendoGames = games_rdd . filter ( lambda x : 'Nintendo' in x [ 3 ]) nintendoGames . take ( 2 ) [(12.0, 10.044444444444444, '10-Yard Fight', 'Nintendo', 10.0, 44, 1985), (5.0, 4.014705882352941, 'Balloon Fight', 'Nintendo', 4.0, 67, 1984)] Take the year and the total grade. nintendoYears = nintendoGames . map ( lambda x : ( x [ - 1 ], x [ 1 ])) nintendoYears . take ( 2 ) [(1985, 10.044444444444444), (1984, 4.014705882352941)] Calculate the minimum grade for each year. minYears = nintendoYears . reduceByKey ( lambda x , y : min ( x , y )) results = minYears . collect () for result in results : print ( 'Year: {:d} \\t Minimum score: {:.2f} ' . format ( result [ 0 ] , result [ 1 ])) Year : 1985 Minimum score : 2.00 Year : 1984 Minimum score : 3.96 Year : 1991 Minimum score : 2.98 Year : 1990 Minimum score : 2.00 Year : 1989 Minimum score : 3.99 Year : 1988 Minimum score : 3.99 Year : 1986 Minimum score : 2.98 Year : 1987 Minimum score : 1.99 Year : 1983 Minimum score : 6.02 Year : 1992 Minimum score : 5.98 Year : 1993 Minimum score : 6.92 Lets try using FlatMap to count the most occurring words in the titles of the NES games. words = games_rdd . flatMap ( lambda x : x [ 2 ] . split ()) words . take ( 10 ) ['10-Yard', 'Fight', '1942', '1943', '720', 'Degrees', '8', 'Eyes', 'Abadox', 'Adventure'] Now we count the words and sort by count. wordCounts = words . map ( lambda x : ( x , 1 )) . reduceByKey ( lambda x , y : x + y ) wordCountsSorted = wordCounts . map ( lambda x : ( x [ 1 ], x [ 0 ])) . sortByKey () results = wordCountsSorted . collect () for count , word in reversed ( results ): print ( word , count ) The 20 of 17 Super 11 the 11 Baseball 9 and 8 2 8 Ninja 7 Man 7 II 7 Mega 6 3 6 Dragon 5 Adventure 5 Tecmo 4 Spy 4 version) 4 .... This will result in a list of words with weird characters, spaces and other unwanted content. The text can be filtered in the flatMap function. import re def normalizeWords ( text ): \"\"\" Remove unwanted text \"\"\" return re . compile ( r '\\W+' , re . UNICODE ) . split ( text [ 2 ] . lower ()) words_normalized = games_rdd . flatMap ( normalizeWords ) wordNormCounts = words_normalized . countByValue () for word , count in sorted ( wordNormCounts . items (), key = lambda x : x [ 1 ], reverse = True ): if word . encode ( 'ascii' , 'ignore' ): print ( word , count ) the 31 of 17 2 12 ii 11 super 11 baseball 9 s 9 and 8 3 7 man 7 ninja 7 dragon 6 mega 6 adventure 5 adventures 4 n 4 donkey 4 kong 4 mario 4 monster 4 warriors 4 version 4 .... We can of course improve the normalize function and use NLTK or any other language processing library to clean up the stopwords, verbs and other undesired words in the text. The notebook can be found here .", "tags": "posts", "url": "hadoop-experiment-spark-with-pyspark-in-jupyter.html", "loc": "hadoop-experiment-spark-with-pyspark-in-jupyter.html" }, { "title": "Hadoop Experiment - MapReduce on Cloudera", "text": "In this example I will extract data with NES reviews from http://videogamecritic.com. I will create a dataframe, add some extra fields and save the data to a CSV-file. This file will be used for a simple MapReduce script. Note: I have a Docker container running with Selenium instead of installing all dependencies on my system. See this page . Extract script The script below is used to retrieve the data. It is ugly code, but it is doing the job. import lxml.html import requests from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities import pandas as pd URL = 'http://videogamecritic.com/nes.htm' def map_rating ( rating ): \"\"\" Function to convert the rating to a number \"\"\" if ( rating == \"A+\" ): return 1 ; if ( rating == \"A\" ): return 2 ; if ( rating == \"A-\" ): return 3 ; if ( rating == \"B+\" ): return 4 ; if ( rating == \"B\" ): return 5 ; if ( rating == \"B-\" ): return 6 ; if ( rating == \"C+\" ): return 7 ; if ( rating == \"C\" ): return 8 ; if ( rating == \"C-\" ): return 9 ; if ( rating == \"D+\" ): return 10 ; if ( rating == \"D\" ): return 11 ; if ( rating == \"D-\" ): return 12 ; if ( rating == \"F\" ): return 13 ; if ( rating == \"F-\" ): return 14 ; return 15 ; resp = requests . get ( URL ) if resp . status_code != 200 : raise Exception ( 'GET ' + link + ' {} ' . format ( resp . status_code )) tree = lxml . html . fromstring ( resp . content ) gamepages = [ 'http://videogamecritic.com/' + game . get ( 'href' ) for game in tree . cssselect ( 'h3 a' )] driver = webdriver . Remote ( \"http://localhost:4444/wd/hub\" , DesiredCapabilities . CHROME ) gamedata = [] for page in gamepages : # Retrieve the data driver . get ( page ) data = lxml . html . fromstring ( driver . page_source ) # Extract the fields grades = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' hdr \\' ]' )] names = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' hdl \\' ]' )] metadata = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' mdl \\' ]' )] votes = [ elem . text_content () for elem in data . cssselect ( 'div[class*= \\' vote \\' ]' )] # Append to dataset gamedata += list ( zip ( names , votes , grades , metadata )) driver . quit () ## # DataFrame magic ## df = pd . DataFrame . from_dict ( gamedata ) df = df . rename ( columns = { 0 : \"name\" , 1 : \"vote\" , 2 : \"grade\" , 3 : \"publisher\" }) # Extract and convert data df [ 'reader_rating' ] = df [ 'vote' ] . str . extract ( 'Readers:\\s(.*?)\\s\\(' , expand = True ) df [ 'reader_rating' ] = df [ 'reader_rating' ] . apply ( lambda x : map_rating ( x )) . astype ( 'int' ) df [ 'number_of_votes' ] = df [ 'vote' ] . str . extract ( '\\((\\d*)\\svotes\\)' , expand = True ) . astype ( 'int' ) df [ 'grade' ] = df [ 'grade' ] . str . replace ( \"Grade:\" , \"\" ) . str . strip () . apply ( lambda x : map_rating ( x )) . astype ( 'int' ) df [ 'publish_year' ] = df [ 'publisher' ] . str . extract ( '\\((\\d*)\\)Reviewed' , expand = True ) df [ 'publisher' ] = df [ 'publisher' ] . str . extract ( \"Publisher:\\s(.*?)\\s\\(\" , expand = True ) df . drop ( 'vote' , axis = 1 , inplace = True ) # Calculate the total grade df [ 'total_grade' ] = ( df [ 'grade' ] + df [ 'reader_rating' ] * df [ 'number_of_votes' ]) / ( df [ 'number_of_votes' ] + 1 ) # Corrections df [ 'publisher' ] = df [ 'publisher' ] . str . replace ( 'Electrobrain' , 'Electro Brain' ) # Save to file df . to_csv ( 'nesgamedata.csv' , sep = ' \\t ' , header = False ) This will result in the following file. 0 10 -Yard Fight 12 Nintendo 10 44 1985 10 .044444444444444 1 1942 11 Capcom 8 65 1985 8 .045454545454545 2 1943 5 Capcom 4 58 1988 4 .016949152542373 3 720 Degrees 13 Tengen 11 24 1989 11 .08 4 8 Eyes 13 Taxan 10 30 1989 10 .096774193548388 5 Abadox 11 Abadox 6 34 1989 6 .142857142857143 6 Adventure Island 7 Hudson Soft 6 63 1987 6 .015625 7 Adventure Island 2 4 Hudson Soft 5 40 1990 4 .975609756097561 8 Adventure Island 3 4 Hudson Soft 5 27 1992 4 .964285714285714 9 Adventures in the Magic Kingdom 5 Capcom 8 23 1990 7 .875 ... MapReduce script Calculate how often what rating is used. Map the reader ratings Reduce to counts per rating Lets create the script that will perform my first MapReduce action. The content of the following cell will be saved to a file which can in turn be used to perform the mapping and reducing. from mrjob.job import MRJob from mrjob.step import MRStep class GamesBreakdown ( MRJob ): def steps ( self ): return [ MRStep ( mapper = self . mapper_get_ratings , reducer = self . reducer_count_ratings ) ] def mapper_get_ratings ( self , _ , line ): ( index , name , grade , publisher , reader_rating , number_of_votes , publish_year , total_grade ) = line . split ( ' \\t ' ) yield reader_rating , 1 def reducer_count_ratings ( self , key , values ): yield key , sum ( values ) if __name__ == '__main__' : GamesBreakdown . run () Execution To use Hadoop, the command should look like the following and should be run on the Hadoop machine, with the hadoop-streaming-jar argument only given in case the .jar is not found: python gamesbreakdown.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar nesgamedata.csv To do this, upload the nesgamesdata.csv file and the script gamesbreakdown.py to HDFS, using the Hue files view and upload functionality. Next copy them to the local folder to be used in the command. Of course other methods to get the files locally on the Hadoop machine can be used. I am using the Cloudera Quickstart image to experiment with Docker and Cloudera and the docker-compose.yml contains the following: version: '2' services: cloudera: hostname: quickstart.cloudera command: /usr/bin/docker-quickstart tty: true privileged: true image: cloudera/quickstart:latest volumes: - ./data:/opt/data ports: - \"8020:8020\" - \"8022:22\" # ssh - \"7180:7180\" # Cloudera Manager - \"8888:8888\" # HUE - \"11000:11000\" # Oozie - \"50070:50070\" # HDFS REST Namenode - \"2181:2181\" - \"11443:11443\" - \"9090:9090\" - \"8088:8088\" - \"19888:19888\" - \"9092:9092\" - \"8983:8983\" - \"16000:16000\" - \"16001:16001\" - \"42222:22\" - \"8042:8042\" - \"60010:60010\" - \"8080:8080\" - \"7077:7077\" Connect to the machine and verify the file is present: jitsejan@ssdnodes-jj-kvm:~/cloudera_docker$ docker exec -ti clouderadocker_cloudera_1 bash [ root@quickstart / ] # cd home/ [ root@quickstart home ] # mkdir gamedata && cd $_ [ root@quickstart gamedata ] # hadoop fs -get gamedata/gamesbreakdown.py gamesbreakdown.py [ root@quickstart gamedata ] # hadoop fs -get gamedata/nesgamedata.csv nesgamedata.csv [ root@quickstart gamedata ] # ll total 20 -rw-r--r-- 1 root root 590 Sep 5 13 :42 gamesbreakdown.py -rw-r--r-- 1 root root 15095 Sep 5 13 :56 nesgamedata.csv Install the mrjob library on the Cloudera container. [ root@quickstart gamedata ] # yum install python-pip -y [ root@quickstart gamedata ] # pip install mrjob Run the script without using Hadoop to verify the installation. [ root@quickstart gamedata ] # python gamesbreakdown.py nesgamedata.csv No configs found ; falling back on auto-configuration Creating temp directory /tmp/gamesbreakdown.root.20170905.135809.859796 Running step 1 of 1 ... Streaming final output from /tmp/gamesbreakdown.root.20170905.135809.859796/output... \"10\" 15 \"11\" 14 \"12\" 12 \"13\" 4 \"2\" 9 \"3\" 28 \"4\" 44 \"5\" 34 \"6\" 48 \"7\" 31 \"8\" 28 \"9\" 15 Removing temp directory /tmp/gamesbreakdown.root.20170905.135809.859796... Now use Hadoop to start the cluster magic. [ root@quickstart gamedata ] # python gamesbreakdown.py -r hadoop nesgamedata.csv No configs found ; falling back on auto-configuration Looking for hadoop binary in $PATH ... Found hadoop binary: /usr/bin/hadoop Using Hadoop version 2 .6.0 Looking for Hadoop streaming jar in /home/hadoop/contrib... Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce... Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar Creating temp directory /tmp/gamesbreakdown.root.20170905.135908.241472 Copying local files to hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/files/... Running step 1 of 1 ... packageJobJar: [] [ /usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar ] /tmp/streamjob3556720292725669631.jar tmpDir = null Connecting to ResourceManager at /0.0.0.0:8032 Connecting to ResourceManager at /0.0.0.0:8032 Total input paths to process : 1 number of splits:2 Submitting tokens for job: job_1504618114531_0001 Submitted application application_1504618114531_0001 The url to track the job: http://quickstart.cloudera:8088/proxy/application_1504618114531_0001/ Running job: job_1504618114531_0001 Job job_1504618114531_0001 running in uber mode : false map 0 % reduce 0 % map 50 % reduce 0 % map 100 % reduce 0 % map 100 % reduce 100 % Job job_1504618114531_0001 completed successfully Output directory: hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/output Counters: 49 File Input Format Counters Bytes Read = 19191 File Output Format Counters Bytes Written = 86 File System Counters FILE: Number of bytes read = 2307 FILE: Number of bytes written = 358424 FILE: Number of large read operations = 0 FILE: Number of read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 19527 HDFS: Number of bytes written = 86 HDFS: Number of large read operations = 0 HDFS: Number of read operations = 9 HDFS: Number of write operations = 2 Job Counters Data-local map tasks = 2 Launched map tasks = 2 Launched reduce tasks = 1 Total megabyte-seconds taken by all map tasks = 8678400 Total megabyte-seconds taken by all reduce tasks = 3588096 Total time spent by all map tasks ( ms )= 8475 Total time spent by all maps in occupied slots ( ms )= 8475 Total time spent by all reduce tasks ( ms )= 3504 Total time spent by all reduces in occupied slots ( ms )= 3504 Total vcore-seconds taken by all map tasks = 8475 Total vcore-seconds taken by all reduce tasks = 3504 Map-Reduce Framework CPU time spent ( ms )= 2450 Combine input records = 0 Combine output records = 0 Failed Shuffles = 0 GC time elapsed ( ms )= 367 Input split bytes = 336 Map input records = 282 Map output bytes = 1737 Map output materialized bytes = 2313 Map output records = 282 Merged Map outputs = 2 Physical memory ( bytes ) snapshot = 651063296 Reduce input groups = 12 Reduce input records = 282 Reduce output records = 12 Reduce shuffle bytes = 2313 Shuffled Maps = 2 Spilled Records = 564 Total committed heap usage ( bytes )= 679477248 Virtual memory ( bytes ) snapshot = 4099473408 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 Streaming final output from hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472/output... \"10\" 15 \"11\" 14 \"12\" 12 \"13\" 4 \"2\" 9 \"3\" 28 \"4\" 44 \"5\" 34 \"6\" 48 \"7\" 31 \"8\" 28 \"9\" 15 Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/gamesbreakdown.root.20170905.135908.241472... Removing temp directory /tmp/gamesbreakdown.root.20170905.135908.241472... Now lets add another reducer step to sort the counts of the ratings. from mrjob.job import MRJob from mrjob.step import MRStep class GamesBreakdownUpdate ( MRJob ): def steps ( self ): return [ MRStep ( mapper = self . mapper_get_ratings , reducer = self . reducer_count_ratings ), MRStep ( reducer = self . reducer_sorted_output ) ] def mapper_get_ratings ( self , _ , line ): ( index , name , grade , publisher , reader_rating , number_of_votes , publish_year , total_grade ) = line . split ( ' \\t ' ) yield reader_rating , 1 def reducer_count_ratings ( self , key , values ): yield str ( sum ( values )) . zfill ( 2 ), key def reducer_sorted_output ( self , count , ratings ): for rating in ratings : yield rating , count if __name__ == '__main__' : GamesBreakdownUpdate . run () [ root@quickstart gamedata ] # python gamesbreakdownupdate.py nesgamedata.csv No configs found ; falling back on auto-configuration Creating temp directory /tmp/gamesbreakdownupdate.root.20170905.142738.885314 Running step 1 of 2 ... Running step 2 of 2 ... Streaming final output from /tmp/gamesbreakdownupdate.root.20170905.142738.885314/output... \"13\" \"04\" \"2\" \"09\" \"12\" \"12\" \"11\" \"14\" \"10\" \"15\" \"9\" \"15\" \"3\" \"28\" \"8\" \"28\" \"7\" \"31\" \"5\" \"34\" \"4\" \"44\" \"6\" \"48\" Removing temp directory /tmp/gamesbreakdownupdate.root.20170905.142738.885314... That concludes my first experiments with Hadoop and MapReduce. Next step: using Pig or Spark to calculate similar statistics without all the overhead.", "tags": "posts", "url": "hadoop-experiment-mapreduce-on-cloudera.html", "loc": "hadoop-experiment-mapreduce-on-cloudera.html" }, { "title": "Getting started with the Hortonworks Hadoop Sandbox", "text": "Download the HDP Docker image from Hortonworks. jitsejan@ssdnodes-jj-kvm:~/downloads$ wget https://downloads-hortonworks.akamaized.net/sandbox-hdp-2.6.1/HDP_2_6_1_docker_image_28_07_2017_14_42_40.tar Load the Docker image from the TAR-file. jitsejan@ssdnodes-jj-kvm:~/downloads$ docker load -i HDP_2_6_1_docker_image_28_07_2017_14_42_40.tar jitsejan@ssdnodes-jj-kvm:~/downloads$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE anaconda3docker_anaconda latest 4faa1524bf2d 18 hours ago 3 .397 GB sandbox-hdp latest c3cef4760133 4 weeks ago 12 .2 GB continuumio/anaconda3 latest f3a9cb1bc160 12 weeks ago 2 .317 GB Download and run the start-up script. jitsejan@ssdnodes-jj-kvm:~/downloads$ wget https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh jitsejan@ssdnodes-jj-kvm:~/downloads$ chmod +x start_sandbox-hdp.sh jitsejan@ssdnodes-jj-kvm:~/downloads$ ./start_sandbox-hdp.sh Verify the container is started. jitsejan@ssdnodes-jj-kvm:~/downloads$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26bdf90de81b sandbox-hdp \"/usr/sbin/sshd -D\" About an hour ago Up About an hour 0 .0.0.0:1000->1000/tcp, 0 .0.0.0:1100->1100/tcp, 0 .0.0.0:1220->1220/tcp, 0 .0.0.0:1988->1988/tcp, 0 .0.0.0:2049->2049/tcp, 0 .0.0.0:2100->2100/tcp, 0 .0.0.0:2181->2181/tcp, 0 .0.0.0:3000->3000/tcp, 0 .0.0.0:4040->4040/tcp, 0 .0.0.0:4200->4200/tcp, 0 .0.0.0:4242->4242/tcp, 0 .0.0.0:5007->5007/tcp, 0 .0.0.0:5011->5011/tcp, 0 .0.0.0:6001->6001/tcp, 0 .0.0.0:6003->6003/tcp, 0 .0.0.0:6008->6008/tcp, 0 .0.0.0:6080->6080/tcp, 0 .0.0.0:6188->6188/tcp, 0 .0.0.0:8000->8000/tcp, 0 .0.0.0:8005->8005/tcp, 0 .0.0.0:8020->8020/tcp, 0 .0.0.0:8032->8032/tcp, 0 .0.0.0:8040->8040/tcp, 0 .0.0.0:8042->8042/tcp, 0 .0.0.0:8080->8080/tcp, 0 .0.0.0:8082->8082/tcp, 0 .0.0.0:8086->8086/tcp, 0 .0.0.0:8088->8088/tcp, 0 .0.0.0:8090-8091->8090-8091/tcp, 0 .0.0.0:8188->8188/tcp, 0 .0.0.0:8443->8443/tcp, 0 .0.0.0:8744->8744/tcp, 0 .0.0.0:8765->8765/tcp, 0 .0.0.0:8886->8886/tcp, 0 .0.0.0:8888-8889->8888-8889/tcp, 0 .0.0.0:8983->8983/tcp, 0 .0.0.0:8993->8993/tcp, 0 .0.0.0:9000->9000/tcp, 0 .0.0.0:9995-9996->9995-9996/tcp, 0 .0.0.0:10000-10001->10000-10001/tcp, 0 .0.0.0:10015-10016->10015-10016/tcp, 0 .0.0.0:10500->10500/tcp, 0 .0.0.0:10502->10502/tcp, 0 .0.0.0:11000->11000/tcp, 0 .0.0.0:15000->15000/tcp, 0 .0.0.0:15002->15002/tcp, 0 .0.0.0:15500-15505->15500-15505/tcp, 0 .0.0.0:16000->16000/tcp, 0 .0.0.0:16010->16010/tcp, 0 .0.0.0:16020->16020/tcp, 0 .0.0.0:16030->16030/tcp, 0 .0.0.0:18080-18081->18080-18081/tcp, 0 .0.0.0:19888->19888/tcp, 0 .0.0.0:21000->21000/tcp, 0 .0.0.0:33553->33553/tcp, 0 .0.0.0:39419->39419/tcp, 0 .0.0.0:42111->42111/tcp, 0 .0.0.0:50070->50070/tcp, 0 .0.0.0:50075->50075/tcp, 0 .0.0.0:50079->50079/tcp, 0 .0.0.0:50095->50095/tcp, 0 .0.0.0:50111->50111/tcp, 0 .0.0.0:60000->60000/tcp, 0 .0.0.0:60080->60080/tcp, 0 .0.0.0:2222->22/tcp, 0 .0.0.0:1111->111/tcp sandbox-hdp Login to the machine and run the first Hadoop command. jitsejan@ssdnodes-jj-kvm:~/downloads$ ssh 127 .0.0.1 -p 2222 -l maria_dev [ maria_dev@sandbox ~ ] $ hadoop fs -ls Found 2 items drwxr-xr-x - maria_dev hdfs 0 2017 -09-01 09 :29 .Trash drwxr-xr-x - maria_dev hdfs 0 2017 -09-01 08 :16 hive", "tags": "posts", "url": "getting-started-with-hortonworks-sandbox.html", "loc": "getting-started-with-hortonworks-sandbox.html" }, { "title": "Using Anaconda with Docker", "text": "Install Docker and add user jitsejan@ssdnodes-jj-kvm:~$ sudo apt install docker.io docker-compose -y jitsejan@ssdnodes-jj-kvm:~$ sudo usermod -aG docker $USER Create the folder structure jitsejan@ssdnodes-jj-kvm:~/anaconda3_docker$ tree . ├── data ├── docker-compose.yml ├── Dockerfile ├── notebooks ├── README.md └── requirements.txt The data folder will contain input and output data for the notebooks. The notebooks folder will contain the Jupyter notebooks. The Dockerfile will create the folders in the container and install the Python requirements from the requirements.txt. Content of Dockerfile: FROM continuumio/anaconda3 ADD requirements.txt / RUN pip install -r requirements.txt CMD [\"/opt/conda/bin/jupyter\", \"notebook\", \"--notebook-dir=/opt/notebooks\", \"--ip='*'\", \"--no-browser\", \"--allow-root\"] Content of docker-compose.yml: version: '2' services: anaconda: build: . volumes: - ./notebooks:/opt/notebooks ports: - \"8888:8888\" Start the container jitsejan@ssdnodes-jj-kvm:~/anaconda3_docker$ docker-compose up --build Action Go to your IP-address on the given port and start coding. My final notebook setup can be found on my Github .", "tags": "posts", "url": "using-anaconda-with-docker.html", "loc": "using-anaconda-with-docker.html" }, { "title": "Using Scrapy in Jupyter notebook", "text": "This notebook makes use of the Scrapy library to scrape data from a website. Following the basic example, we create a QuotesSpider and call the CrawlerProcess with this spider to retrieve quotes from http://quotes.toscrape.com . In this notebook two pipelines are defined, both writing results to a JSON file. The first option is to create a separate class that defines the pipeline and explicitly has the functions to write to a file per found item. It enables more flexibility when dealing with stranger data formats, or if you want to setup a custom way of writing items to file. The pipeline is set in the custom_settings parameter ITEM_PIPELINES inside the QuoteSpider class. However, I simply want to write the list of items that are found in the spider to a JSON file and therefor it is easier to choose the second option, where only the FEED_FORMAT has to be set to JSON and the output file needs to be defined in FEED_URI inside the custom settings of the spider. No additional classes or definitions need to be created, making the FEED_FORMAT/FEED_URI a convenient option. Once the quotes are retrieved the JSON file will be created on disk and can be loaded to a Pandas dataframe. This dataframe can then be analyzed, modified and be used for further processing. This notebook simply loads the JSON file to a dataframe and writes it again to a pickle. In [1]: # Settings for notebook from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" # Show Python version import platform platform . python_version () Out[1]: '3.6.1' Import Scrapy In [2]: try : import scrapy except : ! pip install scrapy import scrapy from scrapy.crawler import CrawlerProcess Setup a pipeline This class creates a simple pipeline that writes all found items to a JSON file, where each line contains one JSON element. In [3]: import json class JsonWriterPipeline ( object ): def open_spider ( self , spider ): self . file = open ( 'quoteresult.jl' , 'w' ) def close_spider ( self , spider ): self . file . close () def process_item ( self , item , spider ): line = json . dumps ( dict ( item )) + \" \\n \" self . file . write ( line ) return item Define the spider The QuotesSpider class defines from which URLs to start crawling and which values to retrieve. I set the logging level of the crawler to warning, otherwise the notebook is overloaded with DEBUG messages about the retrieved data. In [4]: import logging class QuotesSpider ( scrapy . Spider ): name = \"quotes\" start_urls = [ 'http://quotes.toscrape.com/page/1/' , 'http://quotes.toscrape.com/page/2/' , ] custom_settings = { 'LOG_LEVEL' : logging . WARNING , 'ITEM_PIPELINES' : { '__main__.JsonWriterPipeline' : 1 }, # Used for pipeline 1 'FEED_FORMAT' : 'json' , # Used for pipeline 2 'FEED_URI' : 'quoteresult.json' # Used for pipeline 2 } def parse ( self , response ): for quote in response . css ( 'div.quote' ): yield { 'text' : quote . css ( 'span.text::text' ) . extract_first (), 'author' : quote . css ( 'span small::text' ) . extract_first (), 'tags' : quote . css ( 'div.tags a.tag::text' ) . extract (), } Start the crawler In [5]: process = CrawlerProcess ({ 'USER_AGENT' : 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process . crawl ( QuotesSpider ) process . start () 2017-08-02 15:22:02 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot) 2017-08-02 15:22:02 [scrapy.utils.log] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'} Out[5]: <Deferred at 0x7f8b9a41c7b8> Check the files Verify that the files has been created on disk. As we can observe the files are both created and have data. The .jl file has line separated JSON elements, while the .json file has one big JSON array containing all the quotes. In [6]: ll quoteresult .* -rw-rw-r-- 1 jitsejan 5551 Aug 2 15:22 quoteresult.jl -rw-rw-r-- 1 jitsejan 5573 Aug 2 15:22 quoteresult.json In [7]: ! tail -n 2 quoteresult.jl {\"text\": \"\\u201cA woman is like a tea bag; you never know how strong it is until it's in hot water.\\u201d\", \"author\": \"Eleanor Roosevelt\", \"tags\": [\"misattributed-eleanor-roosevelt\"]} {\"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\", \"author\": \"Steve Martin\", \"tags\": [\"humor\", \"obvious\", \"simile\"]} In [8]: ! tail -n 2 quoteresult.json {\"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\", \"author\": \"Steve Martin\", \"tags\": [\"humor\", \"obvious\", \"simile\"]} ] Create dataframes Pandas can now be used to create dataframes and save the frames to pickles. The .sjon file can be loaded directly into a frame, whereas for the .jl file we need to specify the JSON objects are divided per line. In [9]: import pandas as pd dfjson = pd . read_json ( 'quoteresult.json' ) dfjson Out[9]: author tags text 0 Marilyn Monroe [friends, heartbreak, inspirational, life, lov... \"This life is what you make it. No matter what... 1 J.K. Rowling [courage, friends] \"It takes a great deal of bravery to stand up ... 2 Albert Einstein [simplicity, understand] \"If you can't explain it to a six year old, yo... 3 Bob Marley [love] \"You may not be her first, her last, or her on... 4 Dr. Seuss [fantasy] \"I like nonsense, it wakes up the brain cells.... 5 Douglas Adams [life, navigation] \"I may not have gone where I intended to go, b... 6 Elie Wiesel [activism, apathy, hate, indifference, inspira... \"The opposite of love is not hate, it's indiff... 7 Friedrich Nietzsche [friendship, lack-of-friendship, lack-of-love,... \"It is not a lack of love, but a lack of frien... 8 Mark Twain [books, contentment, friends, friendship, life] \"Good friends, good books, and a sleepy consci... 9 Allen Saunders [fate, life, misattributed-john-lennon, planni... \"Life is what happens to us while we are makin... 10 Albert Einstein [change, deep-thoughts, thinking, world] \"The world as we have created it is a process ... 11 J.K. Rowling [abilities, choices] \"It is our choices, Harry, that show what we t... 12 Albert Einstein [inspirational, life, live, miracle, miracles] \"There are only two ways to live your life. On... 13 Jane Austen [aliteracy, books, classic, humor] \"The person, be it gentleman or lady, who has ... 14 Marilyn Monroe [be-yourself, inspirational] \"Imperfection is beauty, madness is genius and... 15 Albert Einstein [adulthood, success, value] \"Try not to become a man of success. Rather be... 16 André Gide [life, love] \"It is better to be hated for what you are tha... 17 Thomas A. Edison [edison, failure, inspirational, paraphrased] \"I have not failed. I've just found 10,000 way... 18 Eleanor Roosevelt [misattributed-eleanor-roosevelt] \"A woman is like a tea bag; you never know how... 19 Steve Martin [humor, obvious, simile] \"A day without sunshine is like, you know, nig... In [10]: dfjl = pd . read_json ( 'quoteresult.jl' , lines = True ) dfjl Out[10]: author tags text 0 Marilyn Monroe [friends, heartbreak, inspirational, life, lov... \"This life is what you make it. No matter what... 1 J.K. Rowling [courage, friends] \"It takes a great deal of bravery to stand up ... 2 Albert Einstein [simplicity, understand] \"If you can't explain it to a six year old, yo... 3 Bob Marley [love] \"You may not be her first, her last, or her on... 4 Dr. Seuss [fantasy] \"I like nonsense, it wakes up the brain cells.... 5 Douglas Adams [life, navigation] \"I may not have gone where I intended to go, b... 6 Elie Wiesel [activism, apathy, hate, indifference, inspira... \"The opposite of love is not hate, it's indiff... 7 Friedrich Nietzsche [friendship, lack-of-friendship, lack-of-love,... \"It is not a lack of love, but a lack of frien... 8 Mark Twain [books, contentment, friends, friendship, life] \"Good friends, good books, and a sleepy consci... 9 Allen Saunders [fate, life, misattributed-john-lennon, planni... \"Life is what happens to us while we are makin... 10 Albert Einstein [change, deep-thoughts, thinking, world] \"The world as we have created it is a process ... 11 J.K. Rowling [abilities, choices] \"It is our choices, Harry, that show what we t... 12 Albert Einstein [inspirational, life, live, miracle, miracles] \"There are only two ways to live your life. On... 13 Jane Austen [aliteracy, books, classic, humor] \"The person, be it gentleman or lady, who has ... 14 Marilyn Monroe [be-yourself, inspirational] \"Imperfection is beauty, madness is genius and... 15 Albert Einstein [adulthood, success, value] \"Try not to become a man of success. Rather be... 16 André Gide [life, love] \"It is better to be hated for what you are tha... 17 Thomas A. Edison [edison, failure, inspirational, paraphrased] \"I have not failed. I've just found 10,000 way... 18 Eleanor Roosevelt [misattributed-eleanor-roosevelt] \"A woman is like a tea bag; you never know how... 19 Steve Martin [humor, obvious, simile] \"A day without sunshine is like, you know, nig... In [11]: dfjson . to_pickle ( 'quotejson.pickle' ) dfjl . to_pickle ( 'quotejl.pickle' ) In [12]: ll * pickle -rw-rw-r-- 1 jitsejan 5676 Aug 2 15:22 quotejl.pickle -rw-rw-r-- 1 jitsejan 5676 Aug 2 15:22 quotejson.pickle", "tags": "posts", "url": "using-scrapy-in-jupyter-notebook.html", "loc": "using-scrapy-in-jupyter-notebook.html" }, { "title": "Extend dictionary cell to columns in Pandas dataframe", "text": "df = pd . concat ([ df . drop ([ 'meta' ], axis = 1 ), df [ 'meta' ] . apply ( pd . Series )], axis = 1 )", "tags": "posts", "url": "extend-dictionary-cell-in-pandas.html", "loc": "extend-dictionary-cell-in-pandas.html" }, { "title": "Using Python and Javascript together with Flask", "text": "Introduction In this project I am experimenting with sending data between Javascript and Python using the web framework Flask. Additionally I will use matplotlib to generate a dynamic graph based on the provided user input data. Key learning points Sending data from Python to Javascript Receiving data in Python from Javascript Creating an image dynamically using a special Flask route Important bits Send the outputData from Javascript to Python with a POST call to postmethod and use the form variable canvas_data. The POST call give a response from Python and the page is redirected to the results page with the given uuid. ... $ . post ( \"/postmethod\" , { canvas_data : JSON . stringify ( outputData ) }, function ( err , req , resp ){ window . location . href = \"/results/\" + resp [ \"responseJSON\" ][ \"uuid\" ]; }); ... Retrieve the canvas_data from the POST request and write the content to a file. Return the unique id that was used for writing to the file. ... @app . route ( '/postmethod' , methods = [ 'POST' ]) def post_javascript_data (): jsdata = request . form [ 'canvas_data' ] unique_id = create_csv ( jsdata ) params = { 'uuid' : unique_id } return jsonify ( params ) ... Implementation The core of the web application is inside this file. Here I define the different routes for the website and specify the settings. The default route shows the index.html where a canvas is shown. The result route will show the image once a picture is drawn, based on the provided unique ID. The postmethod route is defined to handle the data coming from Javascript into Python via a POST call. The content of the POST variable are written to a CSV file which can be used again on the result page where data is loaded from this same file. app.py from __future__ import print_function from flask import Flask , render_template , make_response from flask import redirect , request , jsonify , url_for import io import os import uuid from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas from matplotlib.figure import Figure import numpy as np app = Flask ( __name__ ) app . secret_key = 's3cr3t' app . debug = True app . _static_folder = os . path . abspath ( \"templates/static/\" ) @app . route ( '/' , methods = [ 'GET' ]) def index (): title = 'Create the input' return render_template ( 'layouts/index.html' , title = title ) @app . route ( '/results/<uuid>' , methods = [ 'GET' ]) def results ( uuid ): title = 'Result' data = get_file_content ( uuid ) return render_template ( 'layouts/results.html' , title = title , data = data ) @app . route ( '/postmethod' , methods = [ 'POST' ]) def post_javascript_data (): jsdata = request . form [ 'canvas_data' ] unique_id = create_csv ( jsdata ) params = { 'uuid' : unique_id } return jsonify ( params ) @app . route ( '/plot/<imgdata>' ) def plot ( imgdata ): data = [ float ( i ) for i in imgdata . strip ( '[]' ) . split ( ',' )] data = np . reshape ( data , ( 200 , 200 )) fig = Figure () axis = fig . add_subplot ( 1 , 1 , 1 ) axis . axis ( 'off' ) axis . imshow ( data , interpolation = 'nearest' ) canvas = FigureCanvas ( fig ) output = io . BytesIO () canvas . print_png ( output ) response = make_response ( output . getvalue ()) response . mimetype = 'image/png' return response def create_csv ( text ): unique_id = str ( uuid . uuid4 ()) with open ( 'images/' + unique_id + '.csv' , 'a' ) as file : file . write ( text [ 1 : - 1 ] + \" \\n \" ) return unique_id def get_file_content ( uuid ): with open ( 'images/' + uuid + '.csv' , 'r' ) as file : return file . read () if __name__ == '__main__' : app . run ( host = '0.0.0.0' , port = 5000 ) The second part of the magic happens in the Javascript file. In this file a canvas is generated and added to the DOM. The mouse is used to draw dots on the canvas with a predefined color and radius. One button is used to send the data of the current drawing on the canvas and another one is used to clear the canvas. templates/static/js/script.js $ ( document ). ready ( function () { function createCanvas ( parent , width , height ) { var canvas = document . getElementById ( \"inputCanvas\" ); canvas . context = canvas . getContext ( '2d' ); return canvas ; } function init ( container , width , height , fillColor ) { var canvas = createCanvas ( container , width , height ); var ctx = canvas . context ; ctx . fillCircle = function ( x , y , radius , fillColor ) { this . fillStyle = fillColor ; this . beginPath (); this . moveTo ( x , y ); this . arc ( x , y , radius , 0 , Math . PI * 2 , false ); this . fill (); }; ctx . clearTo = function ( fillColor ) { ctx . fillStyle = fillColor ; ctx . fillRect ( 0 , 0 , width , height ); }; ctx . clearTo ( \"#fff\" ); canvas . onmousemove = function ( e ) { if ( ! canvas . isDrawing ) { return ; } var x = e . pageX - this . offsetLeft ; var y = e . pageY - this . offsetTop ; var radius = 10 ; var fillColor = 'rgb(102,153,255)' ; ctx . fillCircle ( x , y , radius , fillColor ); }; canvas . onmousedown = function ( e ) { canvas . isDrawing = true ; }; canvas . onmouseup = function ( e ) { canvas . isDrawing = false ; }; } var container = document . getElementById ( 'canvas' ); init ( container , 200 , 200 , '#ddd' ); function clearCanvas () { var canvas = document . getElementById ( \"inputCanvas\" ); var ctx = canvas . getContext ( \"2d\" ); ctx . clearRect ( 0 , 0 , canvas . width , canvas . height ); } function getData () { var canvas = document . getElementById ( \"inputCanvas\" ); var imageData = canvas . context . getImageData ( 0 , 0 , canvas . width , canvas . height ); var data = imageData . data ; var outputData = [] for ( var i = 0 ; i < data . length ; i += 4 ) { var brightness = 0.34 * data [ i ] + 0.5 * data [ i + 1 ] + 0.16 * data [ i + 2 ]; outputData . push ( brightness ); } $ . post ( \"/postmethod\" , { canvas_data : JSON . stringify ( outputData ) }, function ( err , req , resp ){ window . location . href = \"/results/\" + resp [ \"responseJSON\" ][ \"uuid\" ]; }); } $ ( \"#clearButton\" ). click ( function (){ clearCanvas (); }); $ ( \"#sendButton\" ). click ( function (){ getData (); }); }); Finally we need to define a base template to be used by the index and result page. I know this could be split up nicer and I could make better use of the templating engine, but for this experiment it seemed sufficient. templates/layouts/base.html <!doctype html> < html > < head > {% block head %} < link rel = \"stylesheet\" href = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\" > < link rel = \"stylesheet\" href = \"/static/css/style.css\" > < script src = \"https://code.jquery.com/jquery-2.1.4.min.js\" ></ script > < script src = \"/static/js/script.js\" ></ script > < title > {% block title %}{% endblock %} - Simple Flask app </ title > {% endblock %} </ head > < body > < div class = \"container\" > < nav class = \"navbar navbar-default\" role = \"navigation\" > < div class = \"navbar-header\" > < button type = \"button\" class = \"navbar-toggle\" data-toggle = \"collapse\" data-target = \".navbar-collapse\" > < span class = \"icon-bar\" ></ span > < span class = \"icon-bar\" ></ span > < span class = \"icon-bar\" ></ span > </ button > </ div > < a class = \"navbar-brand\" href = \"#\" ></ a > < div class = \"navbar-collapse collapse\" > < ul class = \"nav navbar-nav navbar-right\" > < li >< a href = \"/\" > Home </ a ></ li > < li >< a href = \"/results\" > Results </ a ></ li > </ ul > </ div > </ nav > </ div > < div id = \"content\" class = \"container main-container\" > {% block content %}{% endblock %} </ div > < div id = \"footer\" class = \"container text-center\" > {% block footer %} &copy; Copyright 2017 by Jitse-Jan. {% endblock %} </ div > < footer > < script src = \"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\" ></ script > </ footer > </ body > </ html > The code for the index.html and results.html can be kept to a minimum this way. templates/layouts/index.html {% extends \"layouts/base.html\" %} {% block title %}{{title}}{% endblock %} {% block head %} {{ super() }} {% endblock %} {% block content %} < div class = \"row text-center\" > < h1 > {{ title}} </ h1 > < canvas id = \"inputCanvas\" width = \"200\" height = \"200\" ></ canvas > </ div > < br />< br /> < div class = \"row text-center\" > < button class = \"btn btn-primary\" id = \"clearButton\" > Clear </ button > < button class = \"btn btn-primary\" id = \"sendButton\" > Send data </ button > </ div > {% endblock %} templates/layouts/result.html {% extends \"layouts/base.html\" %} {% block title %}{{title}}{% endblock %} {% block head %} {{ super() }} {% endblock %} {% block content %} < div class = \"row text-center\" > < h1 > Results </ h1 > < img src = \"{{ url_for('plot', imgdata = data) }}\" alt = \"Image Placeholder\" height = \"500\" > </ div > {% endblock %} Important: Please note that for the source of the image the specific URL for the matplotlib image is used. The route for plot is called with the parameter imgdata containing the data. I have kept the stylesheet very basic since this project is not aimed at making the most slick interface. templates/static/css/style.css . btn { background-color : rgb ( 102 , 153 , 255 ); } # inputCanvas { border : 2 px solid rgb ( 102 , 153 , 255 ); } # footer { margin-top : 100 px ; } After putting all the files together the application can be started and visited on port 5000 on the localhost. ~/code/flask-app $ FLASK_APP = app.py FLASK_DEBUG = 1 flask run See the Github repo for the final code.", "tags": "posts", "url": "python-and-javascript-in-flask.html", "loc": "python-and-javascript-in-flask.html" }, { "title": "Create an API using Eve in Python", "text": "/Users/jitsejan/code $ mkdir eve-api /Users/jitsejan/code $ cd eve-api/ /Users/jitsejan/code/eve-api $ python3 -m pip install eve /Users/jitsejan/code/eve-api $ touch app.py /Users/jitsejan/code/eve-api $ sublime app.py api.py from eve import Eve import settings app = Eve ( settings = settings . settings ) if __name__ == '__main__' : app . run () /Users/jitsejan/code/eve-api $ touch settings.py /Users/jitsejan/code/eve-api $ sublime settings.py settings.py character = { 'schema' : { 'name' : { 'type' : 'string' }, 'color' : { 'type' : 'string' }, 'superpower' : { 'type' : 'string' }, }, } settings = { 'MONGO_HOST' : 'localhost' , 'MONGO_DBNAME' : 'nintendo-database' , 'MONGO_USERNAME' : 'db-user' , 'MONGO_PASSWORD' : 'db-pass' , 'RESOURCE_METHODS' : [ 'GET' ], 'DOMAIN' : { 'character' : character , }, } /Users/jitsejan/code/eve-api $ python app.py Use Postman to connect to localhost:5000 and start using your API. Since the character schema has been defined, the characters are listed on localhost:5000/character. You can simply search by name and only retrieve a specific field using additional parameters. localhost : 5000 / character ? where ={ \"name\" : \"Mario\" }& projection ={ \"name\" : 1 , \"superpower\" : 1 }", "tags": "posts", "url": "creating-api-using-eve-and-mongodb.html", "loc": "creating-api-using-eve-and-mongodb.html" }, { "title": "Using Pythons pickle to save and load variables", "text": "Recently I was playing with some code that generated big dictionaries and had to manipulate these dictonaries several times. I used to save them via Pythons pandas to CSV and load them back from the CSV the next time I was using my script. Luckily I found out an easier way to deal with saving and loading variables, namely by using Pickle . Import import pickle Saving pickle . dump ( variable , open ( picklename , 'wb' )) Loading pickle . load ( open ( picklename , \"rb\" ) )", "tags": "posts", "url": "using-pythons-pickle-to-save-and-load-variables.html", "loc": "using-pythons-pickle-to-save-and-load-variables.html" }, { "title": "Fancy select boxes using FontAwesome", "text": "See the example on my bl.ocks.org . index.html The necessary JS and CSS files are included. Two select boxes are added to the main container. < head > < script type = \"text/javascript\" src = \"https://code.jquery.com/jquery-3.2.1.min.js\" ></ script > < script type = \"text/javascript\" src = \"https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js\" ></ script > < script type = \"text/javascript\" src = \"script.js\" ></ script > < link rel = \"stylesheet\" type = \"text/css\" href = \"https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"https://netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.css\" > < link rel = \"stylesheet\" type = \"text/css\" href = \"style.css\" > </ head > < body > < div class = \"container\" > < div class = \"row\" > < h1 > Drinks </ h1 > < div class = \"input-group col-md-12\" > < input id = \"coffee_cb\" type = \"checkbox\" class = \"coffee drink_cb\" checked /> < label for = \"coffee_cb\" ></ label > < input id = \"wine_cb\" type = \"checkbox\" class = \"wine drink_cb\" checked /> < label for = \"wine_cb\" ></ label > </ div > </ div > < div class = \"row\" > < h1 > Selected filter </ h1 > < table > < tr > < th > Coffee </ th > < td id = \"filterValCoffee\" ></ td > </ tr > < tr > < th > Wine </ th > < td id = \"filterValWine\" ></ td > </ tr > </ table > </ div > </ div > </ body > style.css The checkbox itself is hidden and a background using FontAwesome is used instead. body { margin : 30 px ; } input [ type = checkbox ] { display : none ; } input [ type = checkbox ] + label { color : black ; font-size : 28 px ; } input [ type = checkbox ] + label : before { font-family : FontAwesome ; display : inline-block ; width : 50 px ; height : 50 px ; padding : 2 px ; background-color : white ; text-align : center ; -webkit- border-radius : 50 % ; -moz- border-radius : 50 % ; border-radius : 50 % ; border : black 2 px solid ; margin : 5 px ; } input [ type = checkbox ] . coffee + label : before { content : \"\\f0f4\" ; } input [ type = checkbox ] . wine + label : before { content : \"\\f000\" ; } input [ type = checkbox ] : checked + label : before { color : white ; background-color : black ; } script.js Retrieve the value of the checkboxes and set the HTML of its corresponding element. $ ( document ). ready ( function () { function setItemValues () { $ ( \"#coffee_cb\" ). is ( \":checked\" ) ? coffeeCheck = 'Yes' : coffeeCheck = 'No' ; $ ( \"#wine_cb\" ). is ( \":checked\" ) ? wineCheck = 'Yes' : wineCheck = 'No' ; $ ( '#filterValCoffee' ). html ( coffeeCheck ); $ ( '#filterValWine' ). html ( wineCheck ); } $ ( '.drink_cb' ). change ( function () { setItemValues (); }); setItemValues (); });", "tags": "posts", "url": "select-boxes-with-font-awesome.html", "loc": "select-boxes-with-font-awesome.html" }, { "title": "Using D3.js in Jupyter notebook", "text": "D3.js and Jupyter A short description how to use D3.js in a Jupyter notebook. Input data Lets create a CSV file containing the amounts of 5 crypto currencies over a small period. In [1]: csvstring = \"\"\" Time;BTC;DOGE;ETH;LTC;REP 2017-05-03 23:17;19,70;;78,88;20,81; 2017-05-04 20:18;21,21;;90,45;24,91; 2017-05-05 19:11;20,1;11,58;91,73;24,06; 2017-05-06 18:56;20,28;12,37;92,78;25,91; 2017-05-09 0:50;21,77;20,34;89,27;28,45; 2017-05-09 2:15;21,99;20,58;88,62;28,49;34,70 2017-05-09 23:59;22,46;17,51;87,45;30,14;32,45 2017-05-11 0:57;23,15;18,83;86,94;32,18;34,36 2017-05-11 22:17;24,17;17,48;87,87;29,62;34,36 2017-05-12 1:55;24,13;17,99;88,05;30,02;36,08 2017-05-13 1:57;22,41;17,48;85,25;27,00;33,72 2017-05-14 15:32;23,67;17,04;89,19;28,90;34,24 2017-05-14 23:34;23,47;17,51;88,60;28,01;33,72 2017-05-15 22:12;22,34;16,16;90,20;24,43;32,58 2017-05-16 20:25;23,12;15,68;88,54;24,19;31,36 2017-05-17 22:01;24,00;19,26;86,28;24,50;30,62 2017-05-18 23:45;24,84;21,05;94,93;27,93;32,62 2017-05-19 22:01;25,59;21,98;118,55;27,03;33,62\"\"\" Write the CSV text fo a file. In [2]: ! echo \" $csvstring \" > wallet.csv Check if the file exists. In [3]: ! ls -la wallet.csv -rw-rw-r-- 1 jitsejan jitsejan 833 Aug 2 10:38 wallet.csv Check the last entry of the CSV file. In [4]: ! tail -n 1 wallet.csv 2017-05-19 22:01;25,59;21,98;118,55;27,03;33,62 Read the file into a dataframe. In [5]: import pandas as pd wallet_data = pd . read_csv ( 'wallet.csv' , sep = ';' , decimal = \",\" ) wallet_data = wallet_data . fillna ( 0 ) wallet_data . head () Out[5]: Time BTC DOGE ETH LTC REP 0 2017-05-03 23:17 19.70 0.00 78.88 20.81 0.0 1 2017-05-04 20:18 21.21 0.00 90.45 24.91 0.0 2 2017-05-05 19:11 20.10 11.58 91.73 24.06 0.0 3 2017-05-06 18:56 20.28 12.37 92.78 25.91 0.0 4 2017-05-09 0:50 21.77 20.34 89.27 28.45 0.0 Add a total column for the currency columns. In [6]: wallet_data [ 'total' ] = wallet_data . ix [:, wallet_data . columns != 'Time' ] . sum ( axis = 1 ) /home/jitsejan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix \"\"\"Entry point for launching an IPython kernel. In [7]: wallet_data Out[7]: Time BTC DOGE ETH LTC REP total 0 2017-05-03 23:17 19.70 0.00 78.88 20.81 0.00 119.39 1 2017-05-04 20:18 21.21 0.00 90.45 24.91 0.00 136.57 2 2017-05-05 19:11 20.10 11.58 91.73 24.06 0.00 147.47 3 2017-05-06 18:56 20.28 12.37 92.78 25.91 0.00 151.34 4 2017-05-09 0:50 21.77 20.34 89.27 28.45 0.00 159.83 5 2017-05-09 2:15 21.99 20.58 88.62 28.49 34.70 194.38 6 2017-05-09 23:59 22.46 17.51 87.45 30.14 32.45 190.01 7 2017-05-11 0:57 23.15 18.83 86.94 32.18 34.36 195.46 8 2017-05-11 22:17 24.17 17.48 87.87 29.62 34.36 193.50 9 2017-05-12 1:55 24.13 17.99 88.05 30.02 36.08 196.27 10 2017-05-13 1:57 22.41 17.48 85.25 27.00 33.72 185.86 11 2017-05-14 15:32 23.67 17.04 89.19 28.90 34.24 193.04 12 2017-05-14 23:34 23.47 17.51 88.60 28.01 33.72 191.31 13 2017-05-15 22:12 22.34 16.16 90.20 24.43 32.58 185.71 14 2017-05-16 20:25 23.12 15.68 88.54 24.19 31.36 182.89 15 2017-05-17 22:01 24.00 19.26 86.28 24.50 30.62 184.66 16 2017-05-18 23:45 24.84 21.05 94.93 27.93 32.62 201.37 17 2017-05-19 22:01 25.59 21.98 118.55 27.03 33.62 226.77 Convert the data to a dictionary that Javascript can use. In [8]: wallet_data = wallet_data . to_json ( orient = 'records' ) Attach the data to the current window by using Javascript. In [9]: from IPython.display import Javascript Javascript ( \"\"\" window.walletData= {} ; \"\"\" . format ( wallet_data )) Out[9]: Create the graph First include the D3.js library. In [1]: %% javascript require . config ({ paths : { d3 : '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min' } }); I copied the content of my gist to recreate the graph inside this notebook. Use the HTML function of IPython to attach inline style to the page. In [11]: from IPython.core.display import HTML HTML ( \"\"\" <style> path { stroke-width: 1; fill: none; stroke-linejoin: round; stroke-linecap: round; } circle { stroke-width: 1; } .axis path, .axis line { fill: none; stroke: grey; stroke-width: 1; shape-rendering: crispEdges; } .legend, .label, .hover-text{ font-size: x-small; background-color: white; } </style> \"\"\" ) Out[11]: Create the graph by retrieving the data from the window. In [12]: %% javascript require ([ 'd3' ], function ( d3 ) { //a weird idempotency thing $ ( \"#chart1\" ). remove (); //create canvas element . append ( \"<svg id='chart1' width='960' height='500'></svg>\" ); var svg = d3 . select ( 'svg' ), margin = { top : 20 , right : 50 , bottom : 100 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Wallet chart' ); // Function to convert a string into a time var parseTime = d3 . time . format ( '%Y-%m-%d %H:%M' ). parse ; // Function to show specific time format var formatTime = d3 . time . format ( '%e %B' ); // Set data var data = window . walletData ; data . forEach ( function ( d ) { d . date = parseTime ( d . Time ); }); // Set the X scale var x = d3 . time . scale (). range ([ 0 , width ], 0.5 ); // Set the Y scale var y = d3 . scale . linear (). range ([ height , 0 ]); // Set the color scale var color = d3 . scale . category10 (); var xAxis = d3 . svg . axis () . scale ( x ) . orient ( \"bottom\" ); var yAxis = d3 . svg . axis () . scale ( y ) . orient ( \"left\" ); var line = d3 . svg . line () // .interpolate(\"basis\") . x ( function ( d ) { return x ( d . date ); }) . y ( function ( d ) { return y ( d . worth ); }); color . domain ( d3 . keys ( data [ 0 ]). filter ( function ( key ) { return key !== \"Time\" && key !== \"date\" ; })); var currencies = color . domain (). map ( function ( name ) { return { name : name , values : data . map ( function ( d ) { return { date : d . date , worth : + d [ name ] }; }) }; }); x . domain ( d3 . extent ( data , function ( d ) { return d . date ; })); // Set the Y domain y . domain ([ d3 . min ( currencies , function ( c ) { return d3 . min ( c . values , function ( v ) { return v . worth ; }); }), d3 . max ( currencies , function ( c ) { return d3 . max ( c . values , function ( v ) { return v . worth ; }); }) ]); // Set the X axis g . append ( \"g\" ) . attr ( \"class\" , \"x axis\" ) // .attr(\"fill\", \"none\") . attr ( \"transform\" , \"translate(0,\" + height + \")\" ) . call ( xAxis ); // Set the Y axis g . append ( \"g\" ) . attr ( \"class\" , \"y axis\" ) . call ( yAxis ) . append ( \"text\" ) . attr ( \"transform\" , \"rotate(-90)\" ) . attr ( \"y\" , 6 ) . attr ( \"dy\" , \".71em\" ) . style ( \"text-anchor\" , \"end\" ) . text ( \"Value (USD)\" ); // Draw the lines var currency = g . selectAll ( \".currency\" ) . data ( currencies ) . enter (). append ( \"g\" ) . attr ( \"class\" , \"currency\" ); currency . append ( \"path\" ) . attr ( \"class\" , \"line\" ) . attr ( \"fill\" , \"none\" ) . attr ( \"d\" , function ( d ) { return line ( d . values ); }) . style ( \"stroke\" , function ( d ) { return color ( d . name ); }); // Add the circles currency . append ( \"g\" ). selectAll ( \"circle\" ) . data ( function ( d ) { return d . values }) . enter () . append ( \"circle\" ) . attr ( \"r\" , 2 ) . attr ( \"cx\" , function ( dd ) { return x ( dd . date ) }) . attr ( \"cy\" , function ( dd ) { return y ( dd . worth ) }) . attr ( \"fill\" , \"none\" ) . attr ( \"stroke\" , function ( d ) { return color ( this . parentNode . __data__ . name ) }); // Add label to the end of the line currency . append ( \"text\" ) . attr ( \"class\" , \"label\" ) . datum ( function ( d ) { return { name : d . name , value : d . values [ d . values . length - 1 ] }; }) . attr ( \"transform\" , function ( d ) { return \"translate(\" + x ( d . value . date ) + \",\" + y ( d . value . worth ) + \")\" ; }) . attr ( \"x\" , 3 ) . attr ( \"dy\" , \".35em\" ) . text ( function ( d ) { return d . name ; }); }); In [ ]:", "tags": "posts", "url": "using-d3-in-jupyter-notebook.html", "loc": "using-d3-in-jupyter-notebook.html" }, { "title": "Building a crypto app with ExpressJS, MongoDB and D3.js", "text": "In this post I will describe my initial version of my crypto app, an application where I will simply show some data of my experiments with crypto currencies. Data is handled by Python, put in MongoDB and displayed using ExpressJS and D3.js. The post is a bit long, but I try give my steps as clear as possible so it can be of any help to anyone. Structure of the application The final structure of the app will look like the following. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app$ tree . ├── app.js ├── data │ ├── mining.csv │ └── wallet.csv ├── notebook │ └── CSV to MongoDB.ipynb ├── package.json ├── public │ └── css │ └── style.css └── views ├── pages │ ├── index.ejs │ ├── mining.ejs │ └── wallet.ejs └── partials ├── footer.ejs ├── head.ejs └── header.ejs 7 directories, 11 files Version check jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git --version git version 2 .11.0 ( Apple Git-81 ) jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm -v 4 .2.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ node -v v7.10.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ python --version Python 3 .6.0 :: Anaconda 4 .3.1 ( x86_64 ) jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ jupyter --version 4 .2.1 Initialize jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git init jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm init jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ echo node_modules >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ git add . && git commit -am \"Initial commit\" Create the back-end using a Jupyter notebook Using Python we will read two CSV files and add them to MongoDB. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ echo notebook/.ipynb_checkpoints >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app/notebook $ jupyter notebook Content of the notebook {% notebook csv_to_mongodb.ipynb %} Now the data is in the database we are ready to create the front-end. Create the front-end First install the packages we need and save them to package.json . jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ npm install --save express mongoose ejs mongodb This will result in the following package.json . { \"name\" : \"crypto-app\" , \"version\" : \"1.0.0\" , \"description\" : \"Simple app to track some crypto investments\" , \"main\" : \"app.js\" , \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [ \"crypto\" , \"Python\" , \"mongoose\" , \"expressjs\" ], \"author\" : \"jitsejan\" , \"license\" : \"ISC\" , \"dependencies\" : { \"ejs\" : \"&#94;2.5.6\" , \"express\" : \"&#94;4.15.2\" , \"mongodb\" : \"&#94;2.2.26\" , \"mongoose\" : \"&#94;4.9.9\" } } The core I will create three pages. A blank frontpage, a page for the wallet data and a page for the mining data. First we need setup the application in app.js . This file is responsible for the database connection and serving the templates for each route containing the correct data. // Requirements var express = require ( 'express' ); var app = express (); var mongoose = require ( 'mongoose' ); // Make sure we can use HTML and JavaScript interchangeably app . set ( 'view engine' , 'ejs' ); // Database connection mongoose . connect ( 'mongodb://localhost/crypto-data' ); var db = mongoose . connection ; db . on ( 'error' , console . error . bind ( console , 'connection error:' )); db . once ( 'open' , function callback () { console . log ( 'Connected to Mongo database' ); }); // Define the schema using Mongoose var Schema = mongoose . Schema ; // The Mining schema should be the same as the data we put in Python var miningSchema = new Schema ({ Date : Date , BTC : Number , DRK : Number , LTC : Number }); // Create the model var Mining = db . model ( 'mining' , miningSchema ); // The Wallet schema should be the same as the data we put in Python var walletSchema = new Schema ({ Time : Date , BTC : Number , DOGE : Number , ETH : Number , LTC : Number , REP : Number }); // Create the model var Wallet = db . model ( 'wallet' , walletSchema ); // Create the route for the frontpage app . get ( '/' , function ( req , res ) { res . render ( 'pages/index' , { title : 'Home' }); }); // Create the route to the mining page app . get ( '/mining' , function ( req , res ) { Mining . find ({}, null , { sort : { 'Date' :+ 1 }}, function ( err , minings ){ console . log ( minings ); res . render ( 'pages/mining' , { title : 'Mining' , minings : minings }); }) }); // Create the route to the wallet page app . get ( '/wallet' , function ( req , res ) { Wallet . find ({}, null , { sort : { 'Time' :+ 1 }}, function ( err , wallets ){ console . log ( wallets ); res . render ( 'pages/wallet' , { title : 'Wallet' , wallets : wallets }); }) }); // Define the public directory (where the stylesheet lives) // Normally this would be a subdirectory 'public/css/' app . use ( express . static ( __dirname )); // Start the app on port 3000 app . listen ( 3000 ); console . log ( 'listening on port 3000' ); Partials To easily create templates for the different pages, I will first create the partials for the head, footer and header. I will use Bootstrap to make creating the layout easier. head.ejs < meta charset = \"utf-8\" > < meta http - equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1\" > < meta name = \"description\" content = \"\" > < meta name = \"author\" content = \"\" > < title > Crypto app </ title > < ! -- Bootstrap Core CSS --> < link href = \"http://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css\" rel = \"stylesheet\" > < ! -- Custom CSS --> < link rel = \"stylesheet\" type = \"text/css\" href = \"public/css/style.css\" > < ! -- HTML5 Shim and Respond . js IE8 support of HTML5 elements and media queries --> < ! -- WARNING : Respond . js doesn ' t work if you view the page via file : // --> < ! --[ if lt IE 9 ]> < script src = \"https://oss.maxcdn.com/libs/html5shiv/3.7.3/html5shiv.js\" ></ script > < script src = \"https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js\" ></ script > < ! [ endif ]--> < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > < script src = \"http://labratrevenge.com/d3-tip/javascripts/d3.tip.v0.6.3.js\" ></ script > header.ejs <!-- Navigation --> <nav class= \"navbar navbar-inverse navbar-fixed-top\" role= \"navigation\" > <div class= \"container\" > <!-- Brand and toggle get grouped for better mobile display --> <div class= \"navbar-header\" > <button type= \"button\" class= \"navbar-toggle\" data-toggle= \"collapse\" data-target= \"#bs-example-navbar-collapse-1\" > <span class= \"sr-only\" > Toggle navigation </span> <span class= \"icon-bar\" ></span> <span class= \"icon-bar\" ></span> <span class= \"icon-bar\" ></span> </button> <a class= \"navbar-brand\" href= \"/\" > Crypto app </a> </div> <!-- /.navbar-header --> <div class= \"collapse navbar-collapse\" id= \"bs-example-navbar-collapse-1\" > <ul class= \"nav navbar-nav\" > <li> <a href= \"./mining\" > Mining </a> </li> <li> <a href= \"./wallet\" > Wallet </a> </li> </ul> </div> <!-- /.navbar-collapse --> </div> <!-- /.container --> </nav> footer.ejs <p class= \"text-center text-small text-muted\" > © Copyright 2017 - Crypto app </p> <!-- JQuery JS--> <script src= \"https://code.jquery.com/jquery-3.2.1.min.js\" ></script> <!-- Bootstrap Core JS --> <script src= \"//maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js\" ></script> Pages Now creating the pages is simple. The frontpage is currently empty and will simply look like this: index.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container clear-top\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer class = \"footer\" > <% include ../ partials / footer %> </ footer > </ body > </ html > The mining page template is identical to the frontpage, but we add a placeholder for the D3.js graph and show the data in a table. mining.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> < div class = \"row chart-container\" > < svg class = \"svg-chart\" width = \"960\" height = \"500\" > <!-- placeholder for the chart --> </ svg > </ div > <!-- /.row --> < div class = \"row\" > < div class = \"col-lg-12\" > < table class = \"table\" > < tr > < th > Date </ th > < th > BTC </ th > < th > DRL </ th > < th > LTC </ th > </ tr > <% minings . forEach ( function ( mining ) { %> < tr > < td > <%= mining . Date %> </ td > < td > <%= mining . BTC %> </ td > < td > <%= mining . DRK %> </ td > < td > <%= mining . LTC %> </ td > </ tr > <% }); %> </ table > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer > <% include ../ partials / footer %> </ footer > </ body > </ html > Next I append te template with a Javascript block containing the D3.js graph code. // Convert the bitcoins data to the data we can use in DS.js var data = <%- JSON . stringify ( minings ) %> ; var svg = d3.select('svg'), margin = { top: 20, right: 50, bottom: 100, left: 50 }, width = +svg.attr('width') - margin.left - margin.right, height = +svg.attr('height') - margin.top - margin.bottom, g = svg.append('g').attr('transform', 'translate(' + margin.left + ',' + margin.top + ')'); // Graph title g.append('text') .attr('x', (width / 2)) .attr('y', 0 - (margin.top / 3)) .attr('text-anchor', 'middle') .style('font-size', '16px') .text('Mining chart'); // Function to convert a string into a time var parseTime = d3.time.format('%Y-%m-%dT%H:%M:%S.%LZ').parse; // Function to show specific time format var formatTime = d3.time.format('%e %B'); var tip = d3.tip() .attr('class', 'd3-tip') .offset([-10, 0]) .html(function(d) { return \" <span style= 'color:red' > \" + d.worth + \" </span> <strong> \" + d.currency + \" </strong> \"; }) svg.call(tip); var color = d3.scale.category10(); color.domain(d3.keys(data[0]).filter(function(key) { return key !== \"Date\" && key !== \"_id\"; })); // Correct the types data.forEach(function(d) { d.date = parseTime(d.Date); }); var rewards = color.domain().map(function(name) { return { name: name, values: data.map(function(d) { return { date: d.date, worth: +d[name], currency: name }; }) }; }); var num_bars = d3.keys(rewards).length; var num_days = data.length; var y = d3.scale.linear().range([height, 0]); y.domain([ 0, d3.max(rewards, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ]); var x0 = d3.scale.ordinal() .domain(d3.range(num_days)) .rangeBands([0, width], .2); var x1 = d3.scale.ordinal() .domain(d3.range(num_bars)) .rangeBands([0, x0.rangeBand()]); var color = d3.scale.category10(); var xAxis = d3.svg.axis() .scale(x0) .tickFormat(function(d) { return formatTime(parseTime(data[d].Date)); }) .orient(\"bottom\"); var yAxis = d3.svg.axis() .scale(y) .orient(\"left\"); g.append(\"g\") .attr(\"class\", \"y axis\") .call(yAxis) .append(\"text\") .attr(\"transform\", \"rotate(-90)\") .attr(\"y\", 6) .attr(\"dy\", \".71em\") .style(\"text-anchor\", \"end\") .text(\"Amount\"); g.append(\"g\") .attr(\"class\", \"x axis\") .attr(\"transform\", \"translate(0,\" + height + \")\") .call(xAxis) .selectAll(\"text\") .style(\"text-anchor\", \"end\") .attr(\"dx\", \"-.8em\") .attr(\"dy\", \".15em\") .attr(\"transform\", function(d) { return \"rotate(-90)\" }); // Add the bars g.append(\"g\").selectAll(\".bar\") .data(rewards) .enter().append(\"g\") .style(\"fill\", function(d, i) { return color(i); }) .attr(\"transform\", function(d, i) { return \"translate(\" + x1(i) + \",0)\"; }) .selectAll(\"rect\") .data(function(d) { return d.values; }) .enter().append(\"rect\") .attr(\"class\", \"bar\") .attr(\"width\", x1.rangeBand()) .attr(\"height\", function(d) { return height - y(d.worth); }) .attr(\"x\", function(d, i) { return x0(i); }) .attr(\"y\", function(d) { return y(d.worth); }) .on('mouseover', tip.show) .on('mouseout', tip.hide); var legend = g.append(\"g\") .attr(\"font-family\", \"sans-serif\") .attr(\"font-size\", 10) .attr(\"text-anchor\", \"end\") .selectAll(\"g\") .data(rewards) .enter().append(\"g\") .attr(\"transform\", function(d, i) { return \"translate(0,\" + i * 20 + \")\"; }); legend.append(\"rect\") .attr(\"x\", width - 19) .attr(\"width\", 19) .attr(\"height\", 19) .attr(\"fill\", function(d, i) { return color(i); }) legend.append(\"text\") .attr(\"x\", width - 24) .attr(\"y\", 9.5) .attr(\"dy\", \"0.32em\") .text(function(d) { return d.name; }); For the wallet page I have a similar approach. wallet.ejs <!DOCTYPE html> < html lang = \"en\" > < head > <% include ../ partials / head %> </ head > < body > < header > <% include ../ partials / header %> </ header > < main class = \"main wrap\" > < div class = \"container\" > < div class = \"row\" > < div class = \"col-lg-12 text-center\" > < h1 > <%- title %> </ h1 > </ div > </ div > <!-- /.row --> < div class = \"row chart-container\" > < svg class = \"svg-chart\" width = \"960\" height = \"500\" > <!-- placeholder for the chart --> </ svg > </ div > <!-- /.row --> < div class = \"row\" > < div class = \"col-lg-12\" > < table class = \"table\" > < tr > < th > Date </ th > < th > BTC </ th > < th > DOGE </ th > < th > ETH </ th > < th > LTC </ th > < th > REP </ th > </ tr > <% wallets . forEach ( function ( wallet ) { %> < tr > < td > <%= wallet . Time %> </ td > < td > <%= wallet . BTC %> </ td > < td > <%= wallet . DOGE %> </ td > < td > <%= wallet . ETH %> </ td > < td > <%= wallet . LTC %> </ td > < td > <%= wallet . REP %> </ td > </ tr > <% }); %> </ table > </ div > </ div > <!-- /.row --> </ div > <!-- /.container --> </ main > < footer > <% include ../ partials / footer %> </ footer > </ body > </ html > and the Javascript // Convert the bitcoins data to the data we can use in DS . js var data = <%- JSON . stringify ( wallets ) %> ; // Draw a line chart var svg = d3 . select ( 'svg.svg-chart' ), margin = { top : 20 , right : 50 , bottom : 30 , left : 50 } , width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Wallet chart' ); // Function to convert a string into a time var parseTime = d3 . time . format ( '%Y-%m-%dT%H:%M:%S.%LZ' ). parse ; // Function to show specific time format var formatTime = d3 . time . format ( '%e %B' ); // Set the X scale var x = d3 . time . scale (). range ( [ 0, width ] , 0.5 ); // Set the Y scale var y = d3 . scale . linear (). range ( [ height, 0 ] ); // Set the color scale var color = d3 . scale . category10 (); var xAxis = d3 . svg . axis () . scale ( x ) . orient ( \"bottom\" ); var yAxis = d3 . svg . axis () . scale ( y ) . orient ( \"left\" ); var line = d3 . svg . line () // . interpolate ( \"basis\" ) . x ( function ( d ) { return x ( d . date ); } ) . y ( function ( d ) { return y ( d . worth ); } ); // Select the important columns color . domain ( d3 . keys ( data [ 0 ] ). filter ( function ( key ) { return key !== \"Time\" && key !== \"_id\" ; } )); // Correct the types data . forEach ( function ( d ) { d . date = parseTime ( d . Time ); } ); var currencies = color . domain (). map ( function ( name ) { return { name : name , values : data . map ( function ( d ) { return { date : d . date , worth : + d [ name ] } ; } ) } ; } ); // Set the X domain x . domain ( d3 . extent ( data , function ( d ) { return d . date ; } )); // Set the Y domain y . domain ( [ d3.min(currencies, function(c) { return d3.min(c.values, function(v) { return v.worth; }); }), d3.max(currencies, function(c) { return d3.max(c.values, function(v) { return v.worth; }); }) ] ); // Set the X axis g . append ( \"g\" ) . attr ( \"class\" , \"x axis\" ) . attr ( \"transform\" , \"translate(0,\" + height + \")\" ) . call ( xAxis ); // Set the Y axis g . append ( \"g\" ) . attr ( \"class\" , \"y axis\" ) . call ( yAxis ) . append ( \"text\" ) . attr ( \"transform\" , \"rotate(-90)\" ) . attr ( \"y\" , 6 ) . attr ( \"dy\" , \".71em\" ) . style ( \"text-anchor\" , \"end\" ) . text ( \"Value (USD)\" ); // Draw the lines var currency = g . selectAll ( \".currency\" ) . data ( currencies ) . enter (). append ( \"g\" ) . attr ( \"class\" , \"currency\" ); currency . append ( \"path\" ) . attr ( \"class\" , \"line\" ) . attr ( \"d\" , function ( d ) { return line ( d . values ); } ) . style ( \"stroke\" , function ( d ) { return color ( d . name ); } ); // Add the circles currency . append ( \"g\" ). selectAll ( \"circle\" ) . data ( function ( d ) { return d . values } ) . enter () . append ( \"circle\" ) . attr ( \"r\" , 2 ) . attr ( \"cx\" , function ( dd ) { return x ( dd . date ) } ) . attr ( \"cy\" , function ( dd ) { return y ( dd . worth ) } ) . attr ( \"fill\" , \"none\" ) . attr ( \"stroke\" , function ( d ) { return color ( this . parentNode . __data__ . name ) } ); // Add label to the end of the line currency . append ( \"text\" ) . attr ( \"class\" , \"label\" ) . datum ( function ( d ) { return { name : d . name , value : d . values [ d.values.length - 1 ] } ; } ) . attr ( \"transform\" , function ( d ) { return \"translate(\" + x ( d . value . date ) + \",\" + y ( d . value . worth ) + \")\" ; } ) . attr ( \"x\" , 3 ) . attr ( \"dy\" , \".35em\" ) . text ( function ( d ) { return d . name ; } ); // Add the mouse line var mouseG = g . append ( \"g\" ) . attr ( \"class\" , \"mouse-over-effects\" ); mouseG . append ( \"path\" ) . attr ( \"class\" , \"mouse-line\" ) . style ( \"stroke\" , \"black\" ) . style ( \"stroke-width\" , \"1px\" ) . style ( \"opacity\" , \"0\" ); var lines = document . getElementsByClassName ( 'line' ); var mousePerLine = mouseG . selectAll ( '.mouse-per-line' ) . data ( currencies ) . enter () . append ( \"g\" ) . attr ( \"class\" , \"mouse-per-line\" ); mousePerLine . append ( \"circle\" ) . attr ( \"r\" , 7 ) . style ( \"stroke\" , function ( d ) { return color ( d . name ); } ) . style ( \"fill\" , \"none\" ) . style ( \"stroke-width\" , \"2px\" ) . style ( \"opacity\" , \"0\" ); mousePerLine . append ( \"text\" ) . attr ( \"class\" , \"hover-text\" ) . attr ( \"dy\" , \"-1em\" ) . attr ( \"transform\" , \"translate(10,3)\" ); // Append a rect to catch mouse movements on canvas mouseG . append ( 'svg:rect' ) . attr ( 'width' , width ) . attr ( 'height' , height ) . attr ( 'fill' , 'none' ) . attr ( 'pointer-events' , 'all' ) . on ( 'mouseout' , function () { // on mouse out hide line , circles and text d3 . select ( \".mouse-line\" ) . style ( \"opacity\" , \"0\" ); d3 . selectAll ( \".mouse-per-line circle\" ) . style ( \"opacity\" , \"0\" ); d3 . selectAll ( \".mouse-per-line text\" ) . style ( \"opacity\" , \"0\" ); } ) . on ( 'mouseover' , function () { // on mouse in show line , circles and text d3 . select ( \".mouse-line\" ) . style ( \"opacity\" , \"1\" ); d3 . selectAll ( \".mouse-per-line circle\" ) . style ( \"opacity\" , \"1\" ); d3 . selectAll ( \".mouse-per-line text\" ) . style ( \"opacity\" , \"1\" ); } ) . on ( 'mousemove' , function () { // mouse moving over canvas var mouse = d3 . mouse ( this ); d3 . selectAll ( \".mouse-per-line\" ) . attr ( \"transform\" , function ( d , i ) { var xDate = x . invert ( mouse [ 0 ] ), bisect = d3 . bisector ( function ( d ) { return d . date ; } ). left ; idx = bisect ( d . values , xDate ); d3 . select ( this ). select ( 'text' ) . text ( y . invert ( y ( d . values [ idx ] . worth )). toFixed ( 2 )); d3 . select ( \".mouse-line\" ) . attr ( \"d\" , function () { var data = \"M\" + x ( d . values [ idx ] . date ) + \",\" + height ; data += \" \" + x ( d . values [ idx ] . date ) + \",\" + 0 ; return data ; } ); return \"translate(\" + x ( d . values [ idx ] . date ) + \",\" + y ( d . values [ idx ] . worth ) + \")\" ; } ); } ); Last thing we need to add is some custom style to the pages and graphs. style.css html , body { height : 100 % ; } body { background-color : #eee ; padding-top : 70 px ; padding-bottom : 70 px ; } . wrap { min-height : 100 % ; } . text-small { font-size : small ; } . chart-container { margin : 0 px auto ; padding : 40 px ; } . main { overflow : auto ; padding-bottom : 50 px ; } . footer { position : relative ; margin-top : -50 px ; height : 50 px ; clear : both ; padding-top : 20 px ; } /* Visualization */ path { stroke-width : 1 ; fill : none ; stroke-linejoin : round ; stroke-linecap : round ; } circle { stroke-width : 1 ; fill : steelblue } . axis path , . axis line { fill : none ; stroke : grey ; stroke-width : 1 ; shape-rendering : crispEdges ; } . legend , . label , . hover-text { font-size : x-small ; background-color : white ; } . axis text { font : 10 px sans-serif ; } . axis path , . axis line { fill : none ; stroke : #000 ; shape-rendering : crispEdges ; } . bar : hover { fill : orangered ; } . d3-tip { line-height : 1 ; font-weight : bold ; padding : 12 px ; background : rgba ( 0 , 0 , 0 , 0.8 ); color : #fff ; border-radius : 2 px ; } /* Creates a small triangle extender for the tooltip */ . d3-tip : after { box-sizing : border-box ; display : inline ; font-size : 10 px ; width : 100 % ; line-height : 1 ; color : rgba ( 0 , 0 , 0 , 0.8 ); content : \"\\25BC\" ; position : absolute ; text-align : center ; } /* Style northward tooltips differently */ . d3-tip . n : after { margin : -1 px 0 0 0 ; top : 100 % ; left : 0 ; } And that is it. Now by running the server, the application can be viewed on port 3000 of your localhost! jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/crypto-app $ node app.js Result I did not deploy the application on a server (yet) so it cannot be viewed. However, I have put my code on Bitbucket and the two charts can be viewed as Gist or bl.ocks.org . The line chart can be viewed on this page. Using rawgit I was able to display it using an iframe. The bar chart can be viewed on this page. Because I am using an additional library to create fancy tooltips, the iframe won't load properly. Check my Github repo for the source code.", "tags": "posts", "url": "building-a-crypto-app.html", "loc": "building-a-crypto-app.html" }, { "title": "Setting up a Dapp with Truffle and Metamask", "text": "Update npm jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo npm install -g npm Update nodejs jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo npm install -g n jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sudo n stable Install geth jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ brew tap ethereum/ethereum jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ brew install ethereum Check versions jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ sw_vers ProductName: Mac OS X ProductVersion: 10 .12.4 BuildVersion: 16E195 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ geth version Geth Version: 1 .6.1-stable Git Commit: 021c3c281629baf2eae967dc2f0a7532ddfdc1fb Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: darwin GOPATH = GOROOT = /usr/local/Cellar/go/1.8.1/libexec jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ node -v v7.10.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ npm -v 4 .2.0 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ python -V Python 2 .7.13 :: Anaconda 4 .3.1 ( x86_64 ) Create structure for the app jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ mkdir truffle-dapp jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code $ cd truffle-dapp/ && git init && npm init Install web3 and testrpc Note that you need to use Python 2.7 in order to be able to install the Ethereum testrpc. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm install ethereumjs-testrpc web3 --save --python = python2.7 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ echo node_modules/ >> .gitignore jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git commit -a -m \"Initial commit\" Start the testrpc This includes 10 unlocked accounts with 100 Ether each. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ node_modules/.bin/testrpc Secp256k1 bindings are not compiled. Pure JS implementation will be used. EthereumJS TestRPC v3.0.5 Available Accounts ================== ( 0 ) 0x21af84b1d1b0c26dd470d1e13074d784981a1ca7 ( 1 ) 0xebf0475f0c9dec6d39d353cab90d10b27f0575a8 ( 2 ) 0xc0ffba2ed3bbe73e123cc31fa01215fdd9be3233 ( 3 ) 0x5868efd6f3255925268c53e523a81164f4d86733 ( 4 ) 0xb3ff5cf0790e67e46fad5e476967d1bd42e9a288 ( 5 ) 0x6581ff04d20ad77025d2775c6479bbcf1d292f0c ( 6 ) 0x105a59eb345e602a38553432e7a18360ac3040a8 ( 7 ) 0x8febd769876d764d8d1a7bb3d3d4360df9282401 ( 8 ) 0xb15505faddaa401f23738843956eab8ab0078d74 ( 9 ) 0x56e5a52e65329c75314ec9642725fd272ac908f8 Private Keys ================== ( 0 ) b4b11e97ab1055d41ae8b93d76cb699cc637ab6c45ac3ce769208b37ac7d4e9f ( 1 ) a1222fd97545205ff2e10143a8e6bbe89ea3aa429b4cef64e641885c302a8e4c ( 2 ) 879f33b2dc93fc3add7ab2f189f00c5cf77490090d7d9c46cb4883dd65ece305 ( 3 ) 3ced2c7e98b75eb8220edf8109d934ba229d3d5c996d7a297d0ad3961b716275 ( 4 ) 3bbaeaa98f28ad987c71815c0f07a79be3bc89c1391ae808ea0af98b61201914 ( 5 ) ec6b3c03a904bd7650c803cccb5cf29ccdaa444af336103c021af782b866873c ( 6 ) b1584edd48d57a35acc1176b1a02f7d5c5401e3e00495ce859cf76f8be24b207 ( 7 ) 9aa9ce75c4165b30c4847262a4cbb634a2f199228b3386b92e5fa184830bc95b ( 8 ) 68ab448c5a453ea723561b63fb756879bae3fbb44fc003311de62827023ca49a ( 9 ) 674cf39b8c96a8793bf6e0c6c0b3a7c234644eaf7536de1406148fd8699fede7 HD Wallet ================== Mnemonic: search romance drip card right human valley tilt depart detail nation rich Base HD Path: m/44 '/60' /0 ' /0/ { account_index } Listening on localhost:8545 Note. The mnemonic with the 12 words will be used later in Metamask. Connect to the testrpc Create a client and check the balance. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ node > Web3 = require ( 'web3' ) { [ Function: Web3 ] providers: { HttpProvider: [ Function: HttpProvider ] , IpcProvider: [ Function: IpcProvider ] } } > web3 = new Web3 ( new Web3.providers.HttpProvider ( \"http://localhost:8545\" )) ; > primary = web3.eth.accounts [ 0 ] ; '0x914cb49b14a339d000858dc4c8b4cb0e9195c574' > web3.fromWei ( web3.eth.getBalance ( primary ) , \"ether\" ) .toString () '100' Install the Truffle scaffold jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm install truffle --save --python = python2.7 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle version Truffle v3.2.2 jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle init webpack jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git add . jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ git commit -m \"Truffle init\" Compile jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle compile Compiling ./contracts/ConvertLib.sol... Compiling ./contracts/MetaCoin.sol... Compiling ./contracts/Migrations.sol... Writing artifacts to ./build/contracts Migrate jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ truffle migrate Deploy Start the server in the development environment. jitsejan@Jitse-Jans-MacBook-Pro /Users/jitsejan/code/truffle-dapp $ npm run dev Now by accessing localhost:8080 the web application is visible and we can send some MetaCoin. Open up Google Chrome and make sure Metamask is installed. Next open up Metamask and restore from DEN and fill in the 12 words that the testrpc showed you before. Make sure you are connected to localhost:8545. Click in Metamask on the icon on the top to switch accounts. Click on add and the other accounts will appear. We can now choose one of the other 9 accounts to send some MetaCoin. Click on the copy icon next to the account and enter it in the input field in the form. Add the amount of MetaCoin you want to send and hit Send MetaCoin. Metamask will show a pop-up asking you to accept the transaction. Once the MetaCoin are sent, the amount on the frontpage are reduced. Clicking again on Metamask will show the history of transactions.", "tags": "posts", "url": "creating-dapp-with-truffle-and-metamask.html", "loc": "creating-dapp-with-truffle-and-metamask.html" }, { "title": "Using the Ethereum Web3 client in Python", "text": "Ethereum - Web3 Similar to the RPC client, I will use the web3 API to make a transaction on my private blockchain. Read The Docs GitHub In [1]: from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" I willl use the Python 2.7 environment since Python 3.6 is not yet supported. In [2]: import sys sys . version Out[2]: '2.7.13 |Anaconda 4.3.1 (64-bit)| (default, Dec 20 2016, 23:09:15) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]' ( python2 ) jitsejan@jjvps:~$ pip install web3 Collecting web3 Requirement already satisfied: requests> = 2 .12.4 in /home/jitsejan/anaconda3/envs/python2/lib/python2.7/site-packages ( from web3 ) Collecting ethereum-utils> = 0 .2.0 ( from web3 ) Requirement already satisfied: pysha3> = 0 .3 in /home/jitsejan/anaconda3/envs/python2/lib/python2.7/site-packages ( from web3 ) Collecting ethereum-abi-utils> = 0 .4.0 ( from web3 ) Requirement already satisfied: rlp> = 0 .4.7 in /home/jitsejan/anaconda3/envs/python2/lib/python2.7/site-packages ( from web3 ) Collecting pylru> = 1 .0.9 ( from web3 ) Installing collected packages: ethereum-utils, ethereum-abi-utils, pylru, web3 Successfully installed ethereum-abi-utils-0.4.0 ethereum-utils-0.2.0 pylru-1.0.9 web3-3.8.1 Note: make sure geth is started with arguments --rpc and --rpcapi=\"db,eth,net,web3,personal,web3\" jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 0 --port 30333 --rpc --rpcapi = \"db,eth,net,web3,personal,miner\" In [3]: import web3 from web3 import Web3 , KeepAliveRPCProvider , IPCProvider web3 . __version__ Out[3]: '3.8.1' Setup RPC connection In [4]: web3 = Web3 ( KeepAliveRPCProvider ( host = 'localhost' , port = '8545' )) Retrieve the accounts In [5]: web3 . eth . accounts Out[5]: [u'0x8cf9deda0712f2291fb16739f8759e4a0a575854'] In [6]: address = web3 . eth . accounts [ 0 ] Set the two other machines In [7]: address_vps_one = \"0xc257beaea430afb3a09640ce7f020c906331f805\" address_vps_two = \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" Show their balances In [8]: print \"Host has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' ) print \"VPS 1 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_one ), 'ether' ) print \"VPS 2 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_two ), 'ether' ) prev_host_amount = float ( web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' )) Host has 411 Ether VPS 1 has 150 Ether VPS 2 has 83 Ether Transaction Lets send 12 Ether from the main machine to VPS one. In [9]: amount = 12 # Ether sending_address = address receiving_address = address_vps_one Get the password to unlock the sending account In [10]: from getpass import getpass pw = getpass ( prompt = 'Enter the password for the sender: ' ) Enter the password for the sender: ········ Unlock the account In [11]: web3 . personal . unlockAccount ( address , pw ) Out[11]: True Create the transaction In [12]: params = {} params [ 'to' ] = receiving_address params [ 'from' ] = sending_address params [ 'value' ] = web3 . toWei ( amount , \"ether\" ) tx_hash = web3 . eth . sendTransaction ( params ) Check the transaction In [13]: web3 . eth . getTransaction ( tx_hash ) Out[13]: {u'blockHash': u'0x0000000000000000000000000000000000000000000000000000000000000000', u'blockNumber': None, u'from': u'0x8cf9deda0712f2291fb16739f8759e4a0a575854', u'gas': 90000, u'gasPrice': 20000000000, u'hash': u'0x46e0a5a96dbea0391adbfd401b087ad1d1dfb120684462d8652df7d8b8d7f069', u'input': u'0x', u'nonce': 24, u'r': u'0x63d0a6d39e9f57bf42e874bb268fc11c67cdb72d86cd21a65181b48063ddb531', u's': u'0x44bb390959b28e0f14d4a7891b5ac44c84816a69c2434aeafdc5c94a42997b2b', u'to': u'0xc257beaea430afb3a09640ce7f020c906331f805', u'transactionIndex': 0, u'v': u'0x42', u'value': 12000000000000000000L} Check the receipt The miner is not running, so the receipt returns nothing In [14]: web3 . eth . getTransactionReceipt ( tx_hash ) Start and stop the miner Note: web3.admin.sleepBlocks seems not te be available so I use a simple sleep. Within this sleep it could be that there are more than 1 mining cycles. In [15]: import time miner = web3 . miner miner . start ( 1 ) time . sleep ( 5 ) miner . stop () Out[15]: True Check the receipt again Now the transaction has taken place In [16]: web3 . eth . getTransactionReceipt ( tx_hash ) Out[16]: {u'blockHash': u'0x635486b2100a4a9de155b57c1a9fad4dfafcf1db1d62f9aa55bd38d6a3864869', u'blockNumber': 110, u'contractAddress': None, u'cumulativeGasUsed': 21000, u'from': u'0x8cf9deda0712f2291fb16739f8759e4a0a575854', u'gasUsed': 21000, u'logs': [], u'logsBloom': u'0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000', u'root': u'0xe7ca6b8a99b6e69f52c2834ae84276dd9b0b89daf459c7391d29e2121e49e27e', u'to': u'0xc257beaea430afb3a09640ce7f020c906331f805', u'transactionHash': u'0x46e0a5a96dbea0391adbfd401b087ad1d1dfb120684462d8652df7d8b8d7f069', u'transactionIndex': 0} Check the balances again The host should have 12 Ether less, but received 5 Ether for each mining cycle. The first VPS should have received 12 Ether. In [17]: print \"Host has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' ) print \"VPS 1 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_one ), 'ether' ) print \"VPS 2 has %d Ether\" % web3 . fromWei ( web3 . eth . getBalance ( address_vps_two ), 'ether' ) Host has 404 Ether VPS 1 has 162 Ether VPS 2 has 83 Ether In [18]: mine_bonus = 5 # Ether num_cycles = ( float ( web3 . fromWei ( web3 . eth . getBalance ( address ), 'ether' )) - prev_host_amount + amount ) / mine_bonus print \" %d mining cycle(s) are elapsed\" % num_cycles 1 mining cycle(s) are elapsed", "tags": "posts", "url": "using-ethereum-web3-client-python.html", "loc": "using-ethereum-web3-client-python.html" }, { "title": "Using the Ethereum RPC client in Python", "text": "Ethereum - RPC client In [1]: from IPython.core.interactiveshell import InteractiveShell InteractiveShell . ast_node_interactivity = \"all\" In this notebook I will try out the RPC client of Ethereum using Python. See JSON RPC for more information on JSON RPC . In [2]: import sys sys . version Out[2]: '3.6.0 |Anaconda 4.3.0 (64-bit)| (default, Dec 23 2016, 12:22:00) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]' Prepare environment Install the Python RPC client for Ethereum. Github jitsejan@jjvps:~$ pip install ethereum-rpc-client Start the blockchain making sure RPC is enabled. jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 0 --port 30333 --rpc Verify that geth is running and the account is listed. In [3]: ! geth account list Account #0: {8cf9deda0712f2291fb16739f8759e4a0a575854} keystore:///home/jitsejan/.ethereum/keystore/UTC--2017-05-01T14-58-43.532247863Z--8cf9deda0712f2291fb16739f8759e4a0a575854 Connect to the RPC client In [4]: from eth_rpc_client import Client client = Client ( host = \"127.0.0.1\" , port = \"8545\" ) Inspect the client In [5]: import pdir pdir ( client ) Out[5]: abstract class: __subclasshook__ attribute access: __delattr__ , __dir__ , __getattribute__ , __setattr__ class customization: __init_subclass__ object customization: __format__ , __hash__ , __init__ , __new__ , __repr__ , __sizeof__ , __str__ other: _coinbase_cache , _coinbase_cache_til , _nonce , async_timeout , host , is_async , port , request_queue , request_thread , results , session pickle: __reduce__ , __reduce_ex__ rich comparison: __eq__ , __ge__ , __gt__ , __le__ , __lt__ , __ne__ special attribute: __class__ , __dict__ , __doc__ , __module__ , __weakref__ descriptor: default_from_address: @property with getter, Cache the coinbase address so that we don't make two requests for every function: _make_request: call: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_call construct_json_request: get_accounts: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_accounts get_balance: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getbalance get_block_by_hash: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getblockbyhash get_block_by_number: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getblockbynumber get_block_number: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_blocknumber<F37> get_code: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getcode get_coinbase: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_coinbase get_filter_changes: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getfilterchanges get_filter_logs: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getfilterlogs get_gas_price: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_gasprice get_logs: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getlogs get_max_gas: get_nonce: get_transaction_by_hash: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_gettransactionbyhash get_transaction_receipt: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_gettransactionreceipt make_request: new_block_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_newblockfilter new_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_newfilter new_pending_transaction_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_newpendingtransactionfilter process_requests: Loop that runs in a thread to process requests synchronously. send_transaction: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_sendtransaction uninstall_filter: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_uninstallfilter wait_for_block: wait_for_transaction: Get the coinbase for the blockchain In [6]: address = client . get_coinbase () address Out[6]: '0x8cf9deda0712f2291fb16739f8759e4a0a575854' Retrieve the balance of the main address In [7]: client . get_balance ( address ) Out[7]: 135000419895999999940 Set the addresses of the two other machines: In [8]: address_vps_one = \"0xc257beaea430afb3a09640ce7f020c906331f805\" address_vps_two = \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" In [9]: client . get_balance ( address_vps_one ) client . get_balance ( address_vps_two ) Out[9]: 6999160060000000000 Out[9]: 83000420044000000060 Transaction Lets send 12 Ether from the main machine to VPS one. In [10]: amount = 12 # Ether sending_address = address receiving_address = address_vps_one 1 GWEI = 0.000000001 Ether Get the password to unlock the sending account In [11]: from getpass import getpass pw = getpass ( prompt = 'Enter the password for the sender: ' ) Enter the password for the sender: ········ Unlock the account via the command line (By lack of a better way) In [12]: command = r 'geth --exec \"personal.unlockAccount(\\\" %s \\\", \\\" %s \\\");\" attach ' % ( sending_address , pw ) result = ! $command if result [ 0 ] != 'true' : print ( 'Fail: %s ' % result [ 0 ]) else : print ( 'Account is unlocked!' ) Account is unlocked! Send the transaction In [13]: tx_hash = client . send_transaction ( to = receiving_address , _from = sending_address , value = amount * 10 ** 9 ) Check the transaction details In [14]: client . get_transaction_by_hash ( tx_hash ) Out[14]: {'blockHash': '0x0000000000000000000000000000000000000000000000000000000000000000', 'blockNumber': None, 'from': '0x8cf9deda0712f2291fb16739f8759e4a0a575854', 'gas': '0x15f90', 'gasPrice': '0x4a817c800', 'hash': '0x3d1a193ccfccc4e9ab2a411044069deeec2feef31a594bbf73726b463e8e90b4', 'input': '0x', 'nonce': '0xb', 'r': '0xe8698846a461938e800698fcc34570e0c4e9a3425f0bc441bf3e0716dab7b3e0', 's': '0x4fcd9bda8a1e98a7b0e8d953dec0cc733238c383d97393aa15c43963551f8daf', 'to': '0xc257beaea430afb3a09640ce7f020c906331f805', 'transactionIndex': '0x0', 'v': '0x42', 'value': '0x2cb417800'} Perform one mining step Execute the miner to validate the transaction. In [15]: prev_balance_sen = client . get_balance ( sending_address ) prev_balance_rec = client . get_balance ( receiving_address ) In [16]: result = ! geth --exec \"miner.start();admin.sleepBlocks(1);miner.stop();\" attach if result [ 0 ] != 'true' : print ( 'Fail: %s ' % result [ 0 ]) else : print ( \"Mining finished!\" ) Mining finished! Check if the Ether has been received In [17]: print ( \"Received %d \" % ( client . get_balance ( receiving_address ) - prev_balance_rec )) Received 12000000000 Check if the Ether was sent First check the difference in the balance. In [18]: print ( \"Difference of the sender %d \" % ( client . get_balance ( sending_address ) - prev_balance_sen )) Difference of the sender 4999999988000000000 For mining, the miner will get a mining bonus. In [19]: mining_bonus = 5000000000000000000 To get the amount of Ether sent we need to substract the mining bonus. In [20]: print ( \"Amount difference: %d \" % int ( client . get_balance ( sending_address ) - prev_balance_sen - mining_bonus )) Amount difference: -12000000000", "tags": "posts", "url": "using-ethereum-rpc-client-python.html", "loc": "using-ethereum-rpc-client-python.html" }, { "title": "Setting up a private Ethereum blockchain", "text": "Setup For this experiment, I will use three Linux machines running Ubuntu. Machine one will be used as the base, the second machine will send some Ether to the third machine. jitsejan@jjvps:~$ uname -a Linux jjvps 2 .6.32-042stab123.1 #1 SMP Wed Mar 22 15:21:30 MSK 2017 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjvps:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 jitsejan@jjschi1:~$ uname -a Linux jjschi1 2 .6.32-042stab108.8 #1 SMP Wed Jul 22 17:23:23 MSK 2015 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjschi1:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 jitsejan@jjschi2:~$ uname -a Linux jjschi2 2 .6.32-042stab094.7 #1 SMP Wed Oct 22 12:43:21 MSK 2014 x86_64 x86_64 x86_64 GNU/Linux jitsejan@jjschi2:~$ geth version Geth Version: 1 .6.0-stable Git Commit: facc47cb5cec97b22c815a0a6118816a98f39876 Architecture: amd64 Protocol Versions: [ 63 62 ] Network Id: 1 Go Version: go1.8.1 Operating System: linux GOPATH = GOROOT = /usr/lib/go-1.8 Create new user accounts Repeat the following for all the machines. jitsejan@jjschi1:~$ geth account new WARN [ 05 -01 | 06 :51:51 ] No etherbase set and no accounts found as default Your new account is locked with a password. Please give a password. Do not forget this password. Passphrase: Repeat passphrase: Address: { fb82f6d873addc0032a08aaa05bb1c338ce49b45 } Create a genesis file Create a genesis file with the 3 addresses from the newly created accounts. Set an initial balance to the accounts so we can transfer some 'money'. Set the gasLimit to the maximum and the difficulty low. jitsejan@jjvps:~$ nano jitsejansGenesis.json { \"config\" : { \"chainId\" : 15 , \"homesteadBlock\" : 0 , \"eip155Block\" : 0 , \"eip158Block\" : 0 }, \"nonce\" : \"0x0000000000000042\" , \"mixhash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\" , \"difficulty\" : \"0x4000\" , \"alloc\" : {}, \"coinbase\" : \"0x0000000000000000000000000000000000000000\" , \"timestamp\" : \"0x00\" , \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\" , \"gasLimit\" : \"0xffffffff\" , \"alloc\" : { \"0xfb82f6d873addc0032a08aaa05bb2c338ce49b45\" : { \"balance\" : \"20000000000000000000\" }, \"0xc257beaea430afb3a09640ce7f020c906331f805\" : { \"balance\" : \"40000000000000000000\" }, \"0xe86ee31b7d32b743907fa7438c422a1803717deb\" : { \"balance\" : \"40000000000000000000\" } } } Copy the genesis file to the second machine and third machine. jitsejan@jjschi1:~$ nano jitsejansGenesis.json jitsejan@jjschi2:~$ nano jitsejansGenesis.json Initialize the blockchain Initialize the blockchains on all three machines. jitsejan@jjsvps:~$ geth init jitsejansGenesis.json INFO [ 05 -01 | 06 :56:35 ] Allocated cache and file handles database = /home/jitsejan/.ethereum/geth/chaindata cache = 128 handles = 1024 INFO [ 05 -01 | 06 :56:35 ] Writing custom genesis block INFO [ 05 -01 | 06 :56:35 ] Successfully wrote genesis state hash = 86a3b9…deee1b Start the blockchain On the first machine, start the blockchain. jitsejan@jjvps:~$ geth --networkid 23 --nodiscover --maxpeers 2 --port 30333 console Get information about the hosting node. > admin.nodeInfo.enode \"enode://342a11d352151b3dfeb78db02a4319e1255c9fb49bc9a1dc44485f7c1bca9cc638540833e4577016f9a6180d1e911d907280af9b3892c53120e1e30619594eba@[::]:30333?discport=0\" Connect to the blockchain With the node information from the previous step, we can now create a static nodes files on the second and third machine to connect to the running blockchain. jitsejan@jjschi1~$ nano ~/.ethereum/static-nodes.json Replace the [::] with the IP address of the first machine and copy the file on the third machine too. [ \"enode://84f9c7f807a58f98643ac2bff9ea6691bf6be36fe6d0ccd0ad838a83501d16c1027269a82c3251104a10da5982e4fe905de41ae84dd44ba78e8cfb1659d355e8@192.123.345.567:30303?discport=0\" ] Restart the blockchain on the first machine. jitsejan@jjsvps:~$ geth --networkid 23 --nodiscover --maxpeers 1 --port 30333 console > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 20 Connect to the blockchain with the second machine and third machine. jitsejan@jjschi1:~$ geth --networkid 23 --port 30333 console > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 40 Once you perform a mining action, by default Ether will reward you with 5 ether. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 40 > miner.start () ; admin.sleepBlocks ( 1 ) ; miner.stop () ; > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 45 Perform a transaction Unlock the sending machine > personal.unlockAccount ( '0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54' ) Unlock account 0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54 Passphrase: true Send the transaction > eth.sendTransaction ({ from: '0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54' , to: '0xfb82f6d873addc0032a08aaa05bb1c338ce49b45' , value: web3.toWei ( 23 , \"ether\" )}) INFO [ 05 -01 | 15 :09:47 ] Submitted transaction fullhash = 0x9ea76acbba2ad0bf65dc9b4295bfd7f2836435329a1fee9162b0649f35855ad3 recipient = 0xfb82f6d873addc0032a08aaa05bb2c338ce49b45 \"0x9ea76acbba2ad0bf65dc9b4295bfd7f2836435329a1fee9162b0649f35855ad3\" Check for pending transactions. The transaction should be queued. > eth.pendingTransactions [{ blockHash: null, blockNumber: null, from: \"0x1c1ab1dcc7054c35a6029b0904cbead5aab37c54\" , gas: 90000 , gasPrice: 20000000000 , hash: \"0xf63024c9828ff5b77e63c118667394b285735da9ad53d01bf44aa8044b824955\" , input: \"0x\" , nonce: 0 , r: \"0x14ac03d0f4f55b4aa73b4f1f9f04752174bdf304366c994e8e4d26448e7decba\" , s: \"0x3ca7ab2f856d5e1edd6b6429df9b7be7a3c08d4afd4b2ac5a4ca9bdad2ec0caf\" , to: \"0xfb82f6d873addc0032a08aaa05bb1c338ce49b45\" , transactionIndex: 0 , v: \"0x41\" , value: 3000000000000000000 > net.peerCount 1 > net.listening true > txpool.status { pending: 1 , queued: 0 } On the receiving machine we start the mining to receive the transaction. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 96 > miner.start ( 1 ) ; admin.sleepBlocks ( 1 ) ; miner.stop () ; INFO [ 05 -01 | 10 :51:14 ] Updated mining threads threads = 1 INFO [ 05 -01 | 10 :51:14 ] Starting mining operation INFO [ 05 -01 | 10 :51:14 ] Commit new mining work number = 96 txs = 1 uncles = 0 elapsed = 335 .481µs INFO [ 05 -01 | 10 :51:14 ] 🔗 block reached canonical chain number = 14 hash = b323e7…3daf34 INFO [ 05 -01 | 10 :51:20 ] Successfully sealed new block number = 96 hash = 8d2949…3f8a32 INFO [ 05 -01 | 10 :51:20 ] 🔨 mined potential block number = 96 hash = 8d2949…3f8a32 INFO [ 05 -01 | 10 :51:20 ] Commit new mining work number = 97 txs = 0 uncles = 0 elapsed = 772 .386µs true > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 124 .00042 On the sending machine we check again the balance. > balance = web3.fromWei ( eth.getBalance ( eth.accounts [ 0 ]) , \"ether\" ) ; 405 .99958 We can see that indeed the 23 Ether got deducted, plus 0.00042 Ether to pay for the gas. The balance of the receiving machine got another 5 Ether for the mining action and got paid for the gas. If we perform a transaction from the second to the third machine, but perform the mining on the first machine, the third machine will only get the amount transferred while the first machine receives the mining bonus and the payment for the gas.", "tags": "posts", "url": "setting-up-private-ethereum-blockchain.html", "loc": "setting-up-private-ethereum-blockchain.html" }, { "title": "Setting up Ether on my VPS", "text": "Installation steps: jitsejan@jjschi2:~$ sudo apt-get install software-properties-common jitsejan@jjschi2:~$ sudo add-apt-repository -y ppa:ethereum/ethereum jitsejan@jjschi2:~$ sudo apt-get update jitsejan@jjschi2:~$ sudo apt-get install ethereum Get a new account with: jitsejan@jjschi2:~$ geth account new jitsejan@jjschi2:~$ geth account list Start a screen and start mining: jitsejan@jjschi2:~$ geth --mine To connect to this session and check your balance: jitsejan@jjschi2:~$ geth attach jitsejan@jjschi2:~$ > eth.getBalance ( eth.accounts [ 0 ]) I am not sure if my slow VPS will ever successfully mine any Ether, but since I want to know the basics of Ether, this is a good starting point.", "tags": "posts", "url": "setting-up-ether-on-vps.html", "loc": "setting-up-ether-on-vps.html" }, { "title": "Creating a dashboard with MEAN.JS", "text": "I simply keep track of the amount, price and location and try to display it in interesting graphs. This tutorial is mainly an attempt to understand the MEAN stack and work with D3.js. Installation of MEAN.JS I will skip the biggest part of installing MEAN.JS itself, since it is clearly explained on their website. Follow the instructions from MEAN.JS to generate the scaffold. jitsejan@jjvps:~/code$ npm install -g yo jitsejan@jjvps:~/code$ npm install -g generator-meanjs jitsejan@jjvps:~/code$ yo meanjs Answer 'No' to all questions. Initialization jitsejan@jjvps:~/code$ cd mean-dashboard/ jitsejan@jjvps:~/code/mean-dashboard$ git init jitsejan@jjvps:~/code/mean-dashboard$ git add . jitsejan@jjvps:~/code/mean-dashboard$ git commit -m \"Initial commit\" jitsejan@jjvps:~/code/mean-dashboard$ grunt At this point you should see the boilerplate of the MEAN.JS application. By changing the modules/core/client/views/home.client.view.html the frontpage can be changed. I stripped it down to the following. < section ng-controller = \"HomeController\" > < div class = \"jumbotron text-center\" > < div class = \"row\" > < p class = \"lead\" > JJ's dashboards </ p > </ div > </ div > </ section > I also changed the title of the page by adapting config/env/default.js. Change the database Change MongoDB url in config/env/development.js from: uri : process . env . MONGOHQ_URL || process . env . MONGOLAB_URI || 'mongodb://' + ( process . env . DB_1_PORT_27017_TCP_ADDR || 'localhost' ) + '/mean-dev' , to uri : process . env . MONGOHQ_URL || process . env . MONGOLAB_URI || 'mongodb://' + ( process . env . DB_1_PORT_27017_TCP_ADDR || 'localhost' ) + '/mean-dashboard' , Create a CRUD-module jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:crud-module refills Next I want to add more fields to the refills by changing the model. The fields I want are: Date of the refill [required] Amount of fuel [required] Litre price [required] Total cost [required] Address [required] Type of fuel [required] Longitude of the location [optional] Latitude of the location [optional] Distance [optional] To modify the model, change the code in modules/refills/server/models/refill.server.model.js. 'use strict' ; /** * Module dependencies. */ var mongoose = require ( 'mongoose' ), Schema = mongoose . Schema ; /** * Refill Schema */ var RefillSchema = new Schema ({ name : { type : String , default : '' , required : 'Please fill Refill name' , trim : true }, created : { type : Date , default : Date . now }, user : { type : Schema . ObjectId , ref : 'User' } }); mongoose . model ( 'Refill' , RefillSchema ); New code: var RefillSchema = new Schema ({ name : { type : String , default : '' , required : 'Please fill Refill name' , trim : true }, date : { type : Date , required : 'Please fill Refill date' }, kilometers : { type : Number , default : 0 , required : 'Please fill Refill kilometers' }, volume : { type : Number , default : 0 , required : 'Please fill Refill volume' }, price : { type : Number , default : 0 , required : 'Please fill Refill litre price' }, cost : { type : Number , default : 0 , required : 'Please fill Refill cost' }, created : { type : Date , default : Date . now }, user : { type : Schema . ObjectId , ref : 'User' } }); To create a new instance, I also updated the form in modules/refills/client/form-refill.client.view.html , but since this is pretty straight forward I will not show the code here. At this point you should be able to create and list the refills. Update the other views to show the new fields. Create the line chart Create a directive and choose refills as the module and 'line-chart' as the name. jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive line-chart This will create the file refills/client/directives/line-chart.client.directive.js . Install D3 via Bower. jitsejan@jjvps:~/code/mean-dashboard$ bower install d3 --save Add the JS file to the default config in 'config/assets/default.js' to the client JS. ... 'public/lib/d3/d3.min.js' , ... The challenging part will be creating the D3 visualizations. The first graph is a line chart and the directive will have the following content. ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'lineChart' , lineChart ); lineChart . $inject = [ '$window' ]; function lineChart ( $window ) { return { template : '<svg width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var data = scope . vm . refills ; console . log ( d3 . version ); console . log ( data ); var svg = d3 . select ( 'svg' ), margin = { top : 20 , right : 20 , bottom : 30 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // 2017-04-18T22:35:19.352Z var parseTime = d3 . utcParse ( '%Y-%m-%dT%H:%M:%S.%LZ' ); var x = d3 . scaleTime (). rangeRound ([ 0 , width ]); var y = d3 . scaleLinear (). rangeRound ([ height , 0 ]); var line = d3 . line () . x ( function ( d ) { return x ( parseTime ( d . date )); }) . y ( function ( d ) { return y ( d . kilometers ); }); x . domain ( d3 . extent ( data , function ( d ) { return parseTime ( d . date ); })); y . domain ( d3 . extent ( data , function ( d ) { return d . kilometers ; })); g . append ( 'g' ) . attr ( 'transform' , 'translate(0,' + height + ')' ) . call ( d3 . axisBottom ( x )) . select ( '.domain' ) . remove (); g . append ( 'g' ) . call ( d3 . axisLeft ( y )) . append ( 'text' ) . attr ( 'fill' , '#000' ) . attr ( 'transform' , 'rotate(-90)' ) . attr ( 'y' , 6 ) . attr ( 'dy' , '0.71em' ) . attr ( 'text-anchor' , 'end' ) . text ( 'Amount of kilometers' ); g . append ( 'path' ) . datum ( data ) . attr ( 'fill' , 'none' ) . attr ( 'stroke' , 'steelblue' ) . attr ( 'stroke-linejoin' , 'round' ) . attr ( 'stroke-linecap' , 'round' ) . attr ( 'stroke-width' , 1.5 ) . attr ( 'd' , line ); } }; } })(); Finally the graph needs to be added to a view. In my case I have added it to the list view of the refills. < div ng-if = \"vm.refills.$resolved && vm.refills.length\" > < div line-chart ></ div > </ div > Important note: I have used the ng-if statement in order to have my data available in the directive. Without the if-statement I was not able to get the data in properly. When you navigate to the refills list page the graph will now be visible. Some additional graphs. Please note that currently the graphs are not-responsive and rescaling the window will not resize the graph. Bar chart jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive bar-chart modules/refills/client/directives/bar-chart.client.directive.js ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'barChart' , barChart ); barChart . $inject = [ '$window' ]; function barChart ( $window ) { return { template : '<svg class=\"bar-chart\" width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var data = scope . vm . refills ; // SVG var svg = d3 . select ( 'svg.bar-chart' ), margin = { top : 20 , right : 20 , bottom : 130 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Volume per refill' ); var parseTime = d3 . time . format ( '%Y-%m-%dT%H:%M:%S.%LZ' ). parse ; var x = d3 . scale . ordinal (). rangeRoundBands ([ 0 , width ], 0.5 ); var y = d3 . scale . linear (). range ([ height , 0 ]); x . domain ( data . map ( function ( d ) { return d . date ; })); y . domain ([ 0 , d3 . max ( data , function ( d ) { return d . volume ; })]); // X axis g . append ( 'g' ) . attr ( 'transform' , 'translate(0,' + height + ')' ) . attr ( 'class' , 'x axis' ) . call ( d3 . svg . axis (). scale ( x ). orient ( 'bottom' ). tickFormat ( function ( d ){ return parseTime ( d ). toISOString (). substring ( 0 , 10 );})) . selectAll ( 'text' ) . style ( 'text-anchor' , 'end' ) . attr ( 'dx' , '-.8em' ) . attr ( 'dy' , '.15em' ) . attr ( 'transform' , function ( d ) { return 'rotate(-65)' ; }); // Y axis g . append ( 'g' ) . call ( d3 . svg . axis (). scale ( y ). orient ( 'left' )); // Bars g . selectAll ( '.bar' ) . data ( data ) . enter (). append ( 'rect' ) . attr ( 'class' , 'bar' ) . attr ( 'x' , function ( d ) { return x ( d . date ); }) . attr ( 'width' , x . rangeBand ()) . attr ( 'y' , function ( d ) { return y ( d . volume ); }) . attr ( 'height' , function ( d ) { return height - y ( d . volume ); }); } }; } })(); World map jitsejan@jjvps:~/code/mean-dashboard$ yo meanjs:angular-directive world-map modules/refills/client/directives/world-map.client.directive.js ( function () { 'use strict' ; angular . module ( 'refills' ) . directive ( 'worldMap' , worldMap ); worldMap . $inject = [ '$window' ]; function worldMap ( $window ) { return { template : '<svg class=\"world-map\" width=\"960\" height=\"500\"></svg>' , restrict : 'EA' , link : function postLink ( scope , element , attrs ) { var d3 = $window . d3 ; var topojson = $window . topojson ; var data = scope . vm . refills ; var svg = d3 . select ( 'svg.world-map' ), margin = { top : 20 , right : 20 , bottom : 130 , left : 50 }, width = + svg . attr ( 'width' ) - margin . left - margin . right , height = + svg . attr ( 'height' ) - margin . top - margin . bottom , scale0 = ( width - 1 ) / 2 / Math . PI , g = svg . append ( 'g' ). attr ( 'transform' , 'translate(' + margin . left + ',' + margin . top + ')' ); svg . append ( 'rect' ) . attr ( 'class' , 'overlay' ) . attr ( 'width' , width ) . attr ( 'height' , height ); // Define the div for the tooltip var div = d3 . select ( 'body' ). append ( 'div' ) . attr ( 'class' , 'tooltip' ) . style ( 'opacity' , 0 ); var projection = d3 . geo . mercator () . center ([ 0 , 5 ]) . scale ( 200 ) . rotate ([ 0 , 0 ]); var path = d3 . geo . path () . projection ( projection ); var zoom = d3 . behavior . zoom () . on ( 'zoom' , function () { g . attr ( 'transform' , 'translate(' + d3 . event . translate . join ( ',' ) + ')scale(' + d3 . event . scale + ')' ); g . selectAll ( 'circle' ) . attr ( 'd' , path . projection ( projection )); g . selectAll ( 'path' ) . attr ( 'd' , path . projection ( projection )); }); // Graph title g . append ( 'text' ) . attr ( 'x' , ( width / 2 )) . attr ( 'y' , 0 - ( margin . top / 3 )) . attr ( 'text-anchor' , 'middle' ) . style ( 'font-size' , '16px' ) . text ( 'Locations' ); d3 . json ( 'https://unpkg.com/world-atlas@1/world/50m.json' , function ( error , world ) { if ( error ) throw error ; g . append ( 'path' ) . datum ({ type : 'Sphere' }) . attr ( 'class' , 'sphere' ) . attr ( 'd' , path ); g . append ( 'path' ) . datum ( topojson . merge ( world , world . objects . countries . geometries )) . attr ( 'class' , 'land' ) . attr ( 'd' , path ); g . append ( 'path' ) . datum ( topojson . mesh ( world , world . objects . countries , function ( a , b ) { return a !== b ; })) . attr ( 'class' , 'boundary' ) . attr ( 'd' , path ); g . selectAll ( 'circle' ) . data ( data ) . enter () . append ( 'circle' ) . attr ( 'cx' , function ( d ) { return projection ([ d . longitude , d . latitude ])[ 0 ]; }) . attr ( 'cy' , function ( d ) { return projection ([ d . longitude , d . latitude ])[ 1 ]; }) . attr ( 'r' , 5 ) . style ( 'fill' , 'red' ); }); svg . call ( zoom ) . call ( zoom . event ); } }; } })();", "tags": "posts", "url": "creating-dashboard-with-meanjs.html", "loc": "creating-dashboard-with-meanjs.html" }, { "title": "Using R and Python together in a notebook", "text": "Installation $ sudo apt-get install python-rpy2 $ conda install -c r r-essentials $ conda install -c r r-rjson $ pip install rpy2 Load the magic In [1]: % load_ext rpy2.ipython Create a dataframe Use Python and pandas to create a dataframe. In [2]: import pandas as pd df_from_python = pd . DataFrame ({ 'A' : [ 4 , 3 , 5 , 2 , 1 , 7 , 7 , 5 , 9 ], 'B' : [ 0 , 4 , 3 , 6 , 7 , 10 , 11 , 9 , 13 ], 'C' : [ 1 , 2 , 3 , 1 , 2 , 3 , 1 , 2 , 3 ]}) df_from_python Out[2]: A B C 0 4 0 1 1 3 4 2 2 5 3 3 3 2 6 1 4 1 7 2 5 7 10 3 6 7 11 1 7 5 9 2 8 9 13 3 Create plot in R Import the dataframe df with the -i argument. In [3]: %% R - i df_from_python require ( ggplot2 ) # Plot the DataFrame df ggplot ( data = df_from_python ) + geom_point ( aes ( x = A , y = B , color = C )) + ggtitle ( 'R scatter!' ) /home/jitsejan/anaconda3/lib/python3.6/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Loading required package: ggplot2 warnings.warn(x, RRuntimeWarning) Create a dataframe using R Create a dataframe and export it using -o . In [4]: %% R - o df_from_r d <- c ( 2 , 6 , 6 , 8 , 4 , 7 , 5 , 9 , 3 ) e <- c ( 3 , 1 , 4 , 3 , 6 , 7 , 10 , 11 , 9 ) f <- c ( 3 , 1 , 2 , 3 , 1 , 2 , 3 , 1 , 2 ) df_from_r <- data.frame ( d , e , f ) Now the variable is available for Python. In [5]: df_from_r Out[5]: d e f 1 2.0 3.0 3.0 2 6.0 1.0 1.0 3 6.0 4.0 2.0 4 8.0 3.0 3.0 5 4.0 6.0 1.0 6 7.0 7.0 2.0 7 5.0 10.0 3.0 8 9.0 11.0 1.0 9 3.0 9.0 2.0 Create plot with Python In [6]: import matplotlib.pyplot as plt import matplotlib % matplotlib inline matplotlib . style . use ( 'ggplot' ) plt . scatter ( x = df_from_r [ 'd' ], y = df_from_r [ 'e' ], c = df_from_r [ 'f' ]) plt . title ( 'Python scatter!' ) plt . xlabel ( 'A' ) plt . ylabel ( 'B' ) fig = matplotlib . pyplot . gcf () fig . set_size_inches ( 10 , 10 )", "tags": "posts", "url": "r-in-notebook.html", "loc": "r-in-notebook.html" }, { "title": "MongoDB - First try", "text": "My first experience with MongoDB . I will install MongoDB and Pymongo, insert some data and query it. Next step will be to tryout monary , but for this notebook it is out of scope. Installation steps Run the following commands to install MongoDB. jitsejan@vps:/$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 jitsejan@jjvps:/$ echo \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list jitsejan@jjvps:/$ sudo apt-get update jitsejan@jjvps:/$ sudo apt-get install -y mongodb-org Start MongoDB jitsejan@jjvps:/$ sudo service mongod start In [1]: ! tail /var/log/mongodb/mongod.log 2017-04-05T10:33:15.620-0400 W FTDC [initandlisten] Error checking directory '/sys/block': No such file or directory 2017-04-05T10:33:15.621-0400 I FTDC [initandlisten] Initializing full-time diagnostic data capture with directory '/var/lib/mongodb/diagnostic.data' 2017-04-05T10:33:15.621-0400 I NETWORK [thread1] waiting for connections on port 27017 2017-04-05T10:34:27.470-0400 I NETWORK [conn4] received client metadata from 127.0.0.1:34370 conn4: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:40:10.716-0400 I COMMAND [conn4] dropDatabase nintendo_db starting 2017-04-05T10:40:10.723-0400 I COMMAND [conn4] dropDatabase nintendo_db finished 2017-04-05T10:40:26.823-0400 I NETWORK [conn5] received client metadata from 127.0.0.1:42910 conn5: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:40:26.831-0400 I NETWORK [conn6] received client metadata from 127.0.0.1:42912 conn6: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:53:00.408-0400 I NETWORK [conn7] received client metadata from 127.0.0.1:35700 conn7: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } 2017-04-05T10:53:00.410-0400 I NETWORK [conn8] received client metadata from 127.0.0.1:35702 conn8: { driver: { name: \"PyMongo\", version: \"3.4.0\" }, os: { type: \"Linux\", name: \"debian stretch/sid\", architecture: \"x86_64\", version: \"2.6.32-042stab120.16\" }, platform: \"CPython 3.6.0.final.0\" } Install Python module Use Pymongo to communicate with MongoDB. In [2]: ! pip install pymongo Requirement already satisfied: pymongo in /home/jitsejan/anaconda3/lib/python3.6/site-packages Connect to MongoDB In [3]: import pymongo print ( pymongo . version ) client = pymongo . MongoClient ( 'mongodb://localhost:27017/' ) 3.4.0 Check which databases already exist In [4]: client . database_names () Out[4]: ['admin', 'local', 'nintendo_db'] Create a new database You can create a database by simply selecting the non-existing database. Only when a document is written, the database will physically be created. In [5]: db = client . nintendo_db db Out[5]: Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'nintendo_db') Create a new collection In [6]: characters = db . characters characters Out[6]: Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'nintendo_db'), 'characters') Create documents For simplicity, I will use the data that I have used in another notebook for creating documents. In [7]: import pandas as pd character_df = pd . read_csv ( '../data/nintendo_characters.csv' ) character_df Out[7]: id name description color occupation picture 0 2 Luigi This is Luigi green plumber https://upload.wikimedia.org/wikipedia/en/f/f1... 1 1 Mario This is Mario red plumber https://upload.wikimedia.org/wikipedia/en/9/99... 2 3 Peach My name is Peach pink princess https://s-media-cache-ak0.pinimg.com/originals... 3 4 Toad I like funghi red NaN https://upload.wikimedia.org/wikipedia/en/d/d1... In [8]: import json characters_dict = character_df . to_dict ( orient = 'records' ) print ( json . dumps ( characters_dict [ 0 ], indent = 4 )) { \"id\": 2, \"name\": \"Luigi\", \"description\": \"This is Luigi\", \"color\": \"green\", \"occupation\": \"plumber\", \"picture\": \"https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png\" } In [9]: for character in characters_dict : character_id = characters . insert_one ( character ) . inserted_id print ( character_id ) 58e504df6221ac77482eae4e 58e504df6221ac77482eae4f 58e504df6221ac77482eae50 58e504df6221ac77482eae51 Verify the new collection has been created In [10]: db . collection_names ( include_system_collections = False ) Out[10]: ['characters'] Verify the characters have been added Check the number of documents for the characters collection. In [11]: characters . count () Out[11]: 8 Check if Luigi is in the database. In [12]: characters . find_one ({ \"name\" : \"Luigi\" }) Out[12]: {'_id': ObjectId('58e501db6221ac72c8a3106b'), 'color': 'green', 'description': 'This is Luigi', 'id': 2, 'name': 'Luigi', 'occupation': 'plumber', 'picture': 'https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png'} Retrieve all documents in the characters collection In [13]: characters_from_db = list ( characters . find ({})) characters_from_db [ 0 ] Out[13]: {'_id': ObjectId('58e501db6221ac72c8a3106b'), 'color': 'green', 'description': 'This is Luigi', 'id': 2, 'name': 'Luigi', 'occupation': 'plumber', 'picture': 'https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png'} Find the red characters Only retrieve the name and description of the character. In [14]: red_characters = list ( characters . find ({ \"color\" : \"red\" }, { \"name\" : 1 , \"description\" : 1 , \"_id\" : 0 })) red_characters Out[14]: [{'description': 'This is Mario', 'name': 'Mario'}, {'description': 'I like funghi', 'name': 'Toad'}, {'description': 'This is Mario', 'name': 'Mario'}, {'description': 'I like funghi', 'name': 'Toad'}] Create a dataframe from the results In [15]: import pandas as pd red_characters_df = pd . DataFrame . from_dict ( red_characters ) red_characters_df Out[15]: description name 0 This is Mario Mario 1 I like funghi Toad 2 This is Mario Mario 3 I like funghi Toad Drop the database In [16]: client . drop_database ( 'nintendo_db' )", "tags": "posts", "url": "mongodb_first_try.html", "loc": "mongodb_first_try.html" }, { "title": "First experiments with NLTK", "text": "In [1]: import nltk # Run this the first time you use NLTK # nltk.download() In [2]: from nltk.tokenize import sent_tokenize , word_tokenize EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\" print ( sent_tokenize ( EXAMPLE_TEXT )) print ( word_tokenize ( EXAMPLE_TEXT )) ['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"] ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.'] Stop words Stopwords should be removed from the list of words. In [3]: from nltk.corpus import stopwords In [4]: stop_words = set ( stopwords . words ( 'english' )) In [5]: word_tokens = word_tokenize ( EXAMPLE_TEXT ) filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] print ( word_tokens ) print ( filtered_sentence ) ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.'] ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'weather', 'great', ',', 'Python', 'awesome', '.', 'sky', 'pinkish-blue', '.', \"n't\", 'eat', 'cardboard', '.'] Example one - Bob Dylan - Hurricane's lyrics Lets analyse the lyrics from Bob Dylan's Hurricane. First we get the lyrics from his website: In [6]: import lxml.html import requests response = requests . get ( 'https://bobdylan.com/songs/hurricane/' ) etree = lxml . html . fromstring ( response . text ) lyrics = etree . cssselect ( 'div[class*= \\' lyrics \\' ]' )[ 0 ] . text_content () Tokenize the lyrics based on words: In [7]: word_tokens = word_tokenize ( lyrics ) Remove the stopwords from the words: In [8]: filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] Create a word frequency graph In [9]: from nltk import FreqDist fd = FreqDist ( filtered_sentence ) fd . plot ( 30 , cumulative = False ) What we see in the graph above is that punctuation is disturbing the graph. We use a regular expression now to remove these characters and only select words. In [10]: from nltk.tokenize import RegexpTokenizer tokenizer = RegexpTokenizer ( r '\\w+' ) word_tokens = tokenizer . tokenize ( lyrics ) filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] Lets create the graph again: In [11]: fd = FreqDist ( filtered_sentence ) fd . plot ( 30 , cumulative = False ) Example two - Wikipedia about Nintendo In [12]: import wikipedia nintendo = wikipedia . page ( \"Nintendo\" ) word_tokens = word_tokenize ( nintendo . content ) In [13]: filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] In [14]: fd = FreqDist ( filtered_sentence ) fd . plot ( 10 , cumulative = False ) In [15]: tokenizer = RegexpTokenizer ( r '\\w+' ) word_tokens = tokenizer . tokenize ( nintendo . content ) In [16]: filtered_sentence = [ w for w in word_tokens if not w . lower () in stop_words ] In [17]: fd = FreqDist ( filtered_sentence ) fd . plot ( 10 , cumulative = False )", "tags": "posts", "url": "nltk-first-experiments.html", "loc": "nltk-first-experiments.html" }, { "title": "Basic search with Elasticsearch", "text": "Inspired by this tutorial I tried to continue investigating Elasticsearch since I would like to use a fast indexing tool for the data I am gathering and the applications I am developing. Install the Python library for Elasticsearch https://elasticsearch-py.readthedocs.io/en/master/ $ pip install elasticsearch Note: on my Mac I installed Elasticsearch through Brew $ brew install elasticsearch $ brew services start elasticsearch Creating the data Read the CSV files Read the character data In [1]: import pandas as pd character_df = pd . read_csv ( 'data/nintendo_characters.csv' ) character_df Out[1]: id name description color occupation picture 0 2 Luigi This is Luigi green plumber https://upload.wikimedia.org/wikipedia/en/f/f1... 1 1 Mario This is Mario red plumber https://upload.wikimedia.org/wikipedia/en/9/99... 2 3 Peach My name is Peach pink princess https://s-media-cache-ak0.pinimg.com/originals... 3 4 Toad I like funghi red NaN https://upload.wikimedia.org/wikipedia/en/d/d1... Remove the NaN In [2]: character_df . occupation = character_df . occupation . fillna ( '' ) Read the world data In [3]: world_df = pd . read_csv ( 'data/super_mario_3_worlds.csv' , sep = ';' ) world_df Out[3]: id world name image description picture 0 1 World 1 Grass Land Grass Land.PNG Grass Land is the first world of the game. It ... https://www.mariowiki.com/images/thumb/f/fa/Gr... 1 2 World 2 Desert Land World2SMB3.PNG Desert Land is the second world of the game. I... https://www.mariowiki.com/images/thumb/d/d1/Wo... 2 3 World 3 Water Land Sea Side.PNG Water Land is a water-themed region that was r... https://www.mariowiki.com/images/thumb/b/b7/Se... 3 4 World 4 Giant Land SMAS-Big Island Map.PNG Giant Land is mainly composed of an island in ... https://www.mariowiki.com/images/thumb/9/9c/SM... 4 5 World 5 Sky Land Sky world.PNG Sky Land is the world that has been conquered ... https://www.mariowiki.com/images/thumb/6/69/Sk... 5 6 World 6 Ice Land SMB36.PNG Ice Land is an area covered in snow and ice. T... https://www.mariowiki.com/images/thumb/4/40/SM... 6 7 World 7 Pipe Land Pipe maze.PNG Pipe Land is a series of small islands in a ne... https://www.mariowiki.com/images/thumb/a/aa/Pi... 7 8 World 8 Dark Land Dark land2.PNG The eighth and final world is ruled by King Bo... https://www.mariowiki.com/images/thumb/0/01/Da... 8 9 World 9 Warp Zone World 9.PNG World 9 is only accessible by a Warp Whistle. ... https://www.mariowiki.com/images/thumb/0/09/Wo... Setup Elasticsearch Create the parameters In [4]: ES_HOST = { \"host\" : \"localhost\" , \"port\" : 9200 } INDEX_NAME = 'nintendo' TYPE_NAME = 'character' ID_FIELD = 'id' Setup the Elasticsearch connector In [5]: from elasticsearch import Elasticsearch es = Elasticsearch ( hosts = [ ES_HOST ]) Create the index Create the index for nintendo if it does not exists, otherwise first delete it. In [6]: if es . indices . exists ( INDEX_NAME ): print ( \"Deleting the ' %s ' index\" % ( INDEX_NAME )) res = es . indices . delete ( index = INDEX_NAME ) print ( \"Acknowledged: ' %s '\" % ( res [ 'acknowledged' ])) request_body = { \"settings\" : { \"number_of_shards\" : 1 , \"number_of_replicas\" : 0 } } print ( \"Creating the ' %s ' index!\" % ( INDEX_NAME )) res = es . indices . create ( index = INDEX_NAME , body = request_body ) print ( \"Acknowledged: ' %s '\" % ( res [ 'acknowledged' ])) Deleting the 'nintendo' index Acknowledged: 'True' Creating the 'nintendo' index! Acknowledged: 'True' Create the bulk data Loop through the dataframe and create the data to insert into the index. In [7]: bulk_data = [] In [8]: for index , row in character_df . iterrows (): data_dict = {} for i in range ( len ( row )): data_dict [ character_df . columns [ i ]] = row [ i ] op_dict = { \"index\" : { \"_index\" : 'nintendo' , \"_type\" : 'character' , \"_id\" : data_dict [ 'id' ] } } bulk_data . append ( op_dict ) bulk_data . append ( data_dict ) In [9]: for index , row in world_df . iterrows (): data_dict = {} for i in range ( len ( row )): data_dict [ world_df . columns [ i ]] = row [ i ] op_dict = { \"index\" : { \"_index\" : 'nintendo' , \"_type\" : 'world' , \"_id\" : data_dict [ 'id' ] } } bulk_data . append ( op_dict ) bulk_data . append ( data_dict ) Insert the data into the index In [10]: import json print ( \"Bulk indexing...\" ) res = es . bulk ( index = INDEX_NAME , body = bulk_data , refresh = True ) Bulk indexing... Query using CURL In [11]: ! curl -XGET 'http://localhost:9200/_search?pretty' { \"took\" : 5, \"timed_out\" : false, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"hits\" : { \"total\" : 3295, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"id\" : 2, \"name\" : \"Luigi\", \"description\" : \"This is Luigi\", \"color\" : \"green\", \"occupation\" : \"plumber\", \"picture\" : \"https://upload.wikimedia.org/wikipedia/en/f/f1/LuigiNSMBW.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"id\" : 1, \"name\" : \"Mario\", \"description\" : \"This is Mario\", \"color\" : \"red\", \"occupation\" : \"plumber\", \"picture\" : \"https://upload.wikimedia.org/wikipedia/en/9/99/MarioSMBW.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"id\" : 3, \"name\" : \"Peach\", \"description\" : \"My name is Peach\", \"color\" : \"pink\", \"occupation\" : \"princess\", \"picture\" : \"https://s-media-cache-ak0.pinimg.com/originals/d2/4d/77/d24d77cfbba789256c9c1afa1f69b385.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"character\", \"_id\" : \"4\", \"_score\" : 1.0, \"_source\" : { \"id\" : 4, \"name\" : \"Toad\", \"description\" : \"I like funghi\", \"color\" : \"red\", \"occupation\" : \"\", \"picture\" : \"https://upload.wikimedia.org/wikipedia/en/d/d1/Toad_3D_Land.png\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"1\", \"_score\" : 1.0, \"_source\" : { \"id\" : 1, \"world\" : \"World 1\", \"name\" : \"Grass Land\", \"image\" : \"Grass Land.PNG\", \"description\" : \"Grass Land is the first world of the game. It was attacked by Larry Koopa, who stole the wand of the Grass Land king and turned him into a dog (or a Cobrat from Super Mario Bros. 2 in the remake). The landscape itself is mainly composed of plains, surrounded by hills and even some cliffs in the south. A fortress can be found in the middle of Grass Land, and the king's castle lies to the southeast, surrounded by a circular moat. The enemies Mario encounters here are regular ones, like Goombas, Koopa Troopas and Piranha Plants. The world features a Spade Panel, two Toad Houses and six levels, of which four have to be cleared to reach the king's castle.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/f/fa/Grass_Land.PNG/200px-Grass_Land.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"2\", \"_score\" : 1.0, \"_source\" : { \"id\" : 2, \"world\" : \"World 2\", \"name\" : \"Desert Land\", \"image\" : \"World2SMB3.PNG\", \"description\" : \"Desert Land is the second world of the game. It is a region within a vast desert, filled with sand, palm trees and some pyramids. A fortress is located in the west part of the desert, and a quicksand field can also be found, as well as a great pyramid that the player needs to traverse in order to reach the king's castle. The king was attacked by Morton Koopa Jr., who turned him into a spider (or a Hoopster from Super Mario Bros. 2 in the remake). The world features two Spade Panels and three Toad Houses, of which one lies in a secret area behind a rock that needs to be crushed by a Hammer. The boulder also hides two Fire Brothers which stole the last Warp Whistle. Four of the five levels need to be cleared to get to the great pyramid and the castle. Desert Land houses many desert-related creatures like Fire Snakes and the extremely rare Angry Sun.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/d/d1/World2SMB3.PNG/200px-World2SMB3.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"3\", \"_score\" : 1.0, \"_source\" : { \"id\" : 3, \"world\" : \"World 3\", \"name\" : \"Water Land\", \"image\" : \"Sea Side.PNG\", \"description\" : \"Water Land is a water-themed region that was raided by Wendy O. Koopa, who turned the king into a kappa (or a Dino-Rhino from Super Mario World in the remake). While some levels take place on solid ground, most of the levels and even one of the worlds two fortresses involve water in a certain way. At the northern part of the world map, Mario will encounter drawbridges that open and close in a set pattern. The world's castle is located far to the east on a small, remote island that is only accessible through a Warp Pipe. A boat can be unlocked by using a Hammer on a rock in the south. Through it, the player can reach some bonus Spade Panels and Toad Houses. Water Land contains nine levels in total, of which one can be skipped if a certain drawbridge is closed, and houses several water creatures like Bloopers, Cheep Cheeps, and Big Berthas. The world also introduces a very rare Boo known as a Stretch.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/b/b7/Sea_Side.PNG/200px-Sea_Side.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"4\", \"_score\" : 1.0, \"_source\" : { \"id\" : 4, \"world\" : \"World 4\", \"name\" : \"Giant Land\", \"image\" : \"SMAS-Big Island Map.PNG\", \"description\" : \"Giant Land is mainly composed of an island in the vague shape of a Koopa. It is a relatively green island with plants growing on it that resemble Fire Flowers. The castle at the west coast of the island was attacked by Iggy Koopa, who transformed the king into an orange dinosaur (or Donkey Kong Jr. in the remake). The world has two fortresses, one on the east side and one on a small island in a lake in the world's center. The most prominent feature of Giant Land, which gives this world its name, is the fact that many enlarged versions of regular enemies, blocks, and environmental features can be found here. The world features four Toad Houses, two Spade Panels and six levels, of which five need to be cleared to reach the king's castle.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/9/9c/SMAS-Big_Island_Map.PNG/200px-SMAS-Big_Island_Map.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"5\", \"_score\" : 1.0, \"_source\" : { \"id\" : 5, \"world\" : \"World 5\", \"name\" : \"Sky Land\", \"image\" : \"Sky world.PNG\", \"description\" : \"Sky Land is the world that has been conquered by Roy Koopa, who has turned its king into a condor (Albatoss in the remake). It is divided into two parts: a ground part and a sky part. The player begins on the ground. The most notable feature of this area is the possibility to gain the Goomba's Shoe, an item that can be obtained in level 5-3. After clearing the levels on the ground, the player can reach a spiraling tower that reaches up to the sky. The main part of the level is located here, and there are also some creatures exclusively to this realm, namely the Para-Beetle. After clearing the tower that serves as a link between the two areas, the player can go back to the ground, but they will have to clear the tower again on their way up. If the Koopaling isn't defeated at the first try, his Airship will be able to move freely between sky and ground. There are nine levels in total, three Spade Panels, three Toad Houses and two fortresses. The castle is on the southwest part of the sky part.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/6/69/Sky_world.PNG/200px-Sky_world.PNG\" } }, { \"_index\" : \"nintendo\", \"_type\" : \"world\", \"_id\" : \"6\", \"_score\" : 1.0, \"_source\" : { \"id\" : 6, \"world\" : \"World 6\", \"name\" : \"Ice Land\", \"image\" : \"SMB36.PNG\", \"description\" : \"Ice Land is an area covered in snow and ice. The castle was attacked by Lemmy Koopa - who has turned its king into a fur seal (Monty Mole in the remake) and Mario has to venture there and reclaim the magic wand just like in the previous worlds. Before he can reach the castle however, the player has to navigate Mario through the levels of Ice Land. These levels feature frozen ground which makes movement more difficult, as Mario has poor footing on them and is likely to slip off into a bottomless pit. In some levels, the player can find ice blocks that contain coins or enemies. These blocks can only be melted with one of Fire Mario's fireballs. There are ten levels in total, three Spade Panels, two Toad Houses, and three fortresses. The castle is far to the east near the sea.\", \"picture\" : \"https://www.mariowiki.com/images/thumb/4/40/SMB36.PNG/200px-SMB36.PNG\" } } ] } } Search all worlds: curl -XGET 'http://localhost:9200/nintendo/world/_search?pretty' Pagination: curl -XGET 'http://localhost:9200/nintendo/world/_search?size=2&from=2&pretty' Specify the fields you want to be returned: curl -XGET 'http://localhost:9200/nintendo/character/_search?pretty&q=name:Luigi&fields=name,occupation' Search for the word 'pipe': curl -XGET 'http://localhost:9200/nintendo/world/_search?pretty&q=pipe'", "tags": "posts", "url": "basic-search-with-elasticsearch.html", "loc": "basic-search-with-elasticsearch.html" }, { "title": "Color recognition of images using OpenCV", "text": "As a training for myself to get more familiar with image classification and OpenCV I followed the tutorial on http://www.pyimagesearch.com/2014/08/04/opencv-python-color-detection/ . I modified it slightly to fit my environment. Imports In [1]: import cv2 from IPython.core.interactiveshell import InteractiveShell import matplotlib.pyplot as plt import numpy as np import os Settings In [2]: InteractiveShell . ast_node_interactivity = \"all\" % matplotlib inline DATA_DIR = '../input/images/' Retrieve the first image for each character. In [3]: images = { f [: - 4 ]: DATA_DIR + f for f in os . listdir ( DATA_DIR ) if f . endswith ( '.jpg' ) and '_01' in f } images Out[3]: {'luigi_01': '../input/images/luigi_01.jpg', 'mario_01': '../input/images/mario_01.jpg', 'peach_01': '../input/images/peach_01.jpg', 'toad_01': '../input/images/toad_01.jpg'} Display the Mario image using matplotlib In [4]: img = plt . imread ( images [ 'mario_01' ]) plt . imshow ( img ) plt . axis ( 'off' ); Color detection First read the images using OpenCV. In [5]: mario_image = cv2 . imread ( images [ 'mario_01' ]) luigi_image = cv2 . imread ( images [ 'luigi_01' ]) peach_image = cv2 . imread ( images [ 'peach_01' ]) toad_image = cv2 . imread ( images [ 'toad_01' ]) Next we set up the boundaries for the images. Since I am working with Mario, Luigi, Toad and Peach the easy choices are blue, red, green, pink and white. Note that the colors for OpenCV are not in in RGB order but in BGR. For example for the blue boundary the 0 is the lower bound for blue and 200 the upper bound. In [6]: blue_boundary = ([ 0 , 46 , 0 ], [ 200 , 100 , 112 ]) red_boundary = ([ 0 , 0 , 150 ], [ 40 , 21 , 255 ]) green_boundary = ([ 23 , 140 , 33 ], [ 51 , 191 , 65 ]) pink_boundary = ([ 141 , 79 , 234 ], [ 201 , 144 , 255 ]) white_boundary = ([ 200 , 200 , 200 ], [ 252 , 252 , 252 ]) # Combine the bounaries in one list boundaries = [ blue_boundary , red_boundary , green_boundary , pink_boundary , white_boundary , ] Make a list of the images for the four characters. In [7]: input_images = [ { 'image' : toad_image }, { 'image' : peach_image }, { 'image' : mario_image }, { 'image' : luigi_image } ] Now loop through the images first and for each image loop through the boundaries. The In [8]: # Loop through the images for item in input_images : outputs = [] # Loop through the boundaries for ( lower , upper ) in boundaries : # Create NumPy arrays from the boundaries lower = np . array ( lower , dtype = \"uint8\" ) upper = np . array ( upper , dtype = \"uint8\" ) # Check which pixels fall in between the boundaries and create a mask mask = cv2 . inRange ( item [ 'image' ], lower , upper ) # Apply the mask to the input image output = cv2 . bitwise_and ( item [ 'image' ], item [ 'image' ], mask = mask ) outputs . append ( output ) # Show the mask for each boundary plt . figure () plt . imshow ( cv2 . cvtColor ( np . hstack ([ item [ 'image' ], outputs [ 0 ], outputs [ 1 ], outputs [ 2 ], outputs [ 3 ], outputs [ 4 ]]), cv2 . COLOR_BGR2RGB )) plt . axis ( 'off' ); item [ 'outputs' ] = outputs plt . axis ( 'off' );", "tags": "posts", "url": "color-recognition-opencv.html", "loc": "color-recognition-opencv.html" }, { "title": "Mocking in unittests in Python", "text": "Example - Using mock Imports In [35]: import sys import unittest # Python compatibility if sys . version_info < ( 3 , 3 ): import mock else : import unittest.mock as mock The mock object Create a mock object: In [8]: m = mock . Mock () Show the attributes of the object: In [9]: dir ( m ) Out[9]: ['assert_any_call', 'assert_called_once_with', 'assert_called_with', 'assert_has_calls', 'assert_not_called', 'attach_mock', 'call_args', 'call_args_list', 'call_count', 'called', 'configure_mock', 'method_calls', 'mock_add_spec', 'mock_calls', 'reset_mock', 'return_value', 'side_effect'] Print a fake attribute. It doesn't exist, but will be shown. In [12]: m . fake_attribute Out[12]: <Mock name='mock.fake_attribute' id='2582717621248'> Again show the object. This time the fake_attribute will be shown too. In [13]: dir ( m ) Out[13]: ['assert_any_call', 'assert_called_once_with', 'assert_called_with', 'assert_has_calls', 'assert_not_called', 'attach_mock', 'call_args', 'call_args_list', 'call_count', 'called', 'configure_mock', 'fake_attribute', 'method_calls', 'mock_add_spec', 'mock_calls', 'reset_mock', 'return_value', 'side_effect'] Set a return value for the newly introduced attribute and retrieve it. In [14]: m . fake_attribute . return_value = \"Fake return value\" m . fake_attribute () Out[14]: 'Fake return value' Create another attribute, but this time assign a (fake) function to its return_value . In [108]: def print_fake_value (): print ( \"Fake function is called!\" ) m . another_attribute . return_value = print_fake_value m . another_attribute () Out[108]: <Mock name='mock.another_attribute.print_fake_value()' id='2582735302608'> Same exercise with a function with an argument: In [21]: def print_fake_value_with_arg ( argument ): print ( \"Fake argument %s \" % argument ) m . the_third_attribute . return_value = print_fake_value_with_arg m . the_third_attribute ( 'Print me' ) Out[21]: <function __main__.print_fake_value_with_arg> You can also create a custom exception by using the side_effect . It can be an exception, callable or an iterable. In [22]: m . some_function . side_effect = ValueError ( \"Super error\" ) m . some_function () --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-22-197c49fcbb5e> in <module> () 1 m . some_function . side_effect = ValueError ( \"Super error\" ) ----> 2 m . some_function ( ) C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in __call__ (_mock_self, *args, **kwargs) 915 # in the signature 916 _mock_self . _mock_check_sig ( * args , ** kwargs ) --> 917 return _mock_self . _mock_call ( * args , ** kwargs ) 918 919 C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in _mock_call (_mock_self, *args, **kwargs) 971 if effect is not None : 972 if _is_exception ( effect ) : --> 973 raise effect 974 975 if not _callable ( effect ) : ValueError : Super error To make it an iterable, the following can be used. By calling the mock several times, it will return the values until the limit of the range is reached. In [23]: m . some_iteration_thing . side_effect = range ( 2 ) m . some_iteration_thing () Out[23]: 0 In [24]: m . some_iteration_thing () Out[24]: 1 In [25]: m . some_iteration_thing () --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) <ipython-input-25-218e10d9f5de> in <module> () ----> 1 m . some_iteration_thing ( ) C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in __call__ (_mock_self, *args, **kwargs) 915 # in the signature 916 _mock_self . _mock_check_sig ( * args , ** kwargs ) --> 917 return _mock_self . _mock_call ( * args , ** kwargs ) 918 919 C:\\Users\\J-J van Waterschoot\\Anaconda3\\lib\\unittest\\mock.py in _mock_call (_mock_self, *args, **kwargs) 974 975 if not _callable ( effect ) : --> 976 result = next ( effect ) 977 if _is_exception ( result ) : 978 raise result StopIteration : Finally you can also pass a callable to the side_effect , by doing the following: In [27]: def side_function (): print ( 'This is a side function!' ) m . some_simple_function . side_effect = side_function () m . some_simple_function () This is a side function! Out[27]: <Mock name='mock.some_simple_function()' id='2582717767408'> In [28]: def side_function_with_arg ( argument ): print ( 'This is a side function with argument: %s ' % argument ) m . some_simple_function_with_arg . side_effect = side_function_with_arg m . some_simple_function_with_arg ( 'No argument!' ) This is a side function with argument: No argument! An important function of the side_effect is that you can pass it a class, which can be helpful if you are testing code and verify the behaviour of the class In [29]: class Car ( object ): def __init__ ( self , name ): self . _name = name def print_name ( self ): print ( \"Name: %s \" % self . _name ) m . a_car_attribute . side_effect = Car car = m . a_car_attribute . side_effect ( 'My red car' ) car Out[29]: <__main__.Car at 0x25955fdd6a0> In [30]: car . print_name () Name: My red car Testing Castle Lets define the castle class: In [99]: class Castle ( object ): def __init__ ( self , name ): self . name = name self . boss = 'Bowser' self . world = 'Grass Land' def access ( self , character ): if character . powerup == 'Super Mushroom' : return True else : return False def get_boss ( self ): return self . boss def get_world ( self ): return self . world We will also define a character class: In [100]: class Character ( object ): def __init__ ( self , name ): self . name = name self . powerup = '' def powerup ( self , powerup ): self . powerup = powerup def get_powerup ( self ): return self . powerup Finally we will define a testclass to test the functionality of the classes. In [101]: class CharacterTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character class \"\"\" def setUp ( self ): \"\"\" Set the castle for the test cases \"\"\" self . castle = Castle ( 'Bowsers Castle' ) def test_mock_access_denied ( self ): \"\"\" Access denied for star powerup \"\"\" mock_character = mock . Mock ( powerup = 'Starman' ) self . assertFalse ( self . castle . access ( mock_character )) def test_mock_access_granted ( self ): \"\"\" Access granted for mushroom powerup \"\"\" mock_character = mock . Mock ( powerup = 'Super Mushroom' ) self . assertTrue ( self . castle . access ( mock_character )) def test_default_castle_boss ( self ): \"\"\" Verifty the default boss is Bowser \"\"\" self . assertEqual ( self . castle . get_boss (), \"Bowser\" ) def test_default_castle_world ( self ): \"\"\" Verify the default world is Grass Land \"\"\" self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock a class method @mock . patch . object ( Castle , 'get_boss' ) def test_mock_castle_boss ( self , mock_get_boss ): mock_get_boss . return_value = \"Hammer Bro\" self . assertEqual ( self . castle . get_boss (), \"Hammer Bro\" ) self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) # Mock an instance @mock . patch ( __name__ + '.Castle' ) def test_mock_castle ( self , MockCastle ): instance = MockCastle instance . get_boss . return_value = \"Toad\" instance . get_world . return_value = \"Desert Land\" self . castle = Castle self . assertEqual ( self . castle . get_boss (), \"Toad\" ) self . assertEqual ( self . castle . get_world (), \"Desert Land\" ) # Mock an instance method def test_mock_castle_instance_method ( self ): # Boss is still Bowser self . assertNotEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) # Set a return_value for the get_boss method self . castle . get_boss = mock . Mock ( return_value = \"Koopa Troopa\" ) # Boss is Koopa Troopa now self . assertEqual ( self . castle . get_boss (), \"Koopa Troopa\" ) def test_castle_with_more_bosses ( self ): multi_boss_castle = mock . Mock () # Set a list as side_effect for the get_boss method multi_boss_castle . get_boss . side_effect = [ \"Goomba\" , \"Boo\" ] # First value is Goomba self . assertEqual ( multi_boss_castle . get_boss (), \"Goomba\" ) # Second value is Boo self . assertEqual ( multi_boss_castle . get_boss (), \"Boo\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_boss_castle . get_boss ) def test_calls_to_castle ( self ): self . castle . access = mock . Mock () self . castle . access . return_value = \"No access\" # We should retrieve no access for everybody self . assertEqual ( self . castle . access ( 'Let me in' ), \"No access\" ) self . assertEqual ( self . castle . access ( 'Let me in, please' ), \"No access\" ) self . assertEqual ( self . castle . access ( 'Let me in, please sir!' ), \"No access\" ) # Verify the length of the arguments list self . assertEqual ( len ( self . castle . access . call_args_list ), 3 ) Run the test suite In [102]: import sys suite = unittest . TestLoader () . loadTestsFromTestCase ( CharacterTestClass ) unittest . TextTestRunner ( verbosity = 4 , stream = sys . stderr ) . run ( suite ) test_calls_to_castle (__main__.CharacterTestClass) ... ok test_castle_with_more_bosses (__main__.CharacterTestClass) ... ok test_default_castle_boss (__main__.CharacterTestClass) Verifty the default boss is Bowser ... ok test_default_castle_world (__main__.CharacterTestClass) Verify the default world is Grass Land ... ok test_mock_access_denied (__main__.CharacterTestClass) Access denied for star powerup ... ok test_mock_access_granted (__main__.CharacterTestClass) Access granted for mushroom powerup ... ok test_mock_castle (__main__.CharacterTestClass) ... ok test_mock_castle_boss (__main__.CharacterTestClass) ... ok test_mock_castle_instance_method (__main__.CharacterTestClass) ... ok ---------------------------------------------------------------------- Ran 9 tests in 0.016s OK Out[102]: <unittest.runner.TextTestResult run=9 errors=0 failures=0> In [103]: class CharacterCastleTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character and Castle class together \"\"\" @mock . patch ( __name__ + '.Castle' ) @mock . patch ( __name__ + '.Character' ) def test_mock_castle_and_character ( self , MockCharacter , MockCastle ): # Note the order of the arguments of this test MockCastle . name = 'Mocked Castle' MockCharacter . name = 'Mocked Character' self . assertEqual ( Castle . name , 'Mocked Castle' ) self . assertEqual ( Character . name , 'Mocked Character' ) def test_fake_powerup ( self ): character = Character ( \"Sentinel Character\" ) character . powerup = mock . Mock () character . powerup . return_value = mock . sentinel . fake_superpower self . assertEqual ( character . powerup (), mock . sentinel . fake_superpower ) def test_castle_with_more_powerups ( self ): self . castle = Castle ( 'Beautiful Castle' ) multi_characters = mock . Mock () # Set a list as side_effect for the get_boss method multi_characters . get_powerup . side_effect = [ \"mushroom\" , \"star\" ] # First value is mushroom self . assertEqual ( multi_characters . get_powerup (), \"mushroom\" ) # Second value is star self . assertEqual ( multi_characters . get_powerup (), \"star\" ) # Third value does not exist and raises a StopIteration self . assertRaises ( StopIteration , multi_characters . get_powerup ) In [104]: suite = unittest . TestLoader () . loadTestsFromTestCase ( CharacterCastleTestClass ) unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) test_castle_with_more_powerups (__main__.CharacterCastleTestClass) ... ok test_fake_powerup (__main__.CharacterCastleTestClass) ... ok test_mock_castle_and_character (__main__.CharacterCastleTestClass) ... ok ---------------------------------------------------------------------- Ran 3 tests in 0.006s OK Out[104]: <unittest.runner.TextTestResult run=3 errors=0 failures=0>", "tags": "posts", "url": "mocking-in-unittests-in-python.html", "loc": "mocking-in-unittests-in-python.html" }, { "title": "Unittesting in a Jupyter notebook", "text": "Example - Unittesting In this example I show how to run a unittest within your Jupyter Notebook with two simple classes. Create two classes Castle class The important method for the castle is the access method. It will grant access when the character has the powerup 'Super Mushroom'. In [1]: class Castle ( object ): def __init__ ( self , name ): self . name = name self . _boss = 'Bowser' self . _world = 'Grass Land' def access ( self , character ): if character . powerup == 'Super Mushroom' : return True else : return False def get_boss ( self ): return self . _boss def get_world ( self ): return self . _world Character class By default the powerup of a character is empty. In [2]: class Character ( object ): def __init__ ( self , name ): self . name = name self . powerup = '' def powerup ( self , powerup ): self . powerup = powerup def get_powerup ( self ): return self . powerup Create a test class We will create two characters and test that only the right powerup gives access to the castle. In [5]: import unittest class CharacterTestClass ( unittest . TestCase ): \"\"\" Defines the tests for the Character class \"\"\" def setUp ( self ): \"\"\" Set the castle for the test cases \"\"\" self . castle = Castle ( 'Bowsers Castle' ) def test_default_cannot_access ( self ): \"\"\" Default can not access \"\"\" default = Character ( 'Default' ) self . assertFalse ( self . castle . access ( default )) def test_mario_cannot_access ( self ): \"\"\" Mario cannot access \"\"\" mario = Character ( 'Mario' ) mario . powerup = 'Starman' self . assertFalse ( self . castle . access ( mario )) def test_peach_can_access ( self ): \"\"\" Peach can access \"\"\" peach = Character ( 'Peach' ) peach . powerup = 'Super Mushroom' self . assertTrue ( self . castle . access ( peach )) def test_default_castle_boss ( self ): \"\"\" Verifty the default boss is Bowser \"\"\" self . assertEqual ( self . castle . get_boss (), \"Bowser\" ) def test_default_castle_world ( self ): \"\"\" Verify the default world is Grass Land \"\"\" self . assertEqual ( self . castle . get_world (), \"Grass Land\" ) Run the test suite In [6]: import sys suite = unittest . TestLoader () . loadTestsFromTestCase ( CharacterTestClass ) unittest . TextTestRunner ( verbosity = 4 , stream = sys . stderr ) . run ( suite ) test_default_cannot_access (__main__.CharacterTestClass) Default can not access ... ok test_default_castle_boss (__main__.CharacterTestClass) Verifty the default boss is Bowser ... ok test_default_castle_world (__main__.CharacterTestClass) Verify the default world is Grass Land ... ok test_mario_cannot_access (__main__.CharacterTestClass) Mario cannot access ... ok test_peach_can_access (__main__.CharacterTestClass) Peach can access ... ok ---------------------------------------------------------------------- Ran 5 tests in 0.004s OK Out[6]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>", "tags": "posts", "url": "unittesting-in-jupyter-notebook.html", "loc": "unittesting-in-jupyter-notebook.html" }, { "title": "Add CSS to Splunk dashboard", "text": "The file should be located in %SPLUNK_HOME%/etc/apps/<APPNAME>/appserver/static/ If the file is called dashboard.css , the file will be automatically applied to all dashboards within this application. If you give it a custom name, you need to include it explicitly in the stylesheet attribute in the XML of your dashboard like the following <form stylesheet=\"mycustomstyle.css\"> Currently the base of my dashboard.css looks like the stylesheet of this webpage, to make it clear that I am in my own application. @ import url ( 'https://fonts.googleapis.com/css?family=Raleway' ) ; body , html { background-color : white ; font-family : \"Raleway\" ; height : 100 % ; } header { background-color : rgb ( 102 , 153 , 255 ); border : 0 px ; background-image : url ( './images/Bar.GIF' ); background-repeat : repeat-x ; background-position : bottom ; padding : 0 px 0 px 16 px 0 px ; } . dashboard-body { background-color : white ; height : 100 % ; } . dashboard { background-color : white ; } footer { margin-top : 0 px ; padding-bottom : 50 px ; font-size : small ; color : rgb ( 200 , 200 , 200 ); background-image : url ( './images/Bottom.GIF' ); background-repeat : repeat-x ; background-position : bottom ; } h1 , h2 , a { color : rgb ( 102 , 153 , 255 ); } Note that when a new stylesheet is added, Splunkweb needs a restart. If you simply update the stylesheet, run the following command: http://SPLUNKHOST/en-US/_bump", "tags": "posts", "url": "add-css-splunk-dashboard.html", "loc": "add-css-splunk-dashboard.html" }, { "title": "Create a Splunk app to monitor Nginx", "text": "Using the web interface of Splunk you can easily add a new app. This will create the following structure: nginxwatcher/ |-- bin | `-- README |-- default | |-- app.conf | |-- data | | `-- ui | | |-- nav | | | `-- default.xml | | `-- views | | `-- README | |-- props.conf |-- local | `-- app.conf `-- metadata |-- default.meta `-- local.meta To the nginxwatcher/default folder the files indexes.conf and inputs.conf should be added. The content of indexes.conf should be something like the following: [nginxwatcher] coldPath = $SPLUNK_DB\\nginxwatcher\\colddb enableDataIntegrityControl = 0 enableTsidxReduction = 0 homePath = $SPLUNK_DB\\nginxwatcher\\db maxTotalDataSizeMB = 512000 thawedPath = $SPLUNK_DB\\nginxwatcher\\thaweddb The content of inputs.conf should indicate we are monitoring the Nginx logging folder: [monitor:///var/log/nginx/*.log] disabled = false host = nginxwatcher index = nginxwatcher sourcetype = nginxwatcher_logs The contens of props.conf looks like this: [nginxwatcher_logs] NO_BINARY_CHECK = true TZ = UTC category = Structured pulldown_type = 1 KV_MODE = none disabled = false After restarting Splunk we can start using the new index of the Nginx app and query with: index=\"nginxwatcher\" To retrieve the values of the log files, we can use regular expressions and follow the description on the Nginx website. index=\"nginxwatcher\" | rex field=_raw \"&#94;(?&lt;remote_addr&gt;\\d+.\\d+.\\d+.\\d+)\\s-\\s(?P&lt;remote_user&gt;.*?)\\s\\[(?&lt;localtime&gt;\\d+\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s\\+\\d+)\\]\\s+\\\"(?&lt;request&gt;.*?)\\\"\\s(?&lt;status&gt;\\d+)\\s(?&lt;bytes_sent&gt;\\d+.*?)\\s\\\"(?&lt;http_refererer&gt;.*?)\\\"\\s\\\"(?&lt;http_user_agent&gt;.*)\\\"\" Obviously we do not want to do this every time and we extract the values using the props.conf . The file is changed to: [nginxwatcher_logs] NO_BINARY_CHECK = true TZ = UTC category = Structured pulldown_type = 1 KV_MODE = none disabled = false EXTRACT-e1 = &#94;(?<remote_addr>\\d+.\\d+.\\d+.\\d+)\\s-\\s(?P<remote_user>.*?)\\s\\[(?<localtime>\\d+\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s\\+\\d+)\\]\\s+\\\"(?<request>.*?)\\\"\\s(?<status>\\d+)\\s(?<bytes_sent>\\d+.*?)\\s\\\"(?<http_refererer>.*?)\\\"\\s\\\"(?<http_user_agent>.*)\\\" The following query will show a timechart for the status over time, grouped by 30 minutes. index=\"nginxwatcher\" | timechart count(status) span=30m by status Top visitors: index=\"nginxwatcher\" | top limit=20 remote_addr Pages which produce the most errors: index=\"nginxwatcher\" | search status >= 500 | stats count(status) as cnt by request, status | sort cnt desc All these graphs can of course be added to a dashboard to keep a close watch on the webserver.", "tags": "posts", "url": "create-splunk-app-monitor-nginx.html", "loc": "create-splunk-app-monitor-nginx.html" }, { "title": "Adding aliases to Windows", "text": "Open the Registry Editor and go to the following key Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor and add the key AutoRun with the value of the file containing the aliases. In my case this is \"%USERPROFILE%\\alias.cmd\" Save the change to the registry and restart the command prompt. Currently my alias.cmd contains the following: :: Registry path: Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor :: Key: AutoRun :: Value: \"%USERPROFILE%\\alias.cmd\" @echo off DOSKEY alias = \"C:\\Program Files (x86)\\Notepad++\\notepad++.exe\" \"%USERPROFILE%\\alias.cmd\" DOSKEY ls = dir /B DOSKEY proj1 = cd \"%USERPROFILE%\\Documents\\Projects\\proj1\"", "tags": "posts", "url": "adding-aliases-windows.html", "loc": "adding-aliases-windows.html" }, { "title": "Install KDiff3 on OSX", "text": "Installing the diff/merge tool KDiff3 is easy using the package manager Homebrew extension Cask . The extension makes is possible to install (GUI) applications on the Mac without the dragging and dropping of the DMG-files. jitsejan@MBP $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" jitsejan@MBP $ brew tap caskroom/cask jitsejan@MBP $ brew cask install kdiff3", "tags": "posts", "url": "install-kdiff3-on-osx.html", "loc": "install-kdiff3-on-osx.html" }, { "title": "Installing Splunk on Ubuntu 14.04", "text": "Download the latest version of Splunk Light (Currently version 6.5) to your download folder. jitsejan@jjsvps:~/Downloads$ wget -O splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64&platform=linux&version=6.5.0&product=splunk_light&filename=splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz&wget=true' Extract the archive to the /opt/ folder. jitsejan@jjsvps:~/Downloads$ sudo tar zvzf splunklight-6.5.0-59c8927def0f-Linux-x86_64.tgz -C /opt/ Export the folder where Splunk is installed to your environment. jitsejan@jjsvps:/opt/splunk$ echo 'export SPLUNK_HOME=/opt/splunk/' >> ~/.bashrc jitsejan@jjsvps:/opt/splunk$ source ~/.bashrc Make sure the rights of the /opt/splunk/ folder are correctly set. jitsejan@jjsvps:/opt$ sudo chown -R jitsejan:root splunk/ Enable access to the Splunk web interface by adding a subdomain that links to the right port. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo nano splunk Add the following to the configuration file. Change the subdomain and port to the right values for you. server { listen 80 ; server_name subdomain.jitsejan.com ; location / { proxy_pass http : // localhost : 8888 ; } } Enable the subdomain by creating a system link. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo ln -s /etc/nginx/sites-available/splunk /etc/nginx/sites-enabled/ And finally restart the server. jitsejan@jjsvps:/etc/nginx/sites-available$ sudo service nginx restart Now you can open up the browser and go the the subdomain that you just introduced.", "tags": "posts", "url": "installing-splunk-on-ubuntu-1404.html", "loc": "installing-splunk-on-ubuntu-1404.html" }, { "title": "Add CSS to Jupyter notebook", "text": "To add some style to the notebook, Jupyter has the option to add a custom css file. This file should be located in ~/.jupyter/custom/custom.css . If this file does not exist yet, create the directory if needed and add the stylesheet. Add the following code and refresh the notebook to see if it works. body { background-color : purple ; } To make it easier to modify this file, create a link from the stylesheet to the working directory for the notebooks in order to be able to modify the stylesheet in the browser. jitsejan@jjsvps:~/code/notebooks$ ln -s ~/.jupyter/custom/custom.css .", "tags": "posts", "url": "add-css-to-jupyter-notebook.html", "loc": "add-css-to-jupyter-notebook.html" }, { "title": "Create a choropleth for the top 50 artists I listen on Spotify", "text": "# Import the settings for the notebooks from notebooksettings import GRACENOTE_USERID , SPOTIFY_USERNAME 1. Connect to Spotify I will use the Spotipy library to connect to Spotify. Both reading the library and reading the top tracks will be enabled by setting the scope appropriately. import sys import spotipy import spotipy.util as util # Set scope to read the library and read the top tracks scope = 'user-library-read user-top-read' username = SPOTIFY_USERNAME token = util . prompt_for_user_token ( username , scope ) 2. Retrieve songs from Spotify After creating a token, you can make a new Spotipy instance and connect to your account. Lets retrieve the top 50 of artists of my account and add the artists to a list. LIMIT = 50 OFFSET = 0 artists = {} if token : sp = spotipy . Spotify ( auth = token ) results = sp . current_user_top_artists ( limit = LIMIT , offset = OFFSET ) for artist in results [ 'items' ]: artist_id = artist [ 'id' ] artists [ artist_id ] = sp . artist ( artist_id )[ 'name' ] else : print \"Can't get token for\" , username 3. Create a placeholder for the country mapping To create a choropleth, I will create a list of countries using the Pycountry library. The country data will contain the name, the three character long abbreviation, the number of occurrences of the country for the different artists and a list of artists. import pycountry country_data = [] for cnt in pycountry . countries : country_data . append ([ cnt . name , cnt . alpha3 , 0 , []]) 4. Create a mapping for the country name to country abbreviation To map the country name to a three character abbreviation, we need to make a mapping linking the two together. mapping = { country . name : country . alpha3 for country in pycountry . countries } 5. Retrieve the country of origin for the artists To find the country of origin, I will make use of the pygn library to connect to Gracenote and find metadata for music. First I create a connection with pygn so I can retrieve the metadata from the Gracenote servers. Next I will find the country and map it to the right abbreviation. Finally I will increase the counter in the country data for the corresponding country and add the artist to the list. import pygn clientID = GRACENOTE_USERID userID = pygn . register ( clientID ) for artist_name in artists . values (): # Retrieve metadata metadata = pygn . search ( clientID = clientID , userID = userID , artist = artist_name ) if '2' in metadata [ 'artist_origin' ] . keys (): country = metadata [ 'artist_origin' ][ '2' ][ 'TEXT' ] elif len ( metadata [ 'artist_origin' ] . keys ()) == 0 : country = None else : country = metadata [ 'artist_origin' ][ '1' ][ 'TEXT' ] # Replace names if country == 'South Korea' : country = 'Korea, Republic of' if country == 'North Korea' : country = \"Korea, Democratic People's Republic of\" # Retrieve the mapping country_code = mapping . get ( country , 'No country found' ) # Increase the counter for corresponding country for index , cnt_entry in enumerate ( country_data ): if cnt_entry [ 1 ] == country_code : country_data [ index ][ 2 ] += 1 country_data [ index ][ 3 ] . append ( artist_name ) 6. Create a dataframe from the data Using Pandas we will now create a DataFrame to convert the data from the country data to a Pandas format. import pandas as pd df = pd . DataFrame ( country_data , columns = [ 'Country name' , 'Code' , 'Amount' , 'Artists' ]) df . head () 7. Create a choropleth from the data Using Plotly we can easily make a choropleth for the data that we just retrieved. In the data settings you indicate the type is a choropleth graph, the locations can be found in the 'Code' column and the important data is the column 'Amount'. Next we set the colors and a title and we are good to go. import plotly.plotly as py from plotly.graph_objs import * data = [ dict ( type = 'choropleth' , locations = df [ 'Code' ], z = df [ 'Amount' ], text = df [ 'Country name' ], colorscale = [[ 0 , \"rgb(0, 228, 97)\" ], [ 0.35 , \"rgb(70, 232, 117)\" ], [ 0.5 , \"rgb(100, 236, 138)\" ], [ 0.6 , \"rgb(120, 240, 172)\" ], [ 0.7 , \"rgb(140, 245, 201)\" ], [ 1 , \"rgb(250, 250, 250)\" ]], autocolorscale = False , reversescale = True , marker = dict ( line = dict ( color = 'rgb(180,180,180)' , width = 0.5 ) ), tick0 = 0 , zmin = 0 , dtick = 1000 , colorbar = dict ( autotick = False , tickprefix = '' , title = 'Number of artists' ), ) ] layout = dict ( title = \"Countries of origin of artists I listen on Spotify\" , geo = dict ( showframe = False , showcoastlines = False , projection = dict ( type = 'Mercator' ) ) ) figure = dict ( data = data , layout = layout ) py . iplot ( figure , validate = False ) 8. Conclusion As we can see in the graph above, the hypothesis is not completely true. The majority is still from the States. 9. Extra Looking at the same type of graph for the top 50 tracks on Spotify, ignoring artist that are double, I retrieve the following graph. In this graph it is slightly more evident that lately I listened to too much K-pop.", "tags": "posts", "url": "create-choropleth-top-50-artists-I-listen-on-spotify.html", "loc": "create-choropleth-top-50-artists-I-listen-on-spotify.html" }, { "title": "Install Java version 8", "text": "Install the common software-properties to be able to use the add-app-repository command. shell jitsejan@jjsvps:~$ sudo apt-get install software-properties-common Add the Java repository to the Ubuntu sources. shell jitsejan@jjsvps:~$ sudo add-apt-repository ppa:webupd8team/java Update the sources to retrieve the new Java repository. shell jitsejan@jjsvps:~$ sudo apt-get update Install Java version 8. shell jitsejan@jjsvps:~$ sudo apt-get install oracle-java8-installer", "tags": "posts", "url": "install-java-version-8.html", "loc": "install-java-version-8.html" }, { "title": "Add Flickr photosets to a Django site", "text": "1. Set up connection (Note that I just put a dummy key, secret and userid) import flickrapi key = '123456789abcdefghijklmn' secret = '123456a7890' userid = '123456@N16' flickr = flickrapi . FlickrAPI ( key , secret ) 2. Retrieve the photosets from lxml import etree sets = flickr . photosets . getList ( user_id = userid ) photoset = sets . findall ( \".//photoset\" )[ 0 ] print etree . tostring ( photoset ) will return <photoset id = \"72157674032173850\" primary = \"30292011566\" secret = \"c384c894ce\" server = \"8552\" farm = \"9\" photos = \"31\" videos = \"0\" needs_interstitial = \"0\" visibility_can_see_set = \"1\" count_views = \"0\" count_comments = \"0\" can_comment = \"0\" date_create = \"1476484376\" date_update = \"1476488433\" > <title>Canada 2016 </title> <description>Visit to Toronto, Montreal and the Falls.</description> </photoset> 3. Connect to Django Since I want to add to pictures from Flickr to my Django webpage, I need to connect to the database. import os , sys project_path = '/opt/env/django_project/' os . environ . setdefault ( \"DJANGO_SETTINGS_MODULE\" , \"django_project.settings\" ) sys . path . append ( project_path ) 4. Load the Photoset model Now we set the path to be the Django path, we can import the models from my blog. import django django . setup () from blog.models import Photoset for field in Photoset . _meta . get_fields (): print field This will show the fields of the model. <ManyToOneRel: blog.photo> blog.Photoset.id blog.Photoset.flickr_id blog.Photoset.secret blog.Photoset.title blog.Photoset.description blog.Photoset.date_create blog.Photoset.date_update blog.Photoset.created blog.Photoset.modified 5. Add Flickr photosets to Django if not Photoset . objects . filter ( flickr_id = photoset . get ( 'id' )) . exists (): blog_photoset = Photoset () blog_photoset . flickr_id = photoset . get ( 'id' ) blog_photoset . secret = photoset . get ( 'secret' ) blog_photoset . title = photoset . find ( 'title' ) . text blog_photoset . description = photoset . find ( 'description' ) . text if photoset . find ( 'description' ) . text is 'null' else \"\" blog_photoset . date_create = photoset . get ( 'date_create' ) blog_photoset . date_update = photoset . get ( 'date_update' ) blog_photoset . save () print \"Added photoset ' %s ' to database!\" % ( blog_photoset . title ) else : print \"Photoset ' %s ' already in database!\" % ( photoset . find ( 'title' ) . text ) In my case the photoset has already been added. Photoset 'Canada 2016' already in database! 6. Retrieve photos from Flickr photoset for photo in flickr . walk_set ( photoset . attrib [ 'id' ]): photo_element = flickr . photos . getinfo ( photo_id = photo . get ( 'id' )) . find ( './photo' ) print etree . tostring ( photo_element ) break # Just print one This will return the XML object of a photo. <photo id= \"30326779825\" secret= \"78076b80de\" server= \"8140\" farm= \"9\" dateuploaded= \"1476484428\" isfavorite= \"0\" license= \"0\" safety_level= \"0\" rotation= \"270\" originalsecret= \"cb36a41988\" originalformat= \"jpg\" views= \"1\" media= \"photo\" > <owner nsid= \"45832294@N06\" username= \"jitsejan\" realname= \"Jitse-Jan van Waterschoot\" location= \"\" iconserver= \"5495\" iconfarm= \"6\" path_alias= \"jitsejan\" /> <title> Looking at the city </title> <description/> <visibility ispublic= \"1\" isfriend= \"0\" isfamily= \"0\" /> <dates posted= \"1476484428\" taken= \"2016-08-26 04:14:31\" takengranularity= \"0\" takenunknown= \"0\" lastupdate= \"1476700149\" /> <editability cancomment= \"0\" canaddmeta= \"0\" /> <publiceditability cancomment= \"1\" canaddmeta= \"0\" /> <usage candownload= \"1\" canblog= \"0\" canprint= \"0\" canshare= \"1\" /> <comments> 0 </comments> <notes/> <people haspeople= \"0\" /> <tags/> <urls> <url type= \"photopage\" > https://www.flickr.com/photos/jitsejan/30326779825/ </url> </urls> </photo> 7. Create URL for Flickr photo # url_template = \"http://farm{farm-id}.staticflickr.com/{server-id}/{id}_{secret}_[mstzb].jpg\" url = \"http://farm %(farm)s .staticflickr.com/ %(server)s / %(id)s _ %(secret)s _z.jpg\" % photo . attrib print url Resulting URL: http://farm9.staticflickr.com/8140/30326779825_78076b80de_z.jpg 8. Load the Photo model from blog.models import Photo for field in Photo . _meta . get_fields (): print field Above code will show the fields of the Photo model in Django. blog.Photo.id blog.Photo.flickr_id blog.Photo.title blog.Photo.description blog.Photo.date_posted blog.Photo.date_taken blog.Photo.url blog.Photo.image_url blog.Photo.created blog.Photo.modified blog.Photo.photoset 9. Add Flickr photo to Django blog_photo = Photo () blog_photo . flickr_id = photo . get ( 'id' ) blog_photo . title = photo . get ( 'title' ) blog_photo . description = photo . get ( 'description' ) if photoset . get ( 'description' ) is 'null' else \"\" blog_photo . date_posted = photo_element . find ( 'dates' ) . attrib [ 'posted' ] blog_photo . date_taken = photo_element . find ( 'dates' ) . attrib [ 'taken' ] blog_photo . url = photo_element . find ( 'urls/url' ) . text blog_photo . image_url = url try : blog_photo . photoset = blog_photoset except : blog_photo . photoset = Photoset . objects . filter ( flickr_id = photoset . get ( 'id' ))[ 0 ] if not Photo . objects . filter ( flickr_id = blog_photo . flickr_id ) . exists (): blog_photo . save () print \"Added photo ' %s ' to database!\" % ( blog_photo . title ) else : print \"Photo ' %s ' already in database!\" % ( blog_photo . title ) Again, the item is already in the database in my case. Photo 'Looking at the city' already in database!", "tags": "posts", "url": "add-flickr-photosets-django.html", "loc": "add-flickr-photosets-django.html" }, { "title": "Change PostgreSQL database encoding", "text": "First login to the PostgreSQL shell. ( env ) jitsejan@jjsvps:/opt/canadalando_env/canadalando_django$ sudo -u postgres psql Check the list of databases. postgres = # \\l Here I could see my database had the wrong encoding, instead of SQL_ASCII I want UTF8. I dropped the database so I can re-create it with the right encoding. Note that I did NOT make a back-up, since my database was still empty. postgres = # DROP DATABASE website_db; In order to use UTF8 the template for the databases needs to be updated first. Disable the template1. postgres = # UPDATE pg_database SET datistemplate = FALSE WHERE datname ='template1'; Drop the database. postgres = # DROP DATABASE template1; Now re-create it with the right encoding. postgres = # CREATE DATABASE template1 WITH TEMPLATE = template0 ENCODING = 'UNICODE'; Activate the template. postgres = # UPDATE pg_database SET datistemplate = TRUE WHERE datname = 'template1'; Now we can re-create the database that we dropped earlier. postgres = # CREATE DATABASE website_db WITH ENCODING 'UNICODE';", "tags": "posts", "url": "change-postgresql-database-encoding.html", "loc": "change-postgresql-database-encoding.html" }, { "title": "Upgrade PostgreSQL", "text": "older/wrong versions. First check which PostgreSQL is running jitsejan@jjsvps:~$ sudo service postgresql status Probably this will list version 9.3 running on port 5432. Add the PostgreSQL repository to the sources to be able to update to newer versions. jitsejan@jjsvps:~$ sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' jitsejan@jjsvps:~$ wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Now update the system to retrieve data from the new repository. jitsejan@jjsvps:~$ sudo apt-get update jitsejan@jjsvps:~$ sudo apt-get upgrade Next we can install version 9.4 of PostgreSQL. jitsejan@jjsvps:~$ sudo apt-get install postgresql-9.4 If you check the status again, you will see two instances of PostgreSQL running. jitsejan@jjsvps:~$ sudo service postgresql status Optionally the old version can be removed. jitsejan@jjsvps:~$ sudo apt-get remove --purge postgresql-9.3", "tags": "posts", "url": "upgrade-postgresql.html", "loc": "upgrade-postgresql.html" }, { "title": "Write dictionary to CSV in Python", "text": "import csv def write_dictionary_to_csv ( o_file , d ): \"\"\" Write dictionary to output file \"\"\" with open ( o_file , 'wb' ) as csvfile : outputwriter = csv . writer ( csvfile , delimiter = ';' , quoting = csv . QUOTE_MINIMAL ) outputwriter . writerow ( d . keys ()) outputwriter . writerows ( zip ( * d . values ())) dictionary = { \"key1\" : [ 12 , 23 , 34 ], \"key2\" : [ 45 , 56 , 67 ], \"key3\" : [ 78 , 89 , 90 ]} output_file = 'output.csv' write_dictionary_to_csv ( output_file , dictionary )", "tags": "posts", "url": "write-dictionary-to-csv-python.html", "loc": "write-dictionary-to-csv-python.html" }, { "title": "Using Supervisor to start Gunicorn", "text": "Install Supervisor. jitsejan@jjsvps:~$ sudo pip install supervisor Create the default configuration file for Supervisor. jitsejan@jjsvps:~$ sudo echo_supervisord_conf > /etc/supervisord.conf Create the configuration file for the website. jitsejan@jjsvps:~$ sudo nano /etc/supervisor/conf.d/website.conf Enter the following. ; /etc/supervisor/conf.d/website.conf [program:website] command=gunicorn -c /opt/env/gunicorn_config.py django_project.wsgi:application directory=/opt/env/django_project/ user=jitsejan autostart=True autorestart=True redirect_stderr=True Make the file executable. jitsejan@jjsvps:~$ sudo chmod a+x /etc/supervisor/conf.d/website.conf Reload Supervisor to find the new file and update the configuration. jitsejan@jjsvps:~$ sudo supervisorctl reread jitsejan@jjsvps:~$ sudo supervisorctl update Now you can start the website with the following command. jitsejan@jjsvps:~$ sudo supervisorctl start website", "tags": "posts", "url": "using-supervisor-start-gunicorn.html", "loc": "using-supervisor-start-gunicorn.html" }, { "title": "Check the listening ports on the server", "text": "Use netstat to check with ports are listening on the machine. jitsejan@jjsvps:~$ netstat -lnt | awk '$6 == \"LISTEN\"'", "tags": "posts", "url": "check-listening-ports-on-server.html", "loc": "check-listening-ports-on-server.html" }, { "title": "Install Anaconda on Ubuntu 14.04", "text": "Retrieve the last Anaconda version for your system (32 or 64 bit). jitsejan@jjsvps:~$ cd Downloads/ jitsejan@jjsvps:~/Downloads$ wget https://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh Run the installer. jitsejan@jjsvps:~/Downloads$ bash Anaconda2-4.1.1-Linux-x86_64.sh Update the terminal to include the Anaconda references. jitsejan@jjsvps:~/Downloads$ source ~/.bashrc Test if iPython is working now. jitsejan@jjsvps:~$ ipython -v All set.", "tags": "posts", "url": "install-anaconda-ubuntu-1404.html", "loc": "install-anaconda-ubuntu-1404.html" }, { "title": "Move Django database between servers", "text": "Save the database on the old server. ( oldenv ) jitsejan@oldvps:/opt/oldenv/django_project$ sudo python manage.py dumpdata blog > blog.json Load the data on the new server. Make sure the models for both blogs are identical. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py loaddata blog.json", "tags": "posts", "url": "move-django-between-servers.html", "loc": "move-django-between-servers.html" }, { "title": "Install Django on Ubuntu 14.04 with virtualenv, Nginx, Gunicorn and postgres", "text": "Update the system first. jitsejan@jjsvps:~$ sudo apt-get update jitsejan@jjsvps:~$ sudo apt-get upgrade Install the virtual environment for Python. jitsejan@jjsvps:~$ sudo apt-get install python-virtualenv Create a new environment in a folder of your choice. jitsejan@jjsvps:~$ ls /opt jitsejan@jjsvps:~$ sudo virtualenv /opt/env jitsejan@jjsvps:~$ sudo chown jitsejan /opt/env/ Activate the environment. jitsejan@jjsvps:~$ source /opt/env/bin/activate Install Django inside the environment. ( env ) jitsejan@jjsvps:~$ pip install django ( env ) jitsejan@jjsvps:~$ deactivate env Install Postgresql on the system. jitsejan@jjsvps:~$ sudo apt-get install libpq-dev python-dev jitsejan@jjsvps:~$ sudo apt-get install postgresql postgresql-contrib Install the Nginx webserver on the system. jitsejan@jjsvps:~$ sudo apt-get install nginx Install Gunicorn in the environment. jitsejan@jjsvps:~$ source /opt/env/bin/activate ( env ) jitsejan@jjsvps:~$ sudo pip install gunicorn Create a database and a user for the project. jitsejan@jjsvps:~$ sudo -u postgres psql postgres = # CREATE DATABASE django_db; postgres = # CREATE USER django_user WITH PASSWORD 'django_pass'; postgres = # GRANT ALL PRIVILEGES ON DATABASE django_db TO django_user; Create a new project in the environment. ( env ) jitsejan@jjsvps:/opt/env$ django-admin.py startproject django_project Install the Psycopg2 so PostgreSQL can be used in the application. ( env ) jitsejan@jjsvps:~$ sudo pip install psycopg2 Add the database details to the settings.py ( env ) jitsejan@jjsvps:/opt/env/django_project$ nano django_project/settings.py Create the default entries for the application in the database, ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py syncdb ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py migrate ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py makemigrations Start the Django server. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo python manage.py runserver 0 .0.0.0:8080 Now use Gunicorn to connect to the server. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn --bind 0 .0.0.0:8080 django_project.wsgi:application Create a configuration file for Gunicorn. ( env ) jitsejan@jjsvps:/opt/env$ sudo nano gunicorn_config.py Add the following to the configuration file. command = '/opt/env/bin/gunicorn' pythonpath = '/opt/env/django_project' bind = '127.0.0.1:8088' workers = 3 user = 'jitsejan' Use the configuration file for starting Gunicorn. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn -c /opt/env/gunicorn_config.py django_project/django_project.wsgi:application Create a superuser for Django administration. ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo ./manage.py createsuperuser Add the STATIC_URL to the settings.py. ( env ) jitsejan@jjsvps:/opt/env/django_project$ nano django_project/settings.py ... STATIC_URL = '/static/' ... Now collect the static data ( env ) jitsejan@jjsvps:/opt/env/django_project$ sudo ./manage.py collectstatic Create a new site in Nginx for the Django project jitsejan@jjsvps:~$ sudo nano /etc/nginx/sites-available/django_project Add the following. Change the IP address, the folder for the static files and make sure the port is the same as configured for Gunicorn before. server { server_name 123.456.123.456, *.domain.com ; access_log off ; location /static/ { alias /opt/env/django_project/static/ ; } location / { proxy_pass http : // 127.0.0.1 : 8088 ; proxy_set_header X-Forwarded-Host $server_name ; proxy_set_header X-Real-IP $remote_addr ; add_header P3P 'CP=\"ALL DSP COR PSAa PSDa OUR NOR ONL UNI COM NAV\"' ; } } Enable the site by adding a link in the enabled sites. jitsejan@jjsvps:~$ sudo ln -s /etc/nginx/sites-available/django_project /etc/nginx/sites-enabled/ Stop Apache and start Nginx. jitsejan@jjsvps:~$ sudo service apache2 stop jitsejan@jjsvps:~$ sudo service nginx start Now run Gunicorn and visit the page in your browser. ( env ) jitsejan@jjsvps:/opt/env/django_project$ gunicorn -c /opt/env/gunicorn_config.py django_project/django_project.wsgi:application Hopefully the default Django page is shown now.", "tags": "posts", "url": "install-django-ubuntu-1404-virtualenv-nginx-gunicorn-and-postgres.html", "loc": "install-django-ubuntu-1404-virtualenv-nginx-gunicorn-and-postgres.html" }, { "title": "Install Jira on Ubuntu 14.04", "text": "Retrieve the last Jira binary from the website. Note that you should pick the right version, either x32 or x64. jitsejan@jjsvps:~/Downloads$ wget https://www.atlassian.com/software/jira/downloads/binary/atlassian-jira-software-7.2.1-x64.bin Make the binary executable. jitsejan@jjsvps:~/Downloads$ chmod a+x atlassian-jira-software-7.2.1-x64.bin Install the dependencies for Jira. jitsejan@jjsvps:~/Downloads$ sudo ​​apt-get install lsb-core​ default-jdk​ default-jre Execute the binary as sudo. ​jitsejan@jjsvps:~/Downloads$ ​$ sudo ./atlassian-jira-software-7.2.1-x64.bin Start the Jira server. jitsejan@jjsvps:~/Downloads$ sudo sh /opt/atlassian/jira/bin/start-jira.sh Create a Jira user and database. jitsejan@jjsvps:~$ sudo -u postgres psql postgres = # CREATE DATABASE jira; postgres = # CREATE USER jira_user WITH PASSWORD 'bla'; postgres = # GRANT ALL PRIVILEGES ON DATABASE jira TO jira_user; Now go to port 8080 on your IP address and perform the set-up. After some configuration you will be able to use Jira for your projects. Update It could be that the server does not start. Check if the permissions are right. jitsejan@jjsvps:~$ sudo chown -R jira:jira /var/atlassian/application-data/jira", "tags": "posts", "url": "install-jira-ubuntu-1404.html", "loc": "install-jira-ubuntu-1404.html" }, { "title": "Install Docker on Ubuntu 14.04", "text": "First update the system. $ sudo apt-get update $ sudo apt-get -y upgrade Add the recommended package for the current kernel. $ sudo apt-get install linux-image-extra- $( uname -r ) Add the official key for Docker. $ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D Add the source to the sources.list.d and refresh the packages. $ echo \"deb https://apt.dockerproject.org/repo ubuntu-trusty main\" | sudo tee /etc/apt/sources.list.d/docker.list $ sudo apt-get update Now you can install Docker. $ sudo apt-get install docker-engine Change the following in /etc/default/ufw: DEFAULT_APPLICATION_POLICY = \"DROP\" becomes DEFAULT_APPLICATION_POLICY = \"ACCEPT\" Restart the firewall. $ sudo ufw reload Create a Docker group and your current user to it to be able to connect to the Docker daemon. $ sudo groupadd docker $ sudo usermod -aG docker $USER Login again to start using Docker. Now check if Docker is working. $ sudo service docker start $ sudo docker run hello-world Hopefully this last step will download the image and run the container. If you are happy with the result, make it start automatically on system start. $ sudo systemctl enable docker", "tags": "posts", "url": "install-docker-ubuntu-1404.html", "loc": "install-docker-ubuntu-1404.html" }, { "title": "Install lxml for Python on DigitalOcean", "text": "Currently I am using a DigitalOcean droplet with 512 MB to run this website. I ran into an issue when I was trying to install lxml. First make sure the correct libraries are installed before lxml is installed. $ sudo apt-get install python-dev libxml2-dev libxslt1-dev zlib1g-dev Next, be aware that the 512 MB is not enough memory to compile the lxml package with Cython when you use pip to install, which means some additional steps are needed. To virtually increase your work memory, you could use a swapfile. Create a swapfile with these commands: $ sudo dd if = /dev/zero of = /swapfile1 bs = 1024 count = 524288 $ sudo mkswap /swapfile1 $ sudo chown root:root /swapfile1 $ sudo chmod 0600 /swapfile1 Now you can use pip to install the lxml Python module $ sudo pip install lxml And of course you need to clean up after installation is done. $ sudo swapoff -v /swapfile1 $ sudo rm /swapfile1", "tags": "posts", "url": "install-lxml-digital-ocean.html", "loc": "install-lxml-digital-ocean.html" }, { "title": "Getting started with Elasticsearch", "text": "Install ElasticSearch $ mkdir ~/es $ cd ~/es $ wget https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.5/elasticsearch-2.3.5.tar.gz $ tar -xzvf elasticsearch-2.3.5.tar.gz $ cd elasticsearch-2.3.5/ $ ./bin/elasticsearch -d $ curl http://127.0.0.1:9200 At this point you should see something like { \"name\" : \"Gailyn Bailey\" , \"cluster_name\" : \"elasticsearch\" , \"version\" : { \"number\" : \"2.3.5\" , \"build_hash\" : \"90f439ff60a3c0f497f91663701e64ccd01edbb4\" , \"build_timestamp\" : \"2016-07-27T10:36:52Z\" , \"build_snapshot\" : false , \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\" } Create the ES index for the posts In the mappings part we want to differentiate between finding a hit in the title or in the body. A hit of the search in the title has twice as much value as a hit in the body. #!/usr/bin/env python data = { \"settings\" : { \"number_of_shards\" : 4 , \"number_of_replicas\" : 1 }, \"mappings\" : { \"blog\" : { \"properties\" : { \"title\" : { \"type\" : \"string\" , \"boost\" : 4 }, \"body\" : { \"type\" : \"string\" , \"boost\" : 2 }, } } } } import json , requests response = requests . put ( 'http://127.0.0.1:9200/blog_index/' , data = json . dumps ( data )) print response . text Add the entries #!/usr/bin/env python import json , requests from blog.models import Entry data = '' for p in Entry . objects . all (): data += '{\"index\": {\"_id\": \" %s \"}} \\n ' % p . pk data += json . dumps ({ \"title\" : p . title , \"body\" : p . body }) + ' \\n ' response = requests . put ( 'http://127.0.0.1:9200/blog_index/blog/_bulk' , data = data ) print response . text Search the entries #!/usr/bin/env python import json , requests data = { \"query\" : { \"query_string\" : { \"query\" : \"python\" } } } response = requests . post ( 'http://127.0.0.1:9200/blog_index/blog/_search' , data = json . dumps ( data )) print response . json () This gives the following reply: { \"hits\" : { \"hits\" : [ { \"_score\" : 0.63516665 , \"_type\" : \"blog\" , \"_id\" : \"4\" , \"_source\" : { \"body\" : \"```python\\r\\n\\\"\\\"\\\" samples\\/crawl_01.py \\\"\\\"\\\"\\r\\n################################################################################\\r\\n# Application: WebParser example 01\\r\\n# File: samples\\/crawl_01.py\\r\\n# Goal:\\r\\n# Input:\\r\\n# Output:\\r\\n# Example:\\r\\n#\\r\\n# History: 2016-06-27 - JJ Creation of the file\\r\\n# ... main\\r\\n################################################################################\\r\\nif __name__ == \\\"__main__\\\":\\r\\n main()\\r\\n```\" , \"title\" : \"Simple webcrawling in Python \" }, \"_index\" : \"blog_index\" }, { \"_score\" : 0.4232868 , \"_type\" : \"blog\" , \"_id\" : \"7\" , \"_source\" : { \"body\" : \"This is a simple script to crawl information from a website when the content is dynamically loaded.\\r\\n```\\r\\n\\\"\\\"\\\" samples\\/crawl_02.py \\\"\\\"\\\"\\r\\n################################################################################\\r\\n# Application: WebParser example 02\\r\\n# File: samples\\/crawl_01.py\\r\\n# Goal: Retrieve content when JavaScript is used in page\\r\\n# Input:\\r\\n# Output:\\r\\n# Example:\\r\\n#\\r\\n# History: 2016-06-27 - JJ Creation of the file\\r\\n ... main\\r\\n################################################################################\\r\\nif __name__ == \\\"__main__\\\":\\r\\n main()\\r\\n```\" , \"title\" : \"Webcrawling in Python using Selenium\" }, \"_index\" : \"blog_index\" }, { \"_score\" : 0.35721725 , \"_type\" : \"blog\" , \"_id\" : \"13\" , \"_source\" : { \"body\" : \"#### Installation\\r\\nUse the [Anaconda](https:\\/\\/www.continuum.io\\/downloads \\\"Anaconda\\\") package. It will make starting with Data Science way easier, since almost all necessary packages are included and you can start right away.\\r\\n ... [Source](http:\\/\\/twiecki.github.io\\/blog\\/2014\\/11\\/18\\/python-for-data-science\\/ \\\"Twiecki@Github\\\")\" , \"title\" : \"Get started with data science in Python\" }, \"_index\" : \"blog_index\" } ], \"total\" : 3 , \"max_score\" : 0.63516665 }, \"_shards\" : { \"successful\" : 4 , \"failed\" : 0 , \"total\" : 4 }, \"took\" : 23 , \"timed_out\" : false } Each result will get a score and the results will be ordered accordingly. Of course the better the search query, the more the score will say about the likeliness of the result matching your query.", "tags": "posts", "url": "getting-started-with-elasticsearch.html", "loc": "getting-started-with-elasticsearch.html" }, { "title": "Getting started with data science in Python", "text": "Installation Use the Anaconda package. It will make starting with Data Science way easier, since almost all necessary packages are included and you can start right away. $ cd ~/Downloads $ wget http://repo.continuum.io/archive/Anaconda2-4.1.1-Linux-x86_64.sh $ bash Anaconda2-4.1.1-Linux-x86_64.sh $ source ~/.bashrc $ conda --version $ conda update conda Examples Make your first Data Frame #!/usr/bin/env python import pandas as pd df = pd . DataFrame ({ 'A' : 1. , 'B' : pd . Timestamp ( '20130102' ), 'C' : pd . Series ( 1 , index = list ( range ( 4 )), dtype = 'float32' ), 'D' : pd . Series ([ 1 , 2 , 1 , 2 ], dtype = 'int32' ), 'E' : pd . Categorical ([ \"test\" , \"train\" , \"test\" , \"train\" ]), 'F' : 'foo' }) df . groupby ( 'E' ) . sum () . D Create your first plots First update Seaborn $ conda install seaborn Next, create a plot of an example dataset #!/usr/bin/env python import seaborn as sns # Load one of the data sets that come with seaborn tips = sns . load_dataset ( \"tips\" ) tips . head () sns . jointplot ( \"total_bill\" , \"tip\" , tips , kind = 'reg' ); sns . lmplot ( \"total_bill\" , \"tip\" , tips , col = \"smoker\" ); Source", "tags": "posts", "url": "getting-started-with-datascience.html", "loc": "getting-started-with-datascience.html" }, { "title": "Add spacers in your OSX dock", "text": "Run this command in the terminal to add a spacer jitsejan@MBP $ defaults write com.apple.dock persistent-apps -array-add '{tile-data={}; tile-type=\"spacer-tile\";}' and restart the dock by running jitsejan@MBP $ killall Dock", "tags": "posts", "url": "add-spacers-dock-osx.html", "loc": "add-spacers-dock-osx.html" }, { "title": "Setting up an AWS EC instance", "text": "Go to the EC page Launch Instance Select Ubuntu Server 14.04 LTS (HVM), SSD Volume Type - ami-87564feb Select t2.micro (Free tier eligible) Select Next: Configure Instance Details Select Next: Add Storage Select Next: Tag Instance Give a Name to the Instance Select Next: Configure Security Group Create a new security group Add a Security group name Add a Description Add rule by clicking Add Rule First rule should be Custom TCP Rule, TCP Protocol, Port 80 for source Anywhere Click on Launch Select Review and launch In the pop-up, select Create a new key pair Fill in a Key pair name Download the Key Pair and save in a secure location Go to the instance page and wait until the machine is ready On your computer, change the permissions of the key pair you just downloaded $ chmod 400 keypairfile.pem Connect to the machine via ssh. Click on the Connect button in the instance overview for connection information $ ssh -i keypairfile.pem ec2-xx-xx-x-xx.eu-central-1.compute.amazonaws.com", "tags": "posts", "url": "setup-ec-instance.html", "loc": "setup-ec-instance.html" }, { "title": "Change the last modified time of a file", "text": "This script will change the last modified time of a file in the current directory to 4 days back. #!/bin/ksh numDays = 4 diff = 86400 * $numDays export diff newDate = $( perl -e 'use POSIX; print strftime \"%Y%m%d%H%M\", localtime time-$ENV{diff};' ) lastFile = $( ls -lt | egrep -v &#94;d | tail -1 | awk ' { print $9 } ' ) touch -t $newDate $lastFile", "tags": "posts", "url": "change-modified-time-file.html", "loc": "change-modified-time-file.html" }, { "title": "Show all debug information in Django", "text": "< pre > {% filter force_escape %} {% debug %} {% endfilter %} </ pre >", "tags": "posts", "url": "debugging-in-django.html", "loc": "debugging-in-django.html" }, { "title": "Webcrawling in Python using Selenium", "text": "For the script to work, four applications need to be installed first. jitsejan@jjsvps:~$ sudo pip install selenium jitsejan@jjsvps:~$ sudo apt-get install firefox jitsejan@jjsvps:~$ sudo pip install pyvirtualdisplay jitsejan@jjsvps:~$ sudo apt-get install xvfb Now the following script can be used. \"\"\" samples/crawl_02.py \"\"\" ################################################################################ # Application: WebParser example 02 # File: samples/crawl_01.py # Goal: Retrieve content when JavaScript is used in page # Input: # Output: # Example: # # History: 2016-06-27 - JJ Creation of the file # ################################################################################ ################################################################################ # Imports ################################################################################ import lxml.html import urllib2 from pyvirtualdisplay import Display from selenium import webdriver ################################################################################ # Definitions ################################################################################ HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } ################################################################################ # Classes ################################################################################ class WebParser ( object ): \"\"\" Definition of the WebParser \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Initialize the WebParser \"\"\" super ( WebParser , self ) . __init__ ( * args , ** kwargs ) @staticmethod def parse_page ( url ): \"\"\" Open URL and return the element tree of the page \"\"\" display = Display ( visible = 0 , size = ( 1920 , 1080 )) display . start () browser = webdriver . Firefox () browser . get ( url ) data = browser . page_source tree = lxml . html . fromstring ( data ) browser . quit () display . stop () return tree @staticmethod def find_css_element ( etree , element ): \"\"\" Find an element in the element tree and return it \"\"\" return etree . cssselect ( element ) ################################################################################ # Functions ################################################################################ def main (): \"\"\" Main function \"\"\" parser = WebParser () etree = parser . parse_page ( 'http://isitweekendalready.com' ) divs = parser . find_css_element ( etree , '#result' ) print divs [ 0 ] . text . strip () ################################################################################ # main ################################################################################ if __name__ == \"__main__\" : main () Update 05-Oct-2016 Recently I discovered that Selenium does not work well with newer versions of Firefox. Therefore I had to downgrade Firefox to be able to use Selenium. jitsejan@jjsvps:~$ firefox -v jitsejan@jjsvps:~$ sudo apt-get purge firefox jitsejan@jjsvps:~$ wget sourceforge.net/projects/ubuntuzilla/files/mozilla/apt/pool/main/f/firefox-mozilla-build/firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ sudo dpkg -i firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ rm firefox-mozilla-build_39.0.3-0ubuntu1_amd64.deb jitsejan@jjsvps:~$ firefox -v", "tags": "posts", "url": "webcrawling-with-selenium.html", "loc": "webcrawling-with-selenium.html" }, { "title": "Send attachment from command line", "text": "$ echo 'Mail with attachment' | mutt -a \"/file/to/add/\" -s \"FYI: See attachment\" -- name@email.com", "tags": "posts", "url": "send-attachment-from-command-line.html", "loc": "send-attachment-from-command-line.html" }, { "title": "Mount Amazon EC as local folder", "text": "jitsejan@MBP $ sshfs ubuntu@ec2-34-56-7-89.eu-central-1.compute.amazonaws.com:/home/ubuntu/ ~/AmazonEC2/ -oauto_cache,reconnect,defer_permissions,noappledouble,negative_vncache", "tags": "posts", "url": "mount-amazon-ec-local-folder.html", "loc": "mount-amazon-ec-local-folder.html" }, { "title": "Simple webcrawling in Python", "text": "\"\"\" samples/crawl_01.py \"\"\" ################################################################################ # Application: WebParser example 01 # File: samples/crawl_01.py # Goal: # Input: # Output: # Example: # # History: 2016-06-27 - JJ Creation of the file # ################################################################################ ################################################################################ # Imports ################################################################################ import lxml.html import urllib2 ################################################################################ # Definitions ################################################################################ HEADER = { 'Accept-Language' : 'nl-NL' , 'User-Agent' : \"\"\"Mozilla/5.0 (Windows; U; Windows NT 6.1; nl-NL; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729); nl-NL\"\"\" } ################################################################################ # Classes ################################################################################ class WebParser ( object ): \"\"\" Definition of the WebParser \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\" Initialize the WebParser \"\"\" super ( WebParser , self ) . __init__ ( * args , ** kwargs ) @staticmethod def parse_page ( url ): \"\"\" Open URL and return the element tree of the page \"\"\" req = urllib2 . Request ( url , headers = HEADER ) data = urllib2 . urlopen ( req ) . read () tree = lxml . html . fromstring ( data ) return tree @staticmethod def find_css_element ( etree , element ): \"\"\" Find an element in the element tree and return it \"\"\" return etree . cssselect ( element ) ################################################################################ # Functions ################################################################################ def main (): \"\"\" Main function \"\"\" parser = WebParser () etree = parser . parse_page ( 'http://isitweekendyet.com/' ) divs = parser . find_css_element ( etree , 'div' ) print divs [ 0 ] . text . strip () ################################################################################ # main ################################################################################ if __name__ == \"__main__\" : main ()", "tags": "posts", "url": "simple-webcrawling-python.html", "loc": "simple-webcrawling-python.html" }, { "title": "Create big files with dd", "text": "Use dd in Unix to create files with a size of 2.7 GB. #!/bin/ksh dir = /this/is/my/outputdir/ numGig = 2 .7 factor = 1024 memLimit = $( expr $numGig * $factor * $factor * $factor | bc ) cd $dir for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ; do dd if = /dev/urandom of = dummy_ $i .xml count = 204800 bs = $factor done", "tags": "posts", "url": "create-big-files-with-dd.html", "loc": "create-big-files-with-dd.html" }, { "title": "Find most used history command", "text": "$ awk '{print $1}' ~/.bash_history | sort | uniq -c | sort -n", "tags": "posts", "url": "most-used-history-command.html", "loc": "most-used-history-command.html" }] }